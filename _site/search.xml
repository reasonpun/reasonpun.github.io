<?xml version="1.0" encoding="utf-8"?>
<search>
  
    <entry>
      <title><![CDATA[关于苹果账号双重认证开启的一个问题]]></title>
      <url>/ios/2019/03/24/%E5%85%B3%E4%BA%8E%E8%8B%B9%E6%9E%9C%E5%8F%8C%E9%87%8D%E8%AE%A4%E8%AF%81%E5%BC%80%E5%90%AF%E7%9A%84%E4%B8%80%E4%B8%AA%E9%97%AE%E9%A2%98/</url>
      <content type="text"><![CDATA[最近遇到一个问题  我手上的一个苹果开发者账号在开启双重认证的时候忘记了安全提示问题，经过和苹果的多次斗智斗勇，终于得到了官方真传并比较完美的解决了开启双重认证的问题。事情的经过是这样滴我手头有一个很老的APP因为某些原因需要更新版本，所以按部就班的进行操作：修改，build，打包，blabla…..但是突然发现发布证书过期，且账号登录失败，并提示需要开启双重认证；遂，通过手机端登录账号并试图开启双重认证。但是意想不到的情况发生了…    经过是这样的        1. 开始双重认证，需要先验证安全提示问题；    2. 我擦，古老的这些问题已经忘记的一干二净，多次输入后直接被提示失败    3. 转而重设安全提示问题，竟然要求验证几个安全提示问题；    4. 故而选择“忘记安全提示问题”，仍要求回答部分安全问题以验证身份（我勒个去，如果我记得安全提示问题，还用这么麻烦？！）    5. 联系苹果客服：        * 客户小姐姐和小哥哥回复说在多个设备登录成功后等待几天再试，        * 这一等就是一周，然，并没有解决问题 ...经过上述折腾，我的问题还是没有解决，最后愤愤然要求转接了苹果高级技术顾问，并邮件他们问题。大约过了1天（当时是周末），收到了来自苹果的回复，问题最终得以解决。现就将可行的解决方案列在下方，供于我有相同遭遇的小朋友们参考最新的 MacOS 版本添加新的管理人账号 1. 请在配备有最新版本 macOS 的设备上创建新的管理员 (Administrator)：https://support.apple.com/zh-cn/guide/mac-help/set-up-other-users-mtusr001/mac 2. 登出原有的管理员 3. 登入此新创建的管理员帐号 4. 按照屏幕提示输入设定，当屏幕提示输入 Apple ID 时，请输入您的帐号持有人的 Apple ID 5. 登入后，屏幕会提示您开启双重认证，点选 “Use Two-Factor Authentication” 并点击继续 6. 输入受信任的电话号码以获取验证码，并点击继续 7. 输入验证码，并点击继续 8. 若日后不想继续在此 macOS 上使用此管理员，您可以选择删除此管理员，并将您的帐号持有人的 Apple ID 加入原来的管理员 (路径：系统偏好设定 System Preferences —&gt; 网路帐号 Internet Accounts —&gt; iCloud)这个解决方案虽然看着有些怪异，但是我真的没有精力和心情去深入理解，能解决眼前问题就好了。]]></content>
      <categories>
        
          <category> iOS </category>
        
      </categories>
      <tags>
        
          <tag> 苹果账号 </tag>
        
          <tag> 使用总结 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[读 吴军《谷歌方法论》- 为什么学习语文]]></title>
      <url>/2019/03/22/%E5%90%B4%E5%86%9B-%E8%B0%B7%E6%AD%8C%E6%96%B9%E6%B3%95%E8%AE%BA-%E8%AF%AD%E6%96%87%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%84%8F%E4%B9%89/</url>
      <content type="text"><![CDATA[为什么要学习语文。思考如下两个问题：  如果把语文放在所有的课程中，如何看待它的意义？  对比去年的语文水平，你觉得学习语文有什么意义？通过以上两个问题的思考，我们可以得出以下四个结论：  学习语文的目的是将其他学科（如数学，历史和科学学科）融会贯通，增加自己在这些学科的理解深度；  通过语文的学习，可以提升自己对已习得知识表述的准确性；通俗的讲就是增强自己的表达能力；  语文水平的提高，自己做的事情更加职业化、专业化，其他人更容易接受自己的见解；  语文的学习，可以提高个人的素养，提升个人的气质。一言以蔽之，语文的学习不仅能增加理解能力，还能增强表述能力，提升个人对其他学科的学习能力。]]></content>
      <categories>
        
      </categories>
      <tags>
        
          <tag> 读后感 </tag>
        
          <tag> 谷歌方法论 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Pyqt5 QSS]]></title>
      <url>/2019/03/16/pyqt5-QSS%E7%9A%84%E4%B8%80%E4%BA%9B%E6%80%BB%E7%BB%93/</url>
      <content type="text"><![CDATA[  QSS(Qt Style Sheets) Qt样式表，是用来自定义空间外观的一种机制。QSS大量参考了CSS的内容，但是比它要弱得多，体现为选择器少，可以使用的属性也不多，并且并不是多有属性都可以应用到PyQt空间上。未完待续 …参考文献  https://doc.qt.io/archives/qt-4.8/stylesheet-examples.html#customizing-qscrollbar  https://github.com/pyqt/examples  https://blog.csdn.net/qq_20029329/article/details/73698617  https://github.com/PyQt5/PyQt  https://juejin.im/entry/5bd2c17ae51d457a944b6d88  https://doc.qt.io/qt-5/stylesheet-reference.html  https://zhuanlan.zhihu.com/p/47221211  https://fishc.com.cn/thread-61195-1-1.html  http://www.jyguagua.com/?p=2510  https://www.cnblogs.com/skynet/p/4229556.html  https://blog.csdn.net/u010525694/article/details/78452786  http://dgovil.com/blog/2017/02/24/qt_stylesheets/  https://qtguide.ustclug.org/ch08-01.htm  http://www.cnblogs.com/skynet/p/4229556.html  https://www.programcreek.com/python/example/82623/PyQt5.QtWidgets.QSlider]]></content>
      <categories>
        
      </categories>
      <tags>
        
          <tag> pyqt5 </tag>
        
          <tag> QSS </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[内田光子-关于一流水准的思考]]></title>
      <url>/2019/03/14/%E5%86%85%E7%94%B0%E5%85%89%E5%AD%90-%E5%85%B3%E4%BA%8E%E4%B8%80%E6%B5%81%E6%B0%B4%E5%87%86%E7%9A%84%E6%80%9D%E8%80%83/</url>
      <content type="text"><![CDATA[  内田光子从小学习钢琴，后来因为父亲在奥地利担任外交官，全家移居维也纳，她考入了维也纳音乐学院，师从理查·豪瑟（Richard Hauser）、威廉·肯普夫（Wilhelm Kempff）和阿什肯纳齐（Vladimir Ashkenazy）等人，14 岁时首次在维也纳金色大厅登台表演。20 岁时获得贝多芬钢琴比赛冠军，第二年获得肖邦国际钢琴比赛亚军，随后成为世界上为数不多的钢琴独奏家。内田光子一生把所有的时间都花在了练习音乐和在世界各地巡回表演上，以至于一辈子未婚，她自己讲，她的工作性质也不适合组成家庭，可以讲她把自己献给了音乐。2009 年英国女王授予她女爵士的封号。先说一下文章中的这首曲子，是她演奏的莫扎特非常简单的《第 16 号钢琴奏鸣曲》（K.545）（这首曲子有个副标题 Sonata facile 意思是“单纯的奏鸣曲”）。这首奏鸣曲有多简单呢？大约钢琴考过 7 级的孩子都能弹，但是就是这样一首简单的曲子，内田光子却能弹出精彩。那么演奏家是怎么划分的呢？  音乐演奏家的划分方法一般人演奏钢琴，第一步是做到演奏准确，这是对初学者的要求，做到这一点就达到了计算机工程师中“码农”的水平。第二步则是做到弹出来好听，这里面就要求有比较娴熟的手法，和一点简单的专业技巧了，特别是对某些音符的特殊处理。到这个水平，大约相当于钢琴考过 10 级的水平，和第五级的工程师相当。再往上，演奏者就要能弹奏得如行云流水一般流畅动听，这就进入到专业入门的水平。在美国钢琴十级之上还有专业级的考试，要想通过专业级的考试就需要达到这样的水准。如果想在艺术上更上一层楼，需要把音乐弹出层次感，这时一架钢琴发出的音乐像是几层不同的旋律叠加在一起的，听众会听得如痴如醉，这相当于三级工程师。当然，艺术没有止境，再往上就需要超出技巧的范畴，需要对音乐和人生有深刻的理解了，到了这个水平就能很好地诠释音乐了，这时的演奏家可以被称为当代一流的演奏家或者历史上二流的，内田光子大约在这个水平，或者略高一点。再往上，则是被称为几十年出一个的奇才，他们放到历史的长河中也堪称一流，比如 19 世纪的演奏家肖邦、李斯特（Franz Liszt）和克拉拉（Clara，舒曼的妻子），20 世纪的鲁宾斯坦（Arthur Rubinstein）或者弗拉基米尔·霍洛维茨（Vladimir Horowitz）。可以说，真正一流的大师是在任何小事情上都能体现一流水准的人。很多人会觉得，某件事情太简单，它体现不出我的水平，其实反倒是小事情能够见真功夫。很多人讲 iPhone 设计得好，在它简单的设计背后，体现出设计者的匠心独运，产品经理和工程师们精益求精的特点。这才是一流的本质。总结一下一流和二流、三流的区别1. 区分一流和二流、三流并不需要通过什么复杂难做的大事，一流的人可以从平凡中显示出伟大。通俗地讲，就是能炒好土豆丝。2. 俄罗斯有句谚语，“虽然雄鹰有时飞得比麻雀还低，但是麻雀永远飞不到雄鹰能达到的高度”。因此，一流能达到的高度是二流、三流所达不到的。3. 即便是天赋很好，有贵人相助的人，最终成为大家公认的一流人物，也需要花很长时间，并且需要不断努力。有些人少年一朝成名，便忘乎所以，其实他们距离一流的水准还差得远呢。很多省市的高考前几名，不过是现代的方仲永罢了。4.由于一流人才站的高度比常人高，因此他们的职业生涯可以特别长。5. 最后，也是最关键的一点。正如罗曼·罗兰（Romain Rolland）所说，人要成为伟大，而不是显得伟大。对大家来讲，要成为一流，而不是显得像一流。最后，简单地概括一流的几个特点：首先他们能将看似平凡的事情做得不平凡，其次他们都具有自己的特点，三流的人是可以互相替代的，但是一流的人则不能。  要有本事把小事情做好，并且一辈子都要不断努力。]]></content>
      <categories>
        
      </categories>
      <tags>
        
          <tag> 硅谷来信 </tag>
        
          <tag> 艺术 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[读吴军《硅谷来信》心得笔记 - 第一篇]]></title>
      <url>/2019/03/11/%E5%90%B4%E5%86%9B-%E7%A1%85%E8%B0%B7%E6%9D%A5%E4%BF%A1-%E7%AC%94%E8%AE%B0-01/</url>
      <content type="text"><![CDATA[Google能够从一个小公司成长为全球巨无霸的IT，企业创始人的作用非常大。** 首要掌握的技能：切忌固守在自己根深蒂固的思维模式中！ **创始人可以从如下三步入手：第一件事情是招人。找合适的人，把重要的事情交给他们做，而不是事必躬亲，即不要做太多的细节管理（Micro Management）。  作为一个公司的创始人，他需要花很多时间找到各个关键岗位最重要的人，一旦找到那样的人，就相信那些人的能力好了。Google到2002年已经有四百人的时候，包括创始人在内的几个高管还要面试每一位HR已经决定录取的员工，因为招人实在太重要了，面试占掉了他们四分之一的时间。  人招不好，不仅耽误事情，而且常常是请神容易送神难，早去的员工常常占据了很重要的位置，除非他们能够和公司一同成长，否则公司长大之后，他们的工作很难安排。让他们当主管吧，他们又不具有相应的能力和潜力；继续让他们做非常具体的工作吧，他们会觉得创始人太薄情，甚至会纠集一些老员工对抗新来的主管。 1. 小公司招人的一个重要的原则，就是对方除了能力和品性能够胜任工作之外，还必须具有非常强的主动性。 2. 录用的人应该高出现有员工的平均水平。第二件事情是起到刹车的作用，而不是引擎的作用。一个管理有序的公司，动力应该来自于底层，刹车应该来自于高层，这样公司既有活力，也有秩序。  Google公司在发展到一百人的规模时，创始人、CEO和几个其他高管，每周要花一天的时间听下面员工介绍自己的项目。他们只负责一件事情，就是确定员工做的事情是该做的事情，而不至于让公司的业务太分散。至于该怎么做，他们从来不发表什么建议，因为既然是精心挑选出来的员工，就相信他们有能力将要做的事情做好。如果用一个词概括这些高管们在审核项目时的作用，就是刹车。第三件事情是确立公司的价值观和企业文化。一个公司价值观的确立，企业文化的确立，是在它成立的初期。创始人自己的任务就是：找到适合公司自己的商业模式，在早期让整个公司都认可它的价值观。一个创始人如果能做好这样三件事情，已经不容易了，具体的事情应该让下面的人做，自己不要干预太多。当一个创始人能够找到称职的主管工程、销售和行政的负责人，并且足够信任他们，那么公司就步入正轨了。如果创始人像诸葛亮那样事必躬亲，不仅会把自己累死，还会把公司做死。]]></content>
      <categories>
        
      </categories>
      <tags>
        
          <tag> 硅谷来信 </tag>
        
          <tag> 读后感 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[2018年大兴区幼升小政策]]></title>
      <url>/2019/03/11/2018%E5%B9%B4%E5%A4%A7%E5%85%B4%E5%8C%BA%E5%B9%BC%E5%8D%87%E5%B0%8F%E6%94%BF%E7%AD%96/</url>
      <content type="text"><![CDATA[非本市户籍的适龄儿童少年，因父母或者其他法定监护人在本区工作或居住需要在本区接受义务教育的，由其父母或其他法定监护人持本人在京务工就业证明、在京实际住所居住证明、全家户口簿、北京市居住证（或有效期内居住登记卡）、户籍所在地乡镇人民政府或街道办事处出具的在当地没有监护条件的证明等相关材料，到居住地所在镇人民政府或者街道办事处审核，审核通过的参加学龄人口信息采集。属于镇属入学范围的由各镇负责解决；属于直属地区范围的，区教委根据各校可提供学位情况和本人相关条件情况统筹协调入学。区镇两级政府建立非本市户籍适龄儿童少年接受义务教育证明证件材料联合审核机制。各镇人民政府、街道办事处依据本辖区非本市户籍适龄儿童少年接受义务教育证明证件材料审核要求对提出申请的进行联合审核。2018年大兴区非京籍幼升小五证审核细则一、审核程序（一）街道办事处或镇人民政府受理审核申请。非本市户籍适龄儿童少年父母或其他法定监护人（以下简称审核申请人）在规定时间内，到居住地所在街道办事处或镇人民政府提出审核申请，领取《××镇（街道）非本市户籍适龄儿童少年接受义务教育证明证件材料申请表》，按照街道办事处或镇人民政府审核实施细则和工作流程到相关部门审核“五证”。（二）相关部门联合审核。街道办事处和镇人民政府统筹辖区内“五证”审核工作，结合实际明确“五证”审核细则。各相关部门按照细则要求具体负责。１.在大兴区务工就业证明由区人力社保、工商部门分别审核，签有劳动合同或聘用合同或劳动关系证明的由区人力社保部门审核，法定代表人、股东、合伙人和个体工商户由工商部门审核。２.在大兴区实际住所居住证明：房产证（不动产权证书）或《北京市商品房预售合同》由区住建、规土等部门审核；租房材料由街道办事处或镇人民政府负责审核。３.北京市居住证（或有效期内的居住登记卡）由居住地派出所进行审核。４.全家户口簿由街道办事处或镇人民政府派出所负责审核，超龄儿童少年是否已在原籍入学由教育行政部门审核。５.户籍所在地街道办事处或乡镇人民政府出具的在当地没有监护条件的证明，由居住地所在街道办事处或镇人民政府审核。（三）街道办事处或镇人民政府反馈审核结果。审核申请人居住地所在街道办事处或镇人民政府在规定时间内告知审核申请人审核结果，为符合条件的适龄儿童少年开具在京就读证明，并在学龄人口信息采集系统或初中入学服务系统上进行确认。二、审核标准（一）在京务工就业证明审核标准。审核申请人及其配偶均需提供在大兴区务工就业证明，须符合下列条件之一：３.审核申请人及其配偶为法定代表人的，应提供法人代码证书或营业执照原件及复印件等相关证明材料。（二）在京实际住所居住证明审核标准。在大兴区实际住所居住证明须符合下列条件之一：１.审核申请人自有住房且正式入住的应提供区住建、规土等部门登记的《房屋所有权证》或《不动产权证书》；尚未取得《房屋所有权证》或《不动产权证书》的，提供商品房买卖网签合同及购房发票（期房以《北京市商品房预售合同》中交房日期在2018年8月31日前为准）。加强对过道房、车库房、空挂户等情况进行核查，凡不符合实际居住条件的，均不得作为入学资格条件。（三）全家户口簿审核标准。1.儿童少年年龄符合当年入学规定，户口簿上年龄与出生证明上年龄须保持一致。2.超龄儿童应提供户籍所在地街道办事处或乡镇人民政府或县级人民政府教育行政部门出具的未在当地就读一年级的证明，并提供相关证明材料（如医院病历或幼儿园证明等）。说明未按时入学原因并填写《非本市户籍超龄儿童情况表》（此表采用大兴区统一印制的规范文本）。（四）北京市居住证（或有效期内居住登记卡）审核标准。1.截至2018年4月28日，审核申请人及其配偶均需连续持有居住地派出所制发的北京市居住证（或有效期内居住登记卡）。2.北京市居住证（或有效期内居住登记卡）地址与在大兴区实际住所居住证明地址一致。3.北京市居住证（或有效期内居住登记卡）信息应为机打，涂改无效。（五）户籍所在地街道办事处或乡镇人民政府出具的在当地没有监护条件的证明审核标准。审核申请人及其配偶均持北京市居住证（或有效期内居住登记卡）的可视为有此证明。]]></content>
      <categories>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Emacs STEP-1]]></title>
      <url>/2017/01/21/Emacs-STEP-1/</url>
      <content type="text"><![CDATA[Emacs学习入门 STEP－1摘自（ 一年成为Emacs高手（像神一样使用编辑器） ）开端  不要着急从Lisp入手，先着手短小练习，培养兴趣，逐步提高－－以实际问题作为切入点  站在巨人的肩膀上          Copy高手的配置文件 世界级大师 Steve Purcell的Emacs 配置      关键是要实干.一个很好的治愈方法就是把http://planet.emacsen.org/ 上约4000篇文章通读一遍      加入社区更上一层楼                  https://www.reddit.com/r/emacs/          https://www.youtube.com/watch?feature=player_embedded&amp;v=oJTwQvgfgMM                    多读书                  https://www.emacswiki.org/          Bob Glickstein的Writing GNU Emacs Extensions          Xah Lee 提供付费Lisp教程          Steve Yegge的Emacs Lisp教程                      第三方插件          Evil	: 将 Emacs变为Vim      Org	: org-mode,全能的笔记工具      company-mode :	自动完成输入,支持各种语言和后端      expand-region	快捷键选中文本,可将选择区域伸缩      smex :	让输入命令变得飞快      yasnippet :	强大的文本模板输入工具      flymake :	对不同语言做语法检查      ivy or helm :	自动完成,在其上有插件完成具体功能      ido	和helm类似,helm和ido可同时用      js2-mode :	javascript的主模式,自带语法解释器      w3m :	网络浏览器(需安装命令行工具w3m)      simple-httpd	: Lisp 写的 Web 服务器      window-numbering.el :	跳转到不同的子窗口      web-mode :	支持各种 HTML 文件      magit :	玩转git      git-gutter.el :	标记版本控制的diff(支持subversion)        学习org-mode : http://www.cnblogs.com/Open_Source/archive/2011/07/17/2108747.html  行动 行动 行动注意  不需要刻意的去记快捷键，随用随记  用 Smex,可飞快输入命令,快捷键实际上不需要了  如下载了第三方插件,如果发觉其有问题,可以在不碰该插件原始代码的情况下修复.  插件服务器关闭了,应可以在自带的U盘上快速建立镜像.其他官方插件仓库https://elpa.gnu.org下载安装插件]]></content>
      <categories>
        
      </categories>
      <tags>
        
          <tag> Emacs </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[使用Vagrant部署Rails项目]]></title>
      <url>/2016/11/04/Using-Vagrant-for-Rails-Development/</url>
      <content type="text"><![CDATA[简述Vagrant 是一个自动化工具，可以在你的电脑的虚拟机里自动搭建一个开发环境。这就意味着你本地开发环境可以完全与生产服务器上保持一致，你的合作小伙伴也可以和你保持高度一致的运行环境。自个儿的Rails的开发环境也不会随着你开发机的变化而变化。另外，你可以在分分钟内启动一个可能需要在12个月以后重新访问的项目，那时你会感谢自己在项目初期使用了这个工具。我们会使用Chef完成虚拟机开发环境的自动部署。chef会负责为我们的系统配置Ruby和所有依赖的包。非常适合快速开发（It’s pretty RAD）。BB的够多的了，让我们开始吧！配置Vagrant首先，开始执行之前确认下本机至少有1G的空闲内存，因为Vagrant会在你的虚拟机中启动一个完整的操作系统，用来执行Rails程序。第一步，在本机安装Vagrant和VirtualBox。  移步这里下载安装Vagrant  移步这里下载安装VirtualBox虚拟机是运行在VirtualBox里的，这些都是后台运行的程序，只能通过SSH进行交互。其次，我们需要安装Vagrant的两个插件。  vagrant-vbguest 自动安装各种Guest扩展程序  vagrant-librarian-chef 让我们可以在机器启动的时候自动执行chefvagrant plugin install vagrant-vbguestvagrant plugin install vagrant-librarian-chef-nochef这个可能需要些时间才能执行完毕。➜  ds git:(dev) vagrant plugin install vagrant-vbguestInstalling the 'vagrant-vbguest' plugin. This can take a few minutes...Installed the plugin 'vagrant-vbguest (0.13.0)'!➜  ds git:(dev) vagrant plugin install vagrant-librarian-chef-nochefInstalling the 'vagrant-librarian-chef-nochef' plugin. This can take a few minutes...Installed the plugin 'vagrant-librarian-chef-nochef (0.2.0)'!➜  ds git:(dev)创建Vagrant配置文件首要的，进入需要配置chef的Rails项目目录，并执行如下命令。➜  ds git:(dev) vagrant initA `Vagrantfile` has been placed in this directory. You are nowready to `vagrant up` your first virtual environment! Please readthe comments in the Vagrantfile as well as documentation on`vagrantup.com` for more information on using Vagrant.➜  ds git:(dev) ✗ touch Cheffile➜  ds git:(dev) ✗ lsCheffile     README.md    app          config.ru    log          tmpGemfile      Rakefile     bin          db           public       vendorGemfile.lock Vagrantfile  config       lib          test由此会生成Vagrantfile和Cheffile两个文件。Cheffile现在，我们可以配置Chef文件了。这个文件和我们的Rails的gem 文件类似，只不过是用于Chef的。这个文件中会定义很多Chef cookbooks用来在稍后的Vagrantfile中指挥Vagrant配置我们的环境的具体内容。我们仅仅只需要黏贴如下代码到Cheffile中：site "http://community.opscode.com/api/v1"cookbook 'apt'cookbook 'build-essential'cookbook 'mysql', '5.5.3'cookbook 'ruby_build'cookbook 'nodejs'cookbook 'rbenv', git: 'https://github.com/aminin/chef-rbenv'cookbook 'vim'VagrantfileVagrantfile 定义了虚拟机中的操作系统和Chef的配置项。我们会使用Ubuntu 16.04 xenial 64-bit和4G的内存这个版本配置（如果你用的32位的系统，只需要改成xenial32即可）。还需要打开虚拟机的3000端口，这样，我们通过本地浏览器就可以访问虚拟机上的Rails服务了。不过至少到现在为止，我们已经在虚拟机中通过Chef配置好了Ruby2.3.1和MySQL。Vagrantfile文件的内容如下：# -*- mode: ruby -*-# vi: set ft=ruby :VAGRANTFILE_API_VERSION = "2"Vagrant.configure(VAGRANTFILE_API_VERSION) do |config|  # Use Ubuntu 16.04 Xenial Xerus 64-bit as our operating system  config.vm.box = "ubuntu/xenial64"  # Configurate the virtual machine to use 4GB of RAM  config.vm.provider :virtualbox do |vb|    vb.customize ["modifyvm", :id, "--memory", "4048"]  end  # Forward the Rails server default port to the host  config.vm.network :forwarded_port, guest: 3000, host: 3000  # Use Chef Solo to provision our virtual machine  config.vm.provision :chef_solo do |chef|    chef.cookbooks_path = ["cookbooks", "site-cookbooks"]    chef.add_recipe "apt"    chef.add_recipe "nodejs"    chef.add_recipe "ruby_build"    chef.add_recipe "rbenv::user"    chef.add_recipe "rbenv::vagrant"    chef.add_recipe "vim"    chef.add_recipe "mysql::server"    chef.add_recipe "mysql::client"    # Install Ruby 2.3.1 and Bundler    # Set an empty root password for MySQL to make things simple    chef.json = {      rbenv: {        user_installs: [{          user: 'vagrant',          rubies: ["2.3.1"],          global: "2.3.1",          gems: {            "2.3.1" =&gt; [              { name: "bundler" }            ]          }        }]      },      mysql: {        server_root_password: ''      }    }  endend执行Vagrant至此，我们已经配置好了Vagrant和Chef。我们将启动Vagrant虚拟主机，并通过ssh登陆进去！# The commented lines are the output you should see when you run these commandsvagrant up#==&gt; default: Checking if box 'ubuntu/xenial64' is up to date...#==&gt; default: Clearing any previously set forwarded ports...#==&gt; default: Installing Chef cookbooks with Librarian-Chef...#==&gt; default: The cookbook path '/Users/chris/code/test_app/site-cookbooks' doesn't exist. Ignoring...#==&gt; default: Clearing any previously set network interfaces...#==&gt; default: Preparing network interfaces based on configuration...#    default: Adapter 1: nat#==&gt; default: Forwarding ports...#    default: 3000 =&gt; 3000 (adapter 1)#    default: 22 =&gt; 2222 (adapter 1)#==&gt; default: Running 'pre-boot' VM customizations...#==&gt; default: Booting VM...#==&gt; default: Waiting for machine to boot. This may take a few minutes...#    default: SSH address: 127.0.0.1:2222#    default: SSH username: vagrant#    default: SSH auth method: private key#    default: Warning: Connection timeout. Retrying...#==&gt; default: Machine booted and ready!#==&gt; default: Checking for guest additions in VM...#==&gt; default: Mounting shared folders...#    default: /vagrant =&gt; /Users/chris/code/test_app#   default: /tmp/vagrant-chef-1/chef-solo-1/cookbooks =&gt; /Users/chris/code/test_app/cookbooks#==&gt; default: VM already provisioned. Run `vagrant provision` or use `--provision` to force itvagrant ssh#Welcome to Ubuntu 16.04 LTS (GNU/Linux 3.13.0-24-generic x86_64)## * Documentation:  https://help.ubuntu.com/## System information disabled due to load higher than 1.0##  Get cloud support with Ubuntu Advantage Cloud Guest:#    http://www.ubuntu.com/business/services/cloud###vagrant@vagrant-ubuntu-xenial-64:~$首次启动vagrant需要很长很长很长时间，因为需要准备chef文件中设置的诸多配置项，之后再次执行的话，不再执行Chef就会快很多了。如果修改了Vagrantfile和Cheffile文件，你需要使用如下命令重新使机器配置生效vagrant provision在Vagrant中使用RailsVagrant会创建一个名字为/vagrant的共享文件夹，可以是虚拟机和本机之间做文件共享。如果进入/vagrant目录下并执行ls命令，会看到Rails项目的所有文件。bundle在虚拟机中安装所有需要的gem。rbenv rehash确保我们刚刚安装的gem是可用的。rake db:create &amp;&amp; rake db:migrate创建和迁移数据库。该目录下的Rails服务器通过3000端口对外提供服务。可以通过localhost:3000访问Rails。结语Vagrant是一个非常Diao的工具，非常方便，通过见得的Chef配置文件，我们可以随时随地使用。如果你需要再次配置一个Vagrant机器，或者是你的合作小伙伴需要配置一个的话，你们只需要执行vagrant up即可。Q&amp;A      执行vagrant up命令失败A：请检查 VirtualBox的版本是否与Vagrant下载扩展的版本一致，我本地使用的是5.0.24 。    ==&gt; default: Machine booted and ready![default] GuestAdditions versions on your host (4.3.28) and guest (5.0.24) do not match.mesg: ttyname failed: Inappropriate ioctl for devicemesg: ttyname failed: Inappropriate ioctl for device        另外Vagrant 1.8.4与 VirtualBox5.1.x版本不兼容。    ➜  ds git:(dev) ✗ vagrant up --provider=virtualboxNo usable default provider could be found for your system.Vagrant relies on interactions with 3rd party systems, known as"providers", to provide Vagrant with resources to run developmentenvironments. Examples are VirtualBox, VMware, Hyper-V.The easiest solution to this message is to install VirtualBox, whichis available for free on all major platforms.If you believe you already have a provider available, make sure itis properly installed and configured. You can see more details aboutwhy a particular provider isn't working by forcing usage with`vagrant up --provider=PROVIDER`, which should give you a more specificerror message for that particular provider.➜  ds git:(dev) ✗ cat /etc/profile            需要安装ruby-dev    sudo apt-get install ruby-dev            PostgresSQL 头文件    sudo apt-get install libpq-dev      ]]></content>
      <categories>
        
      </categories>
      <tags>
        
          <tag> Vagrant </tag>
        
          <tag> Rails </tag>
        
          <tag> Ruby </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Laravel和Dropzone.js实现文件上传]]></title>
      <url>/2016/06/29/Laravel%E5%92%8CDropzone-js%E5%AE%9E%E7%8E%B0%E6%96%87%E4%BB%B6%E4%B8%8A%E4%BC%A0/</url>
      <content type="text"><![CDATA[Dropzone是一个最好的免费库，可以通过拖拽实现文件的上传。拥有很多特性和选项，可以通过多种方式定制开发。在Laravel项目中加入Dropzone对于没有经验的新手来说会有很多麻烦，所以在这里会讲些最靠谱的解决方法。这篇引导会讲到：  文件自动上传（上传队列自动处理）  在预览页面通过ajax直接删除图片  还可以统计上传的图片数量  保存两种图片尺寸：原始尺寸和icon尺寸  Using Image Intervention package for resizing and image encoding  保存图片数据到数据库  服务器端保存唯一的文件名如果正在找一种上传和编辑图片的最贱的方法，可以checkoutCroppic jQuery plugin。Croppic是一个简单的类似Facebook，Twitter或者LinkedIn的照片属性编辑组件。  整个项目可以从 https://github.com/codingo-me/dropzone-laravel-image-upload 获取实现的效果图如下基本的项目配置我将会使用Laravel安装器安装一版纯净的Laravel环境 and create environment file with database credentials.首先从 https://github.com/enyo/dropzone/tree/master/dist 下载最新的js和styles文件。然后放到项目的public/packages/dropzone文件夹下。前端使用Bootstrap。 把相关文件放到public/packages/bootstrap。需要用到两个第三方包：......    "require": {        "php": "&gt;=5.5.9",        "laravel/framework": "5.2.*",        "laravelcollective/html": "5.2.*",        "intervention/image": "^2.3"    },......通过composer install/update安装完成后，需要在同一个文件中添加：Intervention\Image\ImageServiceProvider::class,Collective\Html\HtmlServiceProvider::class,和facades    'Form'      =&gt; Collective\Html\FormFacade::class,    'HTML'      =&gt; Collective\Html\HtmlFacade::class,    'Image'     =&gt; Intervention\Image\Facades\Image::class视图这个引导只包含一个用来上传图片的页面。但是最好能分割成不同的模块：主布局文件和独立的文件；&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt;    &lt;meta charset="utf-8"&gt;    &lt;title&gt;Image upload in Laravel 5.2 with Dropzone.js&lt;/title&gt;    &lt;meta name="viewport" content="width=device-width, initial-scale=1.0"&gt;    {!! HTML::style('/packages/bootstrap/css/bootstrap.min.css') !!}    {!! HTML::style('/assets/css/style.css') !!}    {!! HTML::script('https://code.jquery.com/jquery-2.1.4.min.js') !!}    @yield('head')&lt;/head&gt;&lt;body&gt;&lt;div class="container"&gt;    &lt;div class="navbar navbar-default"&gt;        &lt;div class="navbar-header"&gt;            &lt;button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse"&gt;                &lt;span class="icon-bar"&gt;&lt;/span&gt;                &lt;span class="icon-bar"&gt;&lt;/span&gt;                &lt;span class="icon-bar"&gt;&lt;/span&gt;            &lt;/button&gt;            &lt;a class="navbar-brand" href="/"&gt;Dropzone + Laravel&lt;/a&gt;        &lt;/div&gt;        &lt;div class="navbar-collapse collapse"&gt;            &lt;ul class="nav navbar-nav"&gt;                &lt;li&gt;&lt;a href="/"&gt;Upload&lt;/a&gt;&lt;/li&gt;            &lt;/ul&gt;        &lt;/div&gt;    &lt;/div&gt;    &lt;br&gt;&lt;br&gt;@yield('content')&lt;/div&gt;&lt;/body&gt;@yield('footer')&lt;/html&gt;主要的布局文件非常简单，只是包含底部导航栏和多个balde的section。@extends('layout')@section('head')    {!! HTML::style('/packages/dropzone/dropzone.css') !!}@stop@section('footer')    {!! HTML::script('/packages/dropzone/dropzone.js') !!}    {!! HTML::script('/assets/js/dropzone-config.js') !!}@stop@section('content')    &lt;div class="row"&gt;        &lt;div class="col-md-offset-1 col-md-10"&gt;            &lt;div class="jumbotron how-to-create" &gt;                &lt;h3&gt;Images &lt;span id="photoCounter"&gt;&lt;/span&gt;&lt;/h3&gt;                &lt;br /&gt;                {!! Form::open(['url' =&gt; route('upload-post'), 'class' =&gt; 'dropzone', 'files'=&gt;true, 'id'=&gt;'real-dropzone']) !!}                &lt;div class="dz-message"&gt;                &lt;/div&gt;                &lt;div class="fallback"&gt;                    &lt;input name="file" type="file" multiple /&gt;                &lt;/div&gt;                &lt;div class="dropzone-previews" id="dropzonePreview"&gt;&lt;/div&gt;                &lt;h4 style="text-align: center;color:#428bca;"&gt;Drop images in this area  &lt;span class="glyphicon glyphicon-hand-down"&gt;&lt;/span&gt;&lt;/h4&gt;                {!! Form::close() !!}            &lt;/div&gt;            &lt;div class="jumbotron how-to-create"&gt;                &lt;ul&gt;                    &lt;li&gt;Images are uploaded as soon as you drop them&lt;/li&gt;                    &lt;li&gt;Maximum allowed size of image is 8MB&lt;/li&gt;                &lt;/ul&gt;            &lt;/div&gt;        &lt;/div&gt;    &lt;/div&gt;    &lt;!-- Dropzone Preview Template --&gt;    &lt;div id="preview-template" style="display: none;"&gt;        &lt;div class="dz-preview dz-file-preview"&gt;            &lt;div class="dz-image"&gt;&lt;img data-dz-thumbnail=""&gt;&lt;/div&gt;            &lt;div class="dz-details"&gt;                &lt;div class="dz-size"&gt;&lt;span data-dz-size=""&gt;&lt;/span&gt;&lt;/div&gt;                &lt;div class="dz-filename"&gt;&lt;span data-dz-name=""&gt;&lt;/span&gt;&lt;/div&gt;            &lt;/div&gt;            &lt;div class="dz-progress"&gt;&lt;span class="dz-upload" data-dz-uploadprogress=""&gt;&lt;/span&gt;&lt;/div&gt;            &lt;div class="dz-error-message"&gt;&lt;span data-dz-errormessage=""&gt;&lt;/span&gt;&lt;/div&gt;            &lt;div class="dz-success-mark"&gt;                &lt;svg width="54px" height="54px" viewBox="0 0 54 54" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:sketch="http://www.bohemiancoding.com/sketch/ns"&gt;                    &lt;!-- Generator: Sketch 3.2.1 (9971) - http://www.bohemiancoding.com/sketch --&gt;                    &lt;title&gt;Check&lt;/title&gt;                    &lt;desc&gt;Created with Sketch.&lt;/desc&gt;                    &lt;defs&gt;&lt;/defs&gt;                    &lt;g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd" sketch:type="MSPage"&gt;                        &lt;path d="M23.5,31.8431458 L17.5852419,25.9283877 C16.0248253,24.3679711 13.4910294,24.366835 11.9289322,25.9289322 C10.3700136,27.4878508 10.3665912,30.0234455 11.9283877,31.5852419 L20.4147581,40.0716123 C20.5133999,40.1702541 20.6159315,40.2626649 20.7218615,40.3488435 C22.2835669,41.8725651 24.794234,41.8626202 26.3461564,40.3106978 L43.3106978,23.3461564 C44.8771021,21.7797521 44.8758057,19.2483887 43.3137085,17.6862915 C41.7547899,16.1273729 39.2176035,16.1255422 37.6538436,17.6893022 L23.5,31.8431458 Z M27,53 C41.3594035,53 53,41.3594035 53,27 C53,12.6405965 41.3594035,1 27,1 C12.6405965,1 1,12.6405965 1,27 C1,41.3594035 12.6405965,53 27,53 Z" id="Oval-2" stroke-opacity="0.198794158" stroke="#747474" fill-opacity="0.816519475" fill="#FFFFFF" sketch:type="MSShapeGroup"&gt;&lt;/path&gt;                    &lt;/g&gt;                &lt;/svg&gt;            &lt;/div&gt;            &lt;div class="dz-error-mark"&gt;                &lt;svg width="54px" height="54px" viewBox="0 0 54 54" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:sketch="http://www.bohemiancoding.com/sketch/ns"&gt;                    &lt;!-- Generator: Sketch 3.2.1 (9971) - http://www.bohemiancoding.com/sketch --&gt;                    &lt;title&gt;error&lt;/title&gt;                    &lt;desc&gt;Created with Sketch.&lt;/desc&gt;                    &lt;defs&gt;&lt;/defs&gt;                    &lt;g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd" sketch:type="MSPage"&gt;                        &lt;g id="Check-+-Oval-2" sketch:type="MSLayerGroup" stroke="#747474" stroke-opacity="0.198794158" fill="#FFFFFF" fill-opacity="0.816519475"&gt;                            &lt;path d="M32.6568542,29 L38.3106978,23.3461564 C39.8771021,21.7797521 39.8758057,19.2483887 38.3137085,17.6862915 C36.7547899,16.1273729 34.2176035,16.1255422 32.6538436,17.6893022 L27,23.3431458 L21.3461564,17.6893022 C19.7823965,16.1255422 17.2452101,16.1273729 15.6862915,17.6862915 C14.1241943,19.2483887 14.1228979,21.7797521 15.6893022,23.3461564 L21.3431458,29 L15.6893022,34.6538436 C14.1228979,36.2202479 14.1241943,38.7516113 15.6862915,40.3137085 C17.2452101,41.8726271 19.7823965,41.8744578 21.3461564,40.3106978 L27,34.6568542 L32.6538436,40.3106978 C34.2176035,41.8744578 36.7547899,41.8726271 38.3137085,40.3137085 C39.8758057,38.7516113 39.8771021,36.2202479 38.3106978,34.6538436 L32.6568542,29 Z M27,53 C41.3594035,53 53,41.3594035 53,27 C53,12.6405965 41.3594035,1 27,1 C12.6405965,1 1,12.6405965 1,27 C1,41.3594035 12.6405965,53 27,53 Z" id="Oval-2" sketch:type="MSShapeGroup"&gt;&lt;/path&gt;                        &lt;/g&gt;                    &lt;/g&gt;                &lt;/svg&gt;            &lt;/div&gt;        &lt;/div&gt;    &lt;/div&gt;    &lt;!-- End Dropzone Preview Template --&gt;{!! Form::hidden('csrf-token', csrf_token(), ['id' =&gt; 'csrf-token']) !!}@stop在上传视图的 头部部分 加入Dropzone的缺省样式，在 底部部分 加入Dropzone缺省的js文件，和自己的Dropzone配置文件。内容部分包含两个，第一个是上传表单部分，第二部分是隐藏的预览模板部分。在Dropzone下载了预览模板的代码。配置文件中的模板部分用来显示上传文件的预览。因为需要使用token完成提交和删除路由，所以在内容的底部部分，加入隐藏的csrf字段。Dropzone的配置Dropzone具有很多可用的配置选项。对于这个项目来说，使用Dropzone实现文件的批量顺序上传，就可以避免重复选择需要上传的文件。并行的上传数最大100个，单个文件的尺寸最大设置为8MB。同时上传成功的文件可以在模板html页面中显示预览。每个预览的图片都可以通过点击下方的删除链接删除，这个操作会触发删除文件事件。这个事件会创建一个ajax发起一个删除请求，并且会返回200状态码，图片数也会相应的减少。var photo_counter = 0;Dropzone.options.realDropzone = {    uploadMultiple: false,    parallelUploads: 100,    maxFilesize: 8,    previewsContainer: '#dropzonePreview',    previewTemplate: document.querySelector('#preview-template').innerHTML,    addRemoveLinks: true,    dictRemoveFile: 'Remove',    dictFileTooBig: 'Image is bigger than 8MB',    // The setting up of the dropzone    init:function() {        this.on("removedfile", function(file) {            $.ajax({                type: 'POST',                url: 'upload/delete',                data: {id: file.name, _token: $('#csrf-token').val()},                dataType: 'html',                success: function(data){                    var rep = JSON.parse(data);                    if(rep.code == 200)                    {                        photo_counter--;                        $("#photoCounter").text( "(" + photo_counter + ")");                    }                }            });        } );    },    error: function(file, response) {        if($.type(response) === "string")            var message = response; //dropzone sends it's own error messages in string        else            var message = response.message;        file.previewElement.classList.add("dz-error");        _ref = file.previewElement.querySelectorAll("[data-dz-errormessage]");        _results = [];        for (_i = 0, _len = _ref.length; _i &lt; _len; _i++) {            node = _ref[_i];            _results.push(node.textContent = message);        }        return _results;    },    success: function(file,done) {        photo_counter++;        $("#photoCounter").text( "(" + photo_counter + ")");    }}在配置文件底部，当文件上传成功后，图片计数器会增加。上传逻辑根据功能逻辑划分不同的实现类，在这个项目中，实现了一个__ImageRepository__类，将其放到目录app/Logic/Image/ImageRepository.php中。迄今为止这个类只有2个功能，主要是负责上传单个文件和删除指定文件。在__ImageController__类文件中实现了这个类的实例。&lt;?phpnamespace App\Http\Controllers;use App\Logic\Image\ImageRepository;use Illuminate\Support\Facades\Input;class ImageController extends Controller{    protected $image;    public function __construct(ImageRepository $imageRepository)    {        $this-&gt;image = $imageRepository;    }    public function getUpload()    {        return view('pages.upload');    }    public function postUpload()    {        $photo = Input::all();        $response = $this-&gt;image-&gt;upload($photo);        return $response;    }    public function deleteUpload()    {        $filename = Input::get('id');        if(!$filename)        {            return 0;        }        $response = $this-&gt;image-&gt;delete( $filename );        return $response;    }}因此这个控制器需要添加3个路由，一个用来显示上传表单，一个用来接收上传的文件，第三个用来发起删除请求。Route::get('/', ['as' =&gt; 'upload', 'uses' =&gt; 'ImageController@getUpload']);Route::post('upload', ['as' =&gt; 'upload-post', 'uses' =&gt;'ImageController@postUpload']);Route::post('upload/delete', ['as' =&gt; 'upload-remove', 'uses' =&gt;'ImageController@deleteUpload']);在类文件__ImageController__ 的 postUpload 方法中将用户的输入传入  ImageRepository 类中的 upload 方法中。这个方法  首先会验证输入的合法性  然后会为上传的文件生成服务器唯一的文件名  最后将文件按照原始尺寸和icon的尺寸分别存入服务器不同的目录。同时，他也可以在数据库中创建完整的实体，从而Laravel可以轻松的掌控上传的图片。首先，创建migration文件，并执行migrations。&lt;?phpuse Illuminate\Database\Schema\Blueprint;use Illuminate\Database\Migrations\Migration;class CreateImages extends Migration{    /**     * Run the migrations.     *     * @return void     */    public function up()    {        Schema::create('images', function (Blueprint $table) {            $table-&gt;increments('id');            $table-&gt;text('original_name');            $table-&gt;text('filename');            $table-&gt;timestamps();        });    }    /**     * Reverse the migrations.     *     * @return void     */    public function down()    {        Schema::drop('images');    }}注意到上传的文件会被保存为两种格式，原始尺寸和icon尺寸。使用 Image Intervention package 修改图片尺寸和编码。&lt;?phpnamespace App\Logic\Image;use Illuminate\Support\Facades\Validator;use Illuminate\Support\Facades\Response;use Illuminate\Support\Facades\Config;use Illuminate\Support\Facades\File;use Intervention\Image\ImageManager;use App\Models\Image;class ImageRepository{    public function upload( $form_data )    {        $validator = Validator::make($form_data, Image::$rules, Image::$messages);        if ($validator-&gt;fails()) {            return Response::json([                'error' =&gt; true,                'message' =&gt; $validator-&gt;messages()-&gt;first(),                'code' =&gt; 400            ], 400);        }        $photo = $form_data['file'];        $originalName = $photo-&gt;getClientOriginalName();        $extension = $photo-&gt;getClientOriginalExtension();        $originalNameWithoutExt = substr($originalName, 0, strlen($originalName) - strlen($extension) - 1);        $filename = $this-&gt;sanitize($originalNameWithoutExt);        $allowed_filename = $this-&gt;createUniqueFilename( $filename, $extension );        $uploadSuccess1 = $this-&gt;original( $photo, $allowed_filename );        $uploadSuccess2 = $this-&gt;icon( $photo, $allowed_filename );        if( !$uploadSuccess1 || !$uploadSuccess2 ) {            return Response::json([                'error' =&gt; true,                'message' =&gt; 'Server error while uploading',                'code' =&gt; 500            ], 500);        }        $sessionImage = new Image;        $sessionImage-&gt;filename      = $allowed_filename;        $sessionImage-&gt;original_name = $originalName;        $sessionImage-&gt;save();        return Response::json([            'error' =&gt; false,            'code'  =&gt; 200        ], 200);    }    public function createUniqueFilename( $filename, $extension )    {        $full_size_dir = Config::get('images.full_size');        $full_image_path = $full_size_dir . $filename . '.' . $extension;        if ( File::exists( $full_image_path ) )        {            // Generate token for image            $imageToken = substr(sha1(mt_rand()), 0, 5);            return $filename . '-' . $imageToken . '.' . $extension;        }        return $filename . '.' . $extension;    }    /**     * Optimize Original Image     */    public function original( $photo, $filename )    {        $manager = new ImageManager();        $image = $manager-&gt;make( $photo )-&gt;save(Config::get('images.full_size') . $filename );        return $image;    }    /**     * Create Icon From Original     */    public function icon( $photo, $filename )    {        $manager = new ImageManager();        $image = $manager-&gt;make( $photo )-&gt;resize(200, null, function ($constraint) {            $constraint-&gt;aspectRatio();            })            -&gt;save( Config::get('images.icon_size')  . $filename );        return $image;    }    /**     * Delete Image From Session folder, based on original filename     */    public function delete( $originalFilename)    {        $full_size_dir = Config::get('images.full_size');        $icon_size_dir = Config::get('images.icon_size');        $sessionImage = Image::where('original_name', 'like', $originalFilename)-&gt;first();        if(empty($sessionImage))        {            return Response::json([                'error' =&gt; true,                'code'  =&gt; 400            ], 400);        }        $full_path1 = $full_size_dir . $sessionImage-&gt;filename;        $full_path2 = $icon_size_dir . $sessionImage-&gt;filename;        if ( File::exists( $full_path1 ) )        {            File::delete( $full_path1 );        }        if ( File::exists( $full_path2 ) )        {            File::delete( $full_path2 );        }        if( !empty($sessionImage))        {            $sessionImage-&gt;delete();        }        return Response::json([            'error' =&gt; false,            'code'  =&gt; 200        ], 200);    }    function sanitize($string, $force_lowercase = true, $anal = false)    {        $strip = array("~", "`", "!", "@", "#", "$", "%", "^", "&amp;", "*", "(", ")", "_", "=", "+", "[", "{", "]",            "}", "\\", "|", ";", ":", "\"", "'", "&amp;#8216;", "&amp;#8217;", "&amp;#8220;", "&amp;#8221;", "&amp;#8211;", "&amp;#8212;",            "â€”", "â€“", ",", "&lt;", ".", "&gt;", "/", "?");        $clean = trim(str_replace($strip, "", strip_tags($string)));        $clean = preg_replace('/\s+/', "-", $clean);        $clean = ($anal) ? preg_replace("/[^a-zA-Z0-9]/", "", $clean) : $clean ;        return ($force_lowercase) ?            (function_exists('mb_strtolower')) ?                mb_strtolower($clean, 'UTF-8') :                strtolower($clean) :            $clean;    }}上传的方法中还会引用Image模型中验证规则验证图片类型，只有符合条件的图片可以上传。然后，通过方法 createUniqueFilename 检查下图片原始的文件名称在服务器上是否唯一。如果存在同名的文件，这个方法会在文件名后追加随机字符串。这是最简单的解决办法，当然了，你可以通过其他方法实现。当获取到唯一的文件名称后，将图片和这个文件名传给方法original() ，这个方法负责保存全尺寸的图片，使用icon()方法保存小尺寸图片。当这两个尺寸的图片被保存后，系统会在images表中插入新的记录。图片上传的目录路径被保存在图片配置文件中。同时使用了自己的sanitize()方法，你也可以使用Laravel内部自建的sanitize().为了项目能正常的工作，需要拷贝.env.example 到.env文件，并且修改其中的值。上传目录需要以”/”结束。&lt;?phpreturn [    'full_size'   =&gt; env('UPLOAD_FULL_SIZE'),    'icon_size'   =&gt; env('UPLOAD_ICON_SIZE'),];删除方法会接受文件名，并且检查这个图片是否存在数据库和对应的两个目录下。如果存在则将其删除。p来源  https://tuts.codingo.me/laravel-5-1-and-dropzone-js-auto-image-uploads-with-removal-links#]]></content>
      <categories>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Laravel 环境安装和配置]]></title>
      <url>/2016/06/13/Laravel-%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85%E5%92%8C%E9%85%8D%E7%BD%AE/</url>
      <content type="text"><![CDATA[PHP环境的安装配置下载PHP源码reasonpun@vultr:~$ wget "http://cn2.php.net/get/php-7.1.4.tar.gz/from/this/mirror" php714.tar.gzreasonpun@vultr:~$ tar xvzf php70.tar.gzreasonpun@vultr:~$ cd php-7.0.7/更新环境reasonpun@vultr:~/php-7.0.7$ sudo apt-get update安装组件# debainreasonpun@vultr:~/php-7.0.7$ sudo apt-get install libxml2-devreasonpun@vultr:~/php-7.0.7$ sudo apt-get install build-essentialreasonpun@vultr:~/php-7.0.7$ sudo apt-get install opensslreasonpun@vultr:~/php-7.0.7$ sudo apt-get install libssl-devreasonpun@vultr:~/php-7.0.7$ sudo apt-get install make curlreasonpun@vultr:~/php-7.0.7$ sudo apt-get install libcurl4-gnutls-devreasonpun@vultr:~/php-7.0.7$ sudo apt-get install libjpeg-devreasonpun@vultr:~/php-7.0.7$ sudo apt-get install libpng-devreasonpun@vultr:~/php-7.0.7$ sudo apt-get install libmcrypt-devreasonpun@vultr:~/php-7.0.7$ sudo apt-get install libreadline6reasonpun@vultr:~/php-7.0.7$ sudo apt-get install libreadline6-dev# centos 7[root@i-d5n1vxda data]# yum groupinstall 'Development Tools'[root@i-d5n1vxda data]# yum install gd.x86_64[root@i-d5n1vxda data]# yum install readline.x86_64 readline-devel.x86_64[root@i-d5n1vxda data]# yum install php-mysql.x86_64[root@i-d5n1vxda php-7.1.4]# yum install libwebp.x86_64 libwebp-devel.x86_64# centos7下需要源码安装libmcrypt[root@i-d5n1vxda data]# wget ftp://mcrypt.hellug.gr/pub/crypto/mcrypt/libmcrypt/libmcrypt-2.5.7.tar.gz[root@i-d5n1vxda libmcrypt-2.5.7]# make &amp; make install编译&amp;安装PHPreasonpun@vultr:~/php-7.0.7$ ./configure --prefix=/usr/local/php --with-config-file-path=/usr/local/php/etc --enable-fpm --with-fpm-user=www --with-fpm-group=www --with-mysqli --with-pdo-mysql --with-iconv-dir --with-freetype-dir --with-jpeg-dir --with-png-dir --with-zlib --with-libxml-dir=/usr --enable-xml --disable-rpath --enable-bcmath --enable-shmop --enable-sysvsem --enable-inline-optimization --with-curl --enable-mbregex --enable-mbstring --with-mcrypt --enable-ftp --with-gd --enable-gd-native-ttf --with-openssl --with-mhash --enable-pcntl --enable-sockets --with-xmlrpc --enable-zip --enable-soap --without-pear --with-gettext --disable-fileinfo --enable-maintainer-ztsreasonpun@vultr:~/php-7.0.7$ make &amp;&amp; sudo make install设置php-fpm运行用户reasonpun@vultr:~/php-7.0.7$ cd /usr/local/php/etcreasonpun@vultr:/usr/local/php/etc$ sudo cp php-fpm.conf.default php-fpm.conf# 切换到配置文件目录reasonpun@vultr:/usr/local/php/etc$ cd /usr/local/php/etc/php-fpm.d/reasonpun@vultr:/usr/local/php/etc/php-fpm.d$ sudo vim www.conf.defaultreasonpun@vultr:/usr/local/php/etc/php-fpm.d$ sudo mv www.conf.default www.conf# 修改运行用户user = www-datagroup = www-data# 启动php-fpmreasonpun@vultr:/usr/local/php/etc/php-fpm.d$ sudo /usr/local/php/sbin/php-fpm# 加入系统变量sudo echo "PATH=$PATH:/usr/local/php/bin"&gt;&gt; /etc/profilesudo echo "export PATH"&gt;&gt; /etc/profilesource /etc/profile# 现在看看版本信息吧reasonpun@vultr:/usr/local/php/etc/php-fpm.d$ php -vPHP 7.0.7 (cli) (built: Jun 13 2016 16:12:37) ( ZTS )Copyright (c) 1997-2016 The PHP GroupZend Engine v3.0.0, Copyright (c) 1998-2016 Zend Technologiesreasonpun@vultr:/usr/local/php/etc/php-fpm.d$Composer 安装下载安装文件reasonpun@vultr:/data/web/17tu$ lsreasonpun@vultr:/data/web/17tu$ php -r "copy('https://getcomposer.org/installer','composer-setup.php');"验证文件合法性reasonpun@vultr:/data/web/17tu$ php -r "if (hash_file('SHA384', 'composer-setup.php') === 'bf16ac69bd8b807bc6e4499b28968ee87456e29a3894767b60c2d4dafa3d10d045ffef2aeb2e78827fa5f024fbe93ca2') { echo 'Installer verified'; } else { echo 'Installer corrupt'; unlink('composer-setup.php'); } echo PHP_EOL;"Installer verified执行安装reasonpun@vultr:/data/web/17tu$ php composer-setup.phpAll settings correct for using ComposerDownloading 1.1.2...Composer successfully installed to: /data/web/17tu/composer.pharUse it: php composer.pharreasonpun@vultr:/data/web/17tu$ lscomposer.phar  composer-setup.php删除安装文件reasonpun@vultr:/data/web/17tu$ php -r "unlink('composer-setup.php');"reasonpun@vultr:/data/web/17tu$ lscomposer.phar将composer执行脚本移动到bin目录下reasonpun@vultr:/data/web/17tu$ mv composer.phar /usr/local/bin/composermv: cannot move ‘composer.phar’ to ‘/usr/local/bin/composer’: Permission deniedreasonpun@vultr:/data/web/17tu$ sudo mv composer.phar /usr/local/bin/composer查看命令的可用性reasonpun@vultr:/data/web/17tu$ composerComposer version 1.1.2 2016-05-31 19:48:11Usage:  command [options] [arguments]Options:  -h, --help                     Display this help message  -q, --quiet                    Do not output any message  -V, --version                  Display this application version      --ansi                     Force ANSI output      --no-ansi                  Disable ANSI output  -n, --no-interaction           Do not ask any interactive question      --profile                  Display timing and memory usage information      --no-plugins               Whether to disable plugins.  -d, --working-dir=WORKING-DIR  If specified, use the given directory as working directory.  -v|vv|vvv, --verbose           Increase the verbosity of messages: 1 for normal output, 2 for more verbose output and 3 for debugAvailable commands:  about           Short information about Composer  archive         Create an archive of this composer package  browse          Opens the package's repository URL or homepage in your browser.  clear-cache     Clears composer's internal package cache.  clearcache      Clears composer's internal package cache.  config          Set config options  create-project  Create new project from a package into given directory.  depends         Shows which packages cause the given package to be installed  diagnose        Diagnoses the system to identify common errors.  dump-autoload   Dumps the autoloader  dumpautoload    Dumps the autoloader  exec            Execute a vendored binary/script  global          Allows running commands in the global composer dir ($COMPOSER_HOME).  help            Displays help for a command  home            Opens the package's repository URL or homepage in your browser.  info            Show information about packages  init            Creates a basic composer.json file in current directory.  install         Installs the project dependencies from the composer.lock file if present, or falls back on the composer.json.  licenses        Show information about licenses of dependencies  list            Lists commands  outdated        Shows a list of installed packages that have updates available, including their latest version.  prohibits       Shows which packages prevent the given package from being installed  remove          Removes a package from the require or require-dev  require         Adds required packages to your composer.json and installs them  run-script      Run the scripts defined in composer.json.  search          Search for packages  self-update     Updates composer.phar to the latest version.  selfupdate      Updates composer.phar to the latest version.  show            Show information about packages  status          Show a list of locally modified packages  suggests        Show package suggestions  update          Updates your dependencies to the latest version according to composer.json, and updates the composer.lock file.  validate        Validates a composer.json and composer.lock  why             Shows which packages cause the given package to be installed  why-not         Shows which packages prevent the given package from being installedreasonpun@vultr:/data/web/17tu$Lavarel 安装配置reasonpun@vultr:/data/web/17tu$ composer global require "laravel/installer"Changed current directory to /home/reasonpun/.composerUsing version ^1.3 for laravel/installer./composer.json has been createdLoading composer repositories with package informationUpdating dependencies (including require-dev)  - Installing symfony/process (v3.1.1)    Downloading: 100%  - Installing symfony/polyfill-mbstring (v1.2.0)    Downloading: 100%  - Installing symfony/console (v3.1.1)    Downloading: 100%  - Installing guzzlehttp/promises (1.2.0)    Downloading: 100%  - Installing psr/http-message (1.0)    Downloading: 100%  - Installing guzzlehttp/psr7 (1.3.0)    Downloading: Connecting...Nginx配置编译Nginx源码./configure --sbin-path=/usr/local/nginx/nginx --conf-path=/usr/local/nginx/nginx.conf --pid-path=/usr/local/nginx/nginx.pid --with-http_ssl_module --with-pcre=../pcre-8.40 --with-zlib=../zlib-1.2.11 --lock-path=/var/run/nginx.lock --user=nginx --group=nginx --with-http_ssl_module --with-http_gunzip_module  --with-http_gzip_static_module --with-http_auth_request_module --with-http_v2_modulenginx配置server {    listen 80 default_server;    listen [::]:80 default_server ipv6only=on;    root /your/path/to/lavarel_code;    index index.php index.html index.htm;    server_name 45.32.16.180;    location / {        try_files $uri $uri/ /index.php?$query_string;    }    # PHP 支持    location ~ \.php$ {        try_files $uri /index.php =404;        fastcgi_split_path_info ^(.+\.php)(/.+)$;        fastcgi_pass unix:/var/run/php5-fpm.sock;        fastcgi_index index.php;        fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;        include fastcgi_params;    }}更改目录权限reasonpun@vultr:/data/web/17tu$ sudo chmod -R 777 bootstrap/cachereasonpun@vultr:/data/web/17tu$ sudo chmod -R 777 storage/参考文献  http://www.oschina.net/question/2010961_242272?fromerr=PD31w5Kw  https://getcomposer.org/doc/00-intro.md#installation-linux-unix-osx]]></content>
      <categories>
        
      </categories>
      <tags>
        
          <tag> Laravel </tag>
        
          <tag> PHP </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[玩转 Oryx2 （一）]]></title>
      <url>/2016/04/08/set-up-oryx-env/</url>
      <content type="text"><![CDATA[准备环境  CDH 5.5.2, Parcel          HDFS      YARN      Zookeeper      Kafka      Spark (on YARN)      下载最新版本      进入下载页面，分别下载          compute-classpath.sh      oryx-batch-2.1.2.jar      oryx-run.sh      oryx-serving-2.1.2.jar      oryx-speed-2.1.2.jar            下载conf文件                  以ALS为例，在源码目录oryx/app/conf下als-example.conf                    并将其重命名为oryx.conf（注：文件需要和每个层的JAR文件放在同一个目录下）                    修改conf文件              # 现阶段只需要修改这么几个字段就OK了  kafka-brokers = "data-mining-46.slave:9092,data-mining-47.slave:9092,data-mining-48.slave:9092,data-mining-49.master:9092"  zk-servers = "data-mining-46.slave:2181,data-mining-47.slave:2181,data-mining-48.slave:2181/kafka"  hdfs-base = "hdfs:///Oryx"  oryx {  id = "ALSExample"  als {    rescorer-provider-class = null  }  input-topic {    broker = ${kafka-brokers}    lock = {      master = ${zk-servers}    }  ...开始[root@data-mining-49 oryx]# ./oryx-run.sh kafka-setupInput   ZK      data-mining-46.slave:2181,data-mining-47.slave:2181,data-mining-48.slave:2181/kafka        Kafka   data-mining-46.slave:9092,data-mining-47.slave:9092,data-mining-48.slave:9092,data-mining-49.master:9092        topic   OryxInputUpdate  ZK      data-mining-46.slave:2181,data-mining-47.slave:2181,data-mining-48.slave:2181/kafka        Kafka   data-mining-46.slave:9092,data-mining-47.slave:9092,data-mining-48.slave:9092,data-mining-49.master:9092        topic   OryxUpdateAll available topics:Input topic OryxInput does not exist. Create it? yCreating topic OryxInputError while executing topic command : org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /brokers/ids[2016-04-08 16:53:09,383] ERROR org.I0Itec.zkclient.exception.ZkNoNodeException: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode =NoNode for /brokers/ids        at org.I0Itec.zkclient.exception.ZkException.create(ZkException.java:47)        at org.I0Itec.zkclient.ZkClient.retryUntilConnected(ZkClient.java:995)        at org.I0Itec.zkclient.ZkClient.getChildren(ZkClient.java:675)        at org.I0Itec.zkclient.ZkClient.getChildren(ZkClient.java:671)        at kafka.utils.ZkUtils.getChildren(ZkUtils.scala:537)        at kafka.utils.ZkUtils.getSortedBrokerList(ZkUtils.scala:172)        at kafka.admin.AdminUtils$.createTopic(AdminUtils.scala:243)        at kafka.admin.TopicCommand$.createTopic(TopicCommand.scala:107)        at kafka.admin.TopicCommand$.main(TopicCommand.scala:60)        at kafka.admin.TopicCommand.main(TopicCommand.scala)Caused by: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /brokers/ids        at org.apache.zookeeper.KeeperException.create(KeeperException.java:111)        at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)        at org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1468)        at org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1496)        at org.I0Itec.zkclient.ZkConnection.getChildren(ZkConnection.java:114)        at org.I0Itec.zkclient.ZkClient$4.call(ZkClient.java:678)        at org.I0Itec.zkclient.ZkClient$4.call(ZkClient.java:675)        at org.I0Itec.zkclient.ZkClient.retryUntilConnected(ZkClient.java:985)        ... 8 more (kafka.admin.TopicCommand$)Status of topic OryxInput:yUpdate topic OryxUpdate does not exist. Create it?Creating topic OryxUpdateError while executing topic command : org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /brokers/ids[2016-04-08 16:53:11,094] ERROR org.I0Itec.zkclient.exception.ZkNoNodeException: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /brokers/ids        at org.I0Itec.zkclient.exception.ZkException.create(ZkException.java:47)        at org.I0Itec.zkclient.ZkClient.retryUntilConnected(ZkClient.java:995)        at org.I0Itec.zkclient.ZkClient.getChildren(ZkClient.java:675)        at org.I0Itec.zkclient.ZkClient.getChildren(ZkClient.java:671)        at kafka.utils.ZkUtils.getChildren(ZkUtils.scala:537)        at kafka.utils.ZkUtils.getSortedBrokerList(ZkUtils.scala:172)        at kafka.admin.AdminUtils$.createTopic(AdminUtils.scala:243)        at kafka.admin.TopicCommand$.createTopic(TopicCommand.scala:107)        at kafka.admin.TopicCommand$.main(TopicCommand.scala:60)        at kafka.admin.TopicCommand.main(TopicCommand.scala)Caused by: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /brokers/ids        at org.apache.zookeeper.KeeperException.create(KeeperException.java:111)        at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)        at org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1468)        at org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1496)        at org.I0Itec.zkclient.ZkConnection.getChildren(ZkConnection.java:114)        at org.I0Itec.zkclient.ZkClient$4.call(ZkClient.java:678)        at org.I0Itec.zkclient.ZkClient$4.call(ZkClient.java:675)        at org.I0Itec.zkclient.ZkClient.retryUntilConnected(ZkClient.java:985)        ... 8 more (kafka.admin.TopicCommand$)Error while executing topic command : Topic OryxUpdate does not exist on ZK path data-mining-46.slave:2181,data-mining-47.slave:2181,data-mining-48.slave:2181/kafka[2016-04-08 16:53:11,833] ERROR java.lang.IllegalArgumentException: Topic OryxUpdate does not exist on ZK path data-mining-46.slave:2181,data-mining-47.slave:2181,data-mining-48.slave:2181/kafka        at kafka.admin.TopicCommand$.alterTopic(TopicCommand.scala:119)        at kafka.admin.TopicCommand$.main(TopicCommand.scala:62)        at kafka.admin.TopicCommand.main(TopicCommand.scala) (kafka.admin.TopicCommand$)Status of topic OryxUpdate:[root@data-mining-49 oryx]#此时出现了一个问题：Error while executing topic command : org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /brokers/ids这个问题应该是：CDH没有启动Kafka服务没有启动。通过CDH后台启动Kafka服务（如果没有添加Kafka服务，则需要先添加）。此时重新执行setup命令：[root@data-mining-49 oryx]# ./oryx-run.sh kafka-setupInput   ZK      data-mining-46.slave:2181,data-mining-47.slave:2181,data-mining-48.slave:2181/kafka        Kafka   data-mining-47.slave:9092,data-mining-48.slave:9092        topic   OryxInputUpdate  ZK      data-mining-46.slave:2181,data-mining-47.slave:2181,data-mining-48.slave:2181/kafka        Kafka   data-mining-47.slave:9092,data-mining-48.slave:9092        topic   OryxUpdateAll available topics:Input topic OryxInput does not exist. Create it? yCreating topic OryxInputCreated topic "OryxInput".Status of topic OryxInput:Topic:OryxInput PartitionCount:4        ReplicationFactor:1     Configs:        Topic: OryxInput        Partition: 0    Leader: 140     Replicas: 140   Isr: 140        Topic: OryxInput        Partition: 1    Leader: 141     Replicas: 141   Isr: 141        Topic: OryxInput        Partition: 2    Leader: 140     Replicas: 140   Isr: 140        Topic: OryxInput        Partition: 3    Leader: 141     Replicas: 141   Isr: 141Update topic OryxUpdate does not exist. Create it? yCreating topic OryxUpdateCreated topic "OryxUpdate".Updated config for topic "OryxUpdate".Status of topic OryxUpdate:Topic:OryxUpdate        PartitionCount:1        ReplicationFactor:1     Configs:retention.ms=86400000,max.message.bytes=16777216        Topic: OryxUpdate       Partition: 0    Leader: 140     Replicas: 140   Isr: 140[root@data-mining-49 oryx]#OK！需要将如上的二进制脚本都同步到其他集群的节点上了[root@data-mining-49 data]# scp -r oryx/ root@192.168.1.48:/data/compute-classpath.sh    100% 1893     1.9KB/s   00:00oryx-run.sh             100%   13KB  13.2KB/s   00:00oryx-batch-2.1.2.jar    100%   26MB  26.0MB/s   00:00oryx-serving-2.1.2.jar  100%   33MB  33.0MB/s   00:01oryx-speed-2.1.2.jar    100%   26MB  26.0MB/s   00:00oryx.conf               100%   1884  1.8KB/s    00:00接下来我们需要找一个可用的数据集：MovieLens 100K Dataset，将这个数据集的格式修改下：[root@data-mining-49 ml-100k]# tr '\t' ',' &lt; u.data &gt; data.csv[root@data-mining-49 ml-100k]# tail data.csv806,421,4,882388897676,538,4,892685437721,262,3,877137285913,209,2,881367150378,78,3,880056976880,476,3,880175444716,204,5,879795543276,1090,1,87479579513,225,2,88239915612,203,3,879959583[root@data-mining-49 ml-100k]#数据准备完毕，我们可以通过[root@data-mining-49 oryx]# ./oryx-run.sh kafka-input --input-file data.csv命令导入系统， 同时打开另一个终端窗口，通过命令[root@data-mining-49 oryx]# ./oryx-run.sh kafka-tailInput   ZK      data-mining-46.slave:2181,data-mining-47.slave:2181,data-mining-48.slave:2181/kafka        Kafka   data-mining-47.slave:9092,data-mining-48.slave:9092        topic   OryxInputUpdate  ZK      data-mining-46.slave:2181,data-mining-47.slave:2181,data-mining-48.slave:2181/kafka        Kafka   data-mining-47.slave:9092,data-mining-48.slave:9092        topic   OryxUpdate279,64,1,875308510646,750,3,888528902654,370,2,887863914617,582,4,883789294913,690,3,880824288660,229,2,891406212421,498,4,892241344495,1091,4,888637503806,421,4,882388897676,538,4,892685437721,262,3,877137285913,209,2,881367150378,78,3,880056976880,476,3,880175444716,204,5,879795543276,1090,1,87479579513,225,2,88239915612,203,3,879959583实时跟踪输入导入情况，当数据全部导入完毕后，用户可以手动的通过Ctrl-C关闭这个命令。如果以上全部成功了，可以关闭这些进程。集群至此已经准备好运行Oryx了。启动服务./oryx-run.sh batch./oryx-run.sh speed./oryx-run.sh servingblablabla…..好多输出，先不要管他。确认服务层启动成功以后，可以导入数据（难道刚才导数据是测试用的？）wget --quiet --post-file data.csv --output-document -  --header "Content-Type: text/csv" http://192.168.1.49:8080/ingest批处理层已经开始触发一个新的计算了。此时看看HDFS中的数据是这样的：[root@data-mining-49 oryx]# hdfs dfs -ls /OryxFound 2 itemsdrwxr-xr-x   - hdfs supergroup          0 2016-04-08 18:40 /Oryx/datadrwxr-xr-x   - hdfs supergroup          0 2016-04-08 18:40 /Oryx/model^[[A[root@data-mining-49 oryx]# hdfs dfs -ls /Oryx/dataFound 1 itemsdrwxr-xr-x   - hdfs supergroup          0 2016-04-08 18:40 /Oryx/data/oryx-1460112000000.data[root@data-mining-49 oryx]# hdfs dfs -ls /Oryx/modelFound 3 itemsdrwxr-xr-x   - hdfs supergroup          0 2016-04-08 18:41 /Oryx/model/.checkpointdrwxr-xr-x   - hdfs supergroup          0 2016-04-08 18:40 /Oryx/model/.temporarydrwxr-xr-x   - hdfs supergroup          0 2016-04-08 18:40 /Oryx/model/1460112018440[root@data-mining-49 oryx]#表明数据已经ok了，通过API确认计算是否完成[root@data-mining-49 ~]# wget --quiet --output-document - --server-response http://192.168.1.49:8080/ready  HTTP/1.1 200 OK  Content-Length: 0  Date: Fri, 08 Apr 2016 10:43:27 GMT  Server: OryxOK！我们来查看下推荐结果[root@data-mining-49 ~]# wget --quiet --output-document -  http://192.168.1.49:8080/recommend/1750,0.8234792673029006127,0.7889597890898585181,0.7442612769082189275,0.7263787714764476258,0.7164891492575407121,0.700639194343239115,0.697495789732784288,0.6787936894688755285,0.669647410511970525,0.6603603088587988[root@data-mining-49 ~]# wget --quiet --output-document -  http://192.168.1.49:8080/recommend/806173,1.08470471354667164,1.01426794147118937,0.9939510896801949183,0.991823517833836422,0.964462065137922869,0.945814429782331202,0.942016797140240711,0.9270770070143044135,0.9227914617804345191,0.9215727103874087[root@data-mining-49 ~]#完美结束。这貌似是走的批处理逻辑，中途实时计算层挂鸟16/04/08 18:42:38 INFO SparkContext: Successfully stopped SparkContext16/04/08 18:42:38 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, data-mining-47.slave): java.io.IOException: org.apache.spark.SparkException: Failed to get broadcast_0_piece0 of broadcast_0        at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1177)        at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:165)        at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:64)        at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:64)        at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:88)        at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)        at org.apache.spark.scheduler.Task.run(Task.scala:88)        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)        at java.lang.Thread.run(Thread.java:745)Caused by: org.apache.spark.SparkException: Failed to get broadcast_0_piece0 of broadcast_0        at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1$$anonfun$2.apply(TorrentBroadcast.scala:138)        at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1$$anonfun$2.apply(TorrentBroadcast.scala:138)        at scala.Option.getOrElse(Option.scala:120)咱们下一篇再找找原因吧。参考文献  http://oryx.io/docs/admin.html  http://oryx.io/docs/endusers.html  http://oryx.io/docs/developer.html  http://oryx.io/docs/performance.html  http://oryx.io/apidocs/index.html]]></content>
      <categories>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Oryx2 性能优化文档]]></title>
      <url>/2016/04/07/Oryx2-performance-doc/</url>
      <content type="text"><![CDATA[这里收集了各种意见，经验法则和基准测试相关的性能：做这些不同的工作需要多少资源。硬件和集群设计一般情况下，对硬件或者集群没有特别的要求。集群资源的需求主要取决于基于Spark的作业，这些往往是内存密集型和CPU密集型的，但是一般不会是I/O绑定的。如果数据的采集率非常的高，Kafka可能需要一些特殊的考虑。在这两种情况下，针对其他任何的Kafka或者Spark作业并没有不同的标准。数据传输因为Kafka是底层传输数据的，存储要求的采集和存放数据对于Kafka也就是这样的。查看Kafka 性能中提到的内容。一般情况下，Kafka并不会接近瓶颈，也能够像其他Kafka的使用一样调整资源占用大小。批处理层批处理层独特的地方是模型的构建, and the element that is of most interest to benchmark are likely the model building processes implemented in the app tier, on top of MLlib. Here again, the resources required to build a model over a certain amount of data are just that of the underlying MLlib implementations of ALS, k-means and decision forests.MLlib的任何性能优化或者是基准测试都适用于这些基于MLlib预制的实现的批处理层，这对于Oryx也是一样的。JVM 优化Choosing the number of Spark executors, cores and memory is a topic in its own right.很自然的，更多的executors意味着更多的核心和内存。但是数量是不能超多集群上机器的总数量的；请查看：oryx.batch.streaming.num-executors。更多的核，意味着可能会有更多的并发进程。在典型的模型构建进程中，如果能达到任务总数的三分之一或者2分之一的话，这将是非常有用的。你可以观察到任务的数量，以及Spark UI中批处理层固有的并发情况。在这数量之下，核数再多也无法增加更多的并发了。少一些是可以的，无非就是增加了点运行时间。当然，在批处理的时间间隔内，充足的核数可以保证批处理的顺利完成。核的数量可以通过oryx.batch.streaming.executor-cores进行配置。如果你的作业发生了内存溢出，驱动器和executors可能需要更多的内存。如果你注意到在批处理层的”存储”标签页中并不是100%的RDD被缓存住了，那么更多的内存可能会有所帮助。查看：oryx.batch.streaming.executor-memory。oryx-run.sh脚本的–jvm-args参数是用来为所有的JVM进程设置内存参数的。举个例子，通过设置 -XX:+UseG1GC ，会达到一个比较合理的效果。服务层REST API 后端服务器是Tomcat。配置文件是没有暴露给用户的，但是对于他的负载已经做了合理的调整。Tomcat容器本身开销很小，不需要担心性能问题。最可能感兴趣的是项目中提供的关于CPU密集型程序实现的性能问题，而不是框架本身。基准测试: 交替最小二乘法推荐由于ALS实现的程序的服务层的大多数的操作是，会在内存中实时的计算一个非常大的矩阵，所以这个程序是最具有挑战性的了。大概其的规则如下：如果需要运行类似的基准测试，可以使用LoadBenchmark，可以按照如下的配置：mvn -DskipTests clean installcd app/oryx-app-serving...mvn -Pbenchmark \ -Doryx.test.als.benchmark.users=1000000 \ -Doryx.test.als.benchmark.items=5000000 \ -Doryx.test.als.benchmark.features=250 \ -Doryx.test.als.benchmark.lshSampleRate=0.3 \ -Doryx.test.als.benchmark.workers=2 \ integration-test内存  内存需求是成线性的：(users + items) x features  -XX:+UseStringDeduplication 在 Java 8中是非常有用的 (reflected below)  At scale, 1M users or items ~= 500-1000M of heap required, depending on featuresExample steady-state heap usage (Java 8):            Features      Users+Items (M)      Heap (MB)                  50      2      1400              50      6      2600              50      21      7500              250      2      3000              250      6      7500              250      21      25800      请求延迟，吞吐量  Recommend and similarity computation time scales linearly with items x features  A single request is parallelized across CPUs; max throughput and minimum latency is already achieved at about 1-2 concurrent requests  Locality sensitive hashing decreases processing time roughly linearly; 0.33 ~= 1/0.33 ~= 3x faster (setting too low adversely affects result quality)Below are representative throughput / latency measurements for the /recommend endpoint using  a 32-core Intel Xeon 2.3GHz (Haswell), OpenJDK 8 and flags -XX:+UseG1GC -XX:+UseStringDeduplication. Heap size was comfortably large enough for the data set in each case. The tests were run with 1-3 concurrent request at a time, as necessary to achieve near-full CPU utilization.With LSH (sample rate = 0.3)            Features      Items (M)      Throughput (qps)      Latency (ms)                  50      1      437      7              250      1      151      13              50      5      84      24              250      5      36      56              50      20      14      69              250      20      6      162      Without LSH (sample rate = 1.0)            Features      Items (M)      Throughput (qps)      Latency (ms)                  50      1      74      27              250      1      23      44              50      5      13      80              250      5      5      191              50      20      4      282              250      20      1      708      JVM 优化机器上运行（多个）服务层，使用越多的可用的核，意味着可以提供更多的并发请求处理能力。在ALS中，一些请求，比如/recommend 可以在一个请求中通过多核计算完成。内存的需求主要是由加载进内存的模型的需求控制的。对于大的模型，比如ALS，这就意味着需要确保服务层有足够的内存以保证不会导致GC崩溃。查看oryx.serving.memory。-XX:+UseG1GC remains a good garbage collection setting to supply with –jvm-args. In Java 8, -XX:+UseStringDeduplication can reduce memory requirements by about 20%.实时计算层实时计算层驱动进程类似服务层那样也是需要大量内存的，因为他也是需要加载一个模型到内存中的。驱动进程的内存是通过oryx.speed.streaming.driver-memory控制的，也需要和服务层内存一样设置，并且也需要JVM的一些flags支持。这也是一个Spark Streaming 作业，也需要想批处理层那样配置executors。一般情况下，需要更少的处理和更低的时间延迟。Executors will have to be sized to consume input Kafka partitions fully in parallel; the number of cores times number of executors should be at least the number of Kafka partitions.]]></content>
      <categories>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Oryx2 终端用户文档]]></title>
      <url>/2016/04/06/Oryx2-end-user-doc/</url>
      <content type="text"><![CDATA[运行注意：你必须已经按照管理员文档中提到的配置好了你的集群。下载最新的Oryx版本，包括批处理层，实时计算层和服务层的jar文件和sh脚本。或者，源码编译他们并从deploy/bin/获取最新的脚本。拷贝二进制和脚本到hadoop集群的机器上。他们可以会被部署到不同的机器，或者是被部署到一个测试机器上。实时和批处理层应该运行且只能运行在一台机器上（The Speed and Batch Layers should run on at most one machine, each），服务层则可以运行于多个节点上。创建一个配置文件，可以简单的拷贝例子中的conf/als-example.conf。并修改host名称，端口和目录。实际上，选择hdfs上已经存在的数据和模型目录便于用户运行Oryx 二进制命令。拷贝该配置文件，并重命名为oryx.conf，将他放到每个机器的上二进制命令和脚本相同的目录下。执行如下命令开始这3个层的运行：./oryx-run.sh batch./oryx-run.sh speed./oryx-run.sh serving参数–layer-jar your-layer.jar and –conf your-config.conf可以指定某一层特定位置的jar文件和配置文件。也可以使用–jvm-args直接传递更多的参数给Spark驱动程序，比如：–jvm-args=”-Dkey=value”这都不需要在同一台机器上，但是也不一定（如果配置特殊指定批处理和实时处理层，服务层API不同的端口）。服务层可以运行在多个机器上。举个例子，批处理层SparkUI运行在启动脚本所在的机器的4040端口（除非通过配置更改）。一个简单的基于web端的控制台的服务层默认是运行在8080端口的。完美！尝试下ALS的例子吧如果你已经使用了上述的配置，你就已经可以运行一个基于ALS的推荐程序实例。自获取GroupLens 100K的数据集，并且找到u.data文件，这个文件的内容需要转换成csv格式：tr '\t' ',' &lt; u.data &gt; data.csv将这些数据放入服务层，使用本地的命令行工具，如下：wget --quiet --post-file data.csv --output-document - \  --header "Content-Type: text/csv" \  http://your-serving-layer:8080/ingest如果你使用tail命令查看输入的内容，可以看到如下数据：196,242,3.0,881250949186196,242,3.0,881250949186,302,3.0,89171774222,377,1.0,878887116244,51,2.0,880606923166,346,1.0,886397596298,474,4.0,884182806...很快的，你也可以看到批处理层已经开始触发一个新的计算了。这个例子被配置为5分钟一个周期。数据首先会被写入HDFS。默认配置被写入hdfs:///user/example/Oryx/data/目录下。且目录以时间戳命名，每一部分都包含Hadoop part-r-* 文件，都是以文本的序列话文件的方式存储。虽然不是纯文本，打印出来的话，还是有一部分是可以识别的，因为这其实真的是文本。SEQorg.apache.hadoop.io.Textorg.apache.hadoop.io.Text����^�]�XسN�22,377,1.0,87888711662...模型计算开始。这些批处理层会以大量的新的分布式的作业形式展现。在这个例子中，Spark UI可以通过http://your-batch-layer:4040访问。模型计算是非常快的，执行完毕以后会合并PMML和支持数据文件并存储到目录hdfs:///user/example/Oryx/model/下。举个例子，model.pmml 的内容如下：&lt;?xml version="1.0" encoding="UTF-8" standalone="yes"?&gt;&lt;PMML xmlns="http://www.dmg.org/PMML-4_2" version="4.2.1"&gt;    &lt;Header&gt;        &lt;Application name="Oryx"/&gt;        &lt;Timestamp&gt;2014-12-18T04:48:54-0800&lt;/Timestamp&gt;    &lt;/Header&gt;    &lt;Extension name="X" value="X/"/&gt;    &lt;Extension name="Y" value="Y/"/&gt;    &lt;Extension name="features" value="10"/&gt;    &lt;Extension name="lambda" value="0.001"/&gt;    &lt;Extension name="implicit" value="true"/&gt;    &lt;Extension name="alpha" value="1.0"/&gt;    &lt;Extension name="XIDs"&gt;56 168 222 343 397 ...     ...The X/ and Y/ subdirectories next to it contain feature vectors, like:[56,[0.5746282834154238,-0.08896614131333057,-0.029456222765775263,  0.6039821219690552,0.1497901814774658,-0.018654312114339863,  -0.37342063488340266,-0.2370768843521807,1.148260034028485,  1.0645643656769153]][168,[0.8722769882777296,0.4370416943031704,0.27402044461549885,  -0.031252701117490456,-0.7241385753098256,0.026079081002582338,  0.42050973702065714,0.27766923396205817,0.6241033215856671,  -0.48530795198811266]]...如果使用tail命令查看更新的内容。这些数据会很快放入服务层，此时访问/ready会返回200 OK。wget --quiet --output-document - --server-response \  http://your-serving-layer:8080/ready...  HTTP/1.1 200 OK  Content-Length: 0  Date: Tue, 1 Sep 2015 13:26:53 GMT  Server: Oryxwget --quiet --output-document -  http://your-serving-layer:8080/recommend/17...50,0.7749542842056966275,0.7373013861581563258,0.731818692628511181,0.7049967175706345127,0.704518989947498121,0.701463102979374115,0.6954683387287907288,0.677488971102402225,0.6663619887033064285,0.6398968471343595恭喜！实时推荐系统搭建完毕！可以通过Ctrl-C关闭。API手册Oryx 支持多种端到端的程序，包括服务层的REST 接口。协同过滤和推荐  /recommend  /recommendToMany  /recommendToAnonymous  /recommendWithContext  /similarity  /similarityToItem  /knownItems  /estimate  /estimateForAnonymous  /because  /mostSurprising  /popularRepresentativeItems  /mostActiveUsers  /mostPopularItems  /mostActiveUsers  /item/allIDs  /ready  /pref  /ingest分类 / 回归  /predict  /classificationDistribution  /ready  /train聚类  /assign  /distanceToNearest  /ready  /add配置  app/conf/als-example.conf  app/conf/kmeans-example.conf  app/conf/rdf-example.conf]]></content>
      <categories>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Oryx2 开发者文档]]></title>
      <url>/2016/04/06/Oryx2-developer-doc/</url>
      <content type="text"><![CDATA[环境需求  git, 或者一个支持git的IDE  Apache Maven 3.2.1 或者更新版本  Java JDK (不能只有JRE) 7 或者更新版本以上需要已经安装到了你的开发环境中。Building克隆代码到你本地，并且编译：git clone https://github.com/OryxProject/oryx.git oryxcd oryxmvn -DskipTests package会编译出如下的二进制的jar文件：  批处理层: deploy/oryx-batch/target/oryx-batch-2.1.2.jar  实时处理层: deploy/oryx-speed/target/oryx-speed-2.1.2.jar  服务层: deploy/oryx-serving/target/oryx-serving-2.1.2.jar友情提醒，如果你对开发Oryx感兴趣，可以根据这个分支克隆自己的分支，然后就可以提交修改了。Java 8如果使用Java8编译，需要添加参数-Pjava8 并且测试这里的指令。Platform Only默认的编译包括基于Spark MLlib和其他库的端到端的ML程序。如果只是编译lambda和ML层，需要通过参数-P!app-tier关闭其他的选项。注意，在bash中，！需要转义： -P!app-tier。测试mvn 测试会执行所有的但愿测试用例。也同时会执行所有的集成测试，这个可能会需要稍微长点的时间。模型对应表主要的模型和他们对应的层：                   Serving      Speed      Batch                  Binary      oryx-serving      oryx-speed      oryx-batch              App      oryx-app-serving      oryx-app-mllib oryx-app      oryx-app-mllib oryx-app              ML             oryx-ml      oryx-ml              Lambda      oryx-lambda-serving      oryx-lambda      oryx-lambda      支持的模型，比如：oryx-common, oryx-app-common, oryx-api, and oryx-app-api 没有在这里列出了。实现一个Oryx 程序Oryx 中的“app 层”，是实现了推荐的真实的批处理，实时，服务层逻辑，集群和分类。然而，任何实现都需要使用到Oryx。他们也可以混合和匹配。举个例子，你可以重新实现ALS-related推荐的批处理层，但是仍然使用原来的ALS的服务层和实时计算层。创建一个程序在每个例子中，创建一个自定义的批处理层，实时计算层或者服务层的程序都需要实现com.cloudera.oryx.api中的几个关键的Java接口或者Scala的特性。这些接口/特性可以在项目的oryx-api模型中找到。                   Java      Scala                  Batch      batch.BatchLayerUpdate      batch.ScalaBatchLayerUpdate              Speed      speed.SpeedModelManager      speed.ScalaSpeedModelManager              Serving      serving.ServingModelManager      serving.ScalaServingModelManager      com.cloudera.oryx.api也包含大量的关键的类和接口，举个例子，serving.OryxResource  是一个入口，用来编译自定义的JAX-RS 端点，但是不需要使用。编译程序进入你的程序的这些接口/特性，添加一个com.cloudera.oryx:oryx-api的依赖，scope字段需要填写“provided”，在Maven中，需要添加如下依赖：In Maven, this would mean adding a dependency like:&lt;dependencies&gt;  &lt;dependency&gt;    &lt;groupId&gt;com.cloudera.oryx&lt;/groupId&gt;    &lt;artifactId&gt;oryx-api&lt;/artifactId&gt;    &lt;scope&gt;provided&lt;/scope&gt;    &lt;version&gt;2.1.2&lt;/version&gt;  &lt;/dependency&gt;&lt;/dependencies&gt;这些artifacts被放在了Cloudera这个分支下，因此在编译的时候需要引用这个分支：&lt;repositories&gt;  &lt;repository&gt;    &lt;id&gt;cloudera&lt;/id&gt;    &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt;  &lt;/repository&gt;&lt;/repositories&gt;一个最小的实例可以访问example/ 这里。”Word Count” 这个程序是按照空格将行分割成独立的单词，然后统计出排重后的单词出现的次数。编译代码后生成一个JAR文件，包含了程序实现和所有第三方的diam，如果使用Maven，可以通过mvn package命令。编译 Word Count 例子举例：编译样例代码如下：cd app/examplemvn package生成的JAR包在target/example-2.1.2.jar。自定义Oryx程序当发布一个源自Oryx的预打包程序，在某些情况下，有可能会提供一个扩展的实现，从而可以自定义他们的行为。举例，ALS推荐程序暴露了com.cloudera.oryx.app.als.RescorerProvider接口。这些特定程序API类可以在模块oryx-app-api中找到。这些接口的实现也可以在独立模式下被编译，打包，部署。&lt;dependencies&gt;  &lt;dependency&gt;    &lt;groupId&gt;com.cloudera.oryx&lt;/groupId&gt;    &lt;artifactId&gt;oryx-app-api&lt;/artifactId&gt;    &lt;scope&gt;provided&lt;/scope&gt;    &lt;version&gt;2.1.2&lt;/version&gt;  &lt;/dependency&gt;&lt;/dependencies&gt;发布程序拷贝生成的JAR文件–myapp.jar，放到需要执行的Oryx 二进制JAR文件相同的目录下。修改Oryx的配置文件，以便于引用自定义的批处理，实时计算和服务层的实现。当执行批处理，实时计算和服务层时，需要添加–app-jar myapp.jar到oryx-run.sh命令行中。发布 Word Count 例子举例，如果已经编译好了上述的“word count”的程序，你可以执行这个程序，直接引用wordcount-example.conf这个配置文件：./oryx-run.sh batch --conf wordcount-example.conf --app-jar example-2.1.2.jar… 对于实时计算和服务层也是同样的。curl -X POST http://.../add/foo%20bar%20baz...curl http://.../distinct{"foo":2,"bar":2,"baz":2}配置文件本身已经配置好了主机名称和Cloudera Quickstart VM的参数。事实上，这个例子可以作为一个集群配置的例子：Cloudera Quickstart VM Setup。]]></content>
      <categories>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Java关键字volatile]]></title>
      <url>/2016/03/08/Java-keyword-volatile/</url>
      <content type="text"><![CDATA[翻译自：http://tutorials.jenkov.com/java-concurrency/volatile.htmlJava Volatile 关键字Java关键字volatile标识一个变量“被存储在主内存中”。更准确的说法是：每次volatile变量会从主内存中读取，而不是从CPU缓存；每次volatile变量的写操作会写入主内存，而不仅仅是CPU缓存。实际上，自Java5开始，volatile关键字保证的是volatile声明的变量都是写入和读取自主内存的。以下将会详细讲解。Java volatile保证可见性Java中volatile关键字保证了线程之间变量修改的可见性。这个可能会有点抽象，因此让我详细讲解下。在多线程程序中，由于性能的原因，在操作非volatile变量的时候，每个线程会将变量自主内存拷贝🈯️CPU缓存。如果你的计算机是多核的，每个线程就会运行在不同的CPU上。也就是说，每个线程会拷贝变量到不同的CPU缓存中。见插图：使用非volatile变量，当JVM自主内存读取数据到CPU缓存，或者自CPU缓存写入主内存时，是没有保证的。这可能会引起很多问题，接下来章节中会降到。设想一种情况，多个线程可以访问类似如下的同一个共享对象，这个对象包含一个计数器变量：public class SharedObject {    public int counter = 0;}另一个设想，只有线程1改变计数变量，但是线程1和线程2都可能经常读取这个计数变量。如果计数变量没有被声明为volatile，那么自CPU缓存回写到主内存的过程中，就无法保证这个计数变量的值了。也就是说，技术变量的值在通过CPU缓存写会主内存的过程中发生了改变。如图：这个问题是因为线程并没有得到最新的值，另外那个线程并没有将最新的变量写回主内存，这就是所谓的“可见”问题。某一个线程的更新并没有被另外的线程获取。通过声明计数变量为volatile，那么这个计数变量所有的更新都会立即被写回主内存。同时，所有的读操作都会直接通过主内存。如下既是如何声明计数变量为volatile类型：public class SharedObject {    public volatile int counter = 0;}声明一个变量为volatile由此可以保证了变量的写操作对于其他线程都是可见的。The Java volatile Happens-Before Guarantee自Java5开始，volatile关键字保证的不止是变量的主内存读和写，实际上还包含：      如果线程A写入一个volatile变量，此时线程B读取了相同的volatile变量，那么此时所有的变量在写入volatile变量前对线程A是可见的，同时也对线程B在读取了volatile变量后也是可见的。        volatile变量的读写指令不能促使JVM重新排序（只要JVM发现排序不会改变程序的行为，基于性能的原因，JVM会进行排序）。前后的指令会被排序，但是volatile得读写不会混入这些指令。无论如何，指令都会follow一个volatile变量的读写，并且保证这种操作发生在读或者写之后。  这个需要更深入的解释。当一个线程写入一个volatile变量时，并不止这个volatile变量自己写入了主内存。同时所有被线程修改的变量在写入volatile变量之前都会被回写到主内存。当一个线程读取一个volatile变量时，他会同时读取主内存中所有其他的变量，这些变量都是和volatile变量一同写入主内存的。看例子：// Thread A:    sharedObject.nonVolatile = 123;    sharedObject.counter     = sharedObject.counter + 1;// Thread B:    int counter     = sharedObject.counter;    int nonVolatile = sharedObject.nonVolatile;因为线程A在写入volatile变量sharedObject.counter之前写入了非volatile变量sharedObject.nonVolatile，那么当线程A写入sharedObject.counter时，sharedObject.nonVolatile 和 sharedObject.counter 都会被写入主内存。由于线程B开始的时候读取volatile类型的sharedObject.counter，那么sharedObject.counter and sharedObject.nonVolatile都会使用线程B自主内存读取到CPU缓存。等到线程B读取sharedObject.nonVolatile时，就会看到线程A写入的值。开发者可能会使用这种扩展的可见性，保证了线程之间变量的可见性。只需要声明一个或者非常少的volatile变量替换掉每个变量都声明为volatile。如下是一个例子：public class Exchanger {    private Object   object       = null;    private volatile hasNewObject = false;    public void put(Object newObject) {        while(hasNewObject) {            //wait - do not overwrite existing new object        }        object = newObject;        hasNewObject = true; //volatile write    }    public Object take(){        while(!hasNewObject){ //volatile read            //wait - don't take old object (or null)        }        Object obj = object;        hasNewObject = false; //volatile write        return obj;    }}线程A通过调用put()持续的写入对象。线程B通过take()持续的获取对象。这个类只有在线程A调用put()和线程B调用take()时，通过使用volatile变量才能工作正常。如果JVM在不改变排序指令的语义的基础上实现，那么JVM则会通过记录JAVA指令优化性能。while(hasNewObject) {    //wait - do not overwrite existing new object}hasNewObject = true; //volatile writeobject = newObject;注意到volatile变量hasNewObject在被实际设置前已经被执行。对于JVM，这看上去完全合法。这两个写入变量的值相对于另外一个是独立的。然而，排序执行的指令会有损对象变量的可见性。首先，线程B可能在线程A设置新值之前将hasNewObject设置为true。其次，甚至没法保证当新值写入对象后回写到主内存。为了防止如上述情况的发生，volatile关键字采用“发生前保证”机制。”发生前保证“机制保证读和写volatile变量的指令不能被排序。前后指令可以被排序，但是volatile读或者写指令发生前后不能被排序。举个例子：sharedObject.nonVolatile1 = 123;sharedObject.nonVolatile2 = 456;sharedObject.nonVolatile3 = 789;sharedObject.volatile     = true; //a volatile variableint someValue1 = sharedObject.nonVolatile4;int someValue2 = sharedObject.nonVolatile5;int someValue3 = sharedObject.nonVolatile6;JVM会排序前3个指令，只要他们在volatile写指令之前。同样的，只要volatile写入质量发生在后三个指令之前，那么后三个指令就会被排序。在此之前，这三个指令中任何一个指令都不会被排序。That is basically the meaning of the Java volatile happens before guarantee.volatile 并不总是适用的即使volatile关键字保证了所有的volatile变量直接从主内存读取，所有的写操作都是直接写入主内存，但是存在声明了volatile而仍然不够的情况。之前提过的，只有线程1写入共享的counter变量，声明为volatile的时候才能确保线程2总是能获取最新的写入值。事实上，多线程甚至能写入一个共享的volatile变量，如果这个新写入的值不依赖于旧数据的话就将正确的值存入主内存。换句话说, 如果一个线程写入一个值到共享的volatile变量，并不会马上将读取的值用于下一个值得话。一旦一个线程首次读取一个volatile变量的值，并且需要基于这个值生成一个新的共享volatile变量值，那么这个volatile变量就不能保证正确的可见性。读取volatile变量的值并且写入新值得很短的间隙，就会创建一个竞争的条件：多个线程会读取同一个volatile变量的值，并且生成一个新的值，当回写进主内存后就会覆盖掉其他的值。多线程增加相同的计数器值得这种情况，准确的说对于volatile变量是不适用的。一下片段讲话详细讲述这种情况。设想，如果线程1读取共享计数变量，他的值是0，并放入CPU的缓存，使其加1，但是这样的改变并不会回写如主内存。线程2此时可以自主内存读取相同的计数器变量，但是读到的值仍然是0，并将这个值放入了自己的CPU缓存。线程2也可以对其加1，并且可以回写到主内存。这种情况详见下图：线程1和线程2此时并不是同步的。这个共享计数器的值应该是2，但是每个线程在自己的CPU缓存中存放的值却是1，在主内存中的值是0.这已经完全乱了！即使最后每个线程回写这个共享计数变量的值到主内存，这个值也是错误的。volatile在什么时候是适用的?就像之前提到过的，如果是两个线程都需要读取和写入一个共享变量，那么此时适用volatile关键字是不适用的。那么就需要使用synchronized来保证读和写的原子性。读写volatile变量不会阻塞线程的读写。为了实现这一点，你必须使用synchronized关键字。作为synchronized可供选择，需要采用java.util.concurrent包中的某一个。比如AtomicLong，AtomicReference或者其他的任意一个。在这种情况下，只有一个线程读写volatile变量的值，其他线程只是读取变量，那么读取线程就可以通过volatile变量保证可以读取到最新的写入值。如果没有volatile变量，则这个过程就不能被保证。volatile关键字可以被用在32和64位变量中。volatile 性能注意事项volatile变量的读写会触发变量在主内存中的读写。主内存中的读写会比CPU缓存消耗的代价高。使用volatile变量同样会阻止普通性能增强指令的排序。因此，只能在确实需要增加变量可见性基础上使用volatile变量。参考文献  http://www.ibm.com/developerworks/cn/java/j-jtp06197.html  http://www.infoq.com/cn/articles/ftf-java-volatile  http://www.infoq.com/cn/articles/java-memory-model-4  http://sakyone.iteye.com/blog/668091  https://zh.wikipedia.org/wiki/Volatile%E5%8F%98%E9%87%8F  http://www.cnblogs.com/aigongsi/archive/2012/04/01/2429166.html]]></content>
      <categories>
        
      </categories>
      <tags>
        
          <tag> Java </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[正则表达式处理Nginx]]></title>
      <url>/2016/01/24/Reg-Nginx/</url>
      <content type="text"><![CDATA[Nginx 日志配置格式log_format  main        '[$upstream_addr] $remote_addr [$time_local] "$request" $status '        '"$request_body" $body_bytes_sent "$http_referer" "$http_user_agent" '        'RESP:$upstream_response_time '        'REQ:$request_time';样例[192.168.1.5:80] 19.78.22.51 [31/Dec/2015:13:59:02 +0800] "POST /api/mbbb/dup_msg_send?pallow_dubbing=0&amp;partner_msgs_id=279&amp;roles_id=23&amp;score=6700&amp;section_id=512&amp;whole_audio=200.m4a&amp;device_id=112222f0fc3&amp;lang=zh-CN&amp;trigger=user&amp;user_id=516487&amp;v=ios_7.0.3 HTTP/1.1" 200 "audio_fragment=00280%22%3A%7B%22pitch%22%3A47%2C%22rhythm%22%3A95%2C%22tone%22%3A75%7D%2C%226800278%22%3A%7B%22pitch%22%3A70%2C%22rhythm%22%3A90%2%7D%2C%226800276%22%3A%7B%22pitch%22%3A60%2C%22rhythm%22%3A82%2C%22tone%22D&amp;content=hhhhhh%E5%B0%8F%E5%AB%A9%E8%8D%89" 51 "-" "paipao/7.0.3 (iPhone; iOS 9.2; Scale/2.00)" RESP:0.166 REQ:0.167[192.168.1.5:80, 192.169.1.33:88] 60.12.246.5 [31/Dec/2015:23:59:02 +0800] "GET /api/mppb/notice/list?device_id=112233fe6a6d991f&amp;lang=zh-CN&amp;user_id=6120&amp;v=ios_7.0.3 HTTP/1.1" 200 "-" 54 "-" "paipao/7.0.3 (iPhone; iOS 9.2; Scale/2.00)" RESP:0.006 REQ:0.006正则表达式Python版本p = re.compile(            r"\[\-?[\d.\:]*[\ \,]*?.*?\]\ [\d.\:]*\ \[(\d+)/(\w+)/(\d+)\:(\S+)\ [\S]+\]\ \"(\S+)\ (\S+)\ .*?\"\ (\d+)\ \"(.*?)\"\ (\d+)\ \"([^\"]*)\"\ \".*?\" .*?")m = re.findall(p, line)day = m[0][0]month = m[0][1]year = m[0][2]ttime = m[0][3]method = m[0][4]request = m[0][5]status = m[0][6]Scala版本val regex = new Regex( """\[\-?[\d.\:]*[\ \,]*?.*?\]\ [\d.\:]*\ \[(\d+)/(\w+)/(\d+)\:(\S+)\ [\S]+\]\ \"(\S+)\ (\S+)\ .*?\"\ (\d+)\ \"(.*?)\"\ (\d+)\ \"([^\"]*)\"\ \".*?\" .*?""")val regex(day, month, year, time, method, request, status, postData, bytes, refer) = line参考文献  http://www.jianshu.com/p/5d8c802be13d  https://segmentfault.com/a/1190000002727070  http://desert3.iteye.com/blog/1001568  http://stackoverflow.com/questions/996536/regex-in-python]]></content>
      <categories>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Spark优化]]></title>
      <url>/2016/01/04/Spark-tuning/</url>
      <content type="text"><![CDATA[Spark 优化由于Spark内存计算特性，Spark程序会由集群上的如下因素决定其性能  CPU  网络带宽  内存通常来说，如果配置适当的内存，那么瓶颈就是带宽。但是有些时候，有需要做些优化，比如以序列化的形式存储RDD，从而降低内存的占用。此会从两方面分析  数据序列化  内存优化Data Serialization序列化在分布式计算程序中占有非常重要的地位。迟缓的序列化格式，或者消费一个超大的字节数据，都会大大的减缓计算速度。所以，第一件事应该先尝试优化Spark程序。Spark试图达到在易用性（允许你定义任何的Java类型）和性能两方面达到一种平衡，并且提供两种序列化库：      Java序列化：缺省情况下，Spark使用Java的ObjectOutputStream框架序列化对象，  从而可以和任何实现了java.io.Serializable接口的类一起玩耍。也可以更紧密的控制扩展了java.io.Externalizable的序列化的性能。Java的序列化是非常丰富的，但是速度奇慢，并且会导致很多类产生大量的序列化格式。        Kryo序列化：Spark也可以使用Kryo库（Version2）非常迅速的序列化对象。Kryo比Java序列化快很多并且更大的压缩率（通常是10x），但是并不支持所有的类型，另外也需要你在程序中注册类以获得最好的性能。  你可以选择通过设置SparkConf或者调用conf.set(“spark.serializer”, “org.apache.spark.serializer.KryoSerializer”)来使用Kryo初始化Job。这样设置的话，不仅可以配置通过worker节点间shuffling数据，还能将RDD序列化到磁盘上。Kryo不是缺省配置的原因是由于自定义注册的要求决定的，同时我们也想在网络密集型应用中使用它。Spark automatically includes Kryo serializers for the many commonly-used core Scala classes covered in the AllScalaRegistrar from the Twitter chill library.使用Kryo注册自定义类，需要使用registerKryoClasses方法。  val conf = new SparkConf().setMaster(...).setAppName(...)  conf.registerKryoClasses(Array(classOf[MyClass1], classOf[MyClass2]))  val sc = new SparkContext(conf)Kryo文档描述了更多高级的注册选项，比如添加自定义序列代码。如果你的对象非常大，则需要增加spark.kryoserializer.buffer配置参数。这个值缺省为2，但是这个值需要足够大足以保存序列号的对象。最后，如果你没有注册自定义类，Kryo仍然会起作用，但是它不得不存储每个对象的全部类，这样是非常浪费的。内存优化内存优化的方法有3个方面需要考虑的：你的对象使用的总内存量（你可能希望全部数据都放到内存里），对象存取成本，GC的开销（如果你有非常高的对象交换频率）默认情况下，Java对象可以被快速的访问，但是会轻易的耗尽它们字段中比“raw”数据多2到5倍的空间的因子。这取决于多个因素：  每个独立的Java对象包含一个“对象头”，16字节长度并包含诸如只想它的类的一些信息。  对于一个非常小的数据来说（比如Int），这个可能比数据本身都要大一些。  Java字符串有大约40字节的对象头，比“raw”数据要多（因为它们按照字符串数组的形式保存，并且还包含扩展数据，比如说数据的长度），  由于采用了UTF-16编码格式，所以在字符串内部，存储一个字符需要占用2个字节的空间。因此10个字符就可以轻易的消耗掉60字节空间。  普通的集合对象，比如说HashMap和LinkedList，使用的是链式数据结构，对于每个实体都存在一个“wrapper”对象（比如 Map.Entry）。这个对象不仅包含头，链表中还有指向下一个对象的指针（通常需要8个字节）。  私有类型的集合经常以“装箱”对象的形式存储，比如java.lang.Integer。这一章讨论的是如何确定对象的内存占用情况，和改进的方法－不止保护改变你的数据结构，另外还需要以一种序列号的形式存储数据。然后我们就可以覆盖到优化Spark缓存和Java GC的知识了。搞清楚内存消耗测试一个数据集消耗内存的总量最好的方式创建一个RDD，并放到缓冲中，通过web页面查看存储情况。这个页面的内容可以显示RDD到底占有了多少内存。估算一个分区数据消耗的内存，可以使用SizeEstimator’s estimate的方法，这是一种非常有用的方式去试验不同的数据结构去减少内存的使用，即可以确定广播变量占用空间可以消耗每个可执行堆的情况。优化数据结构首选的降低内存消耗的方法是避免使用具有Java的特性导致的开销，比如基于指针的数据结构核包装类。有多种方式可以做到：  设计你的数据结构以提升对象数组和原始类型，用来替换标准的Java或者Scala的集合类（比如：HashMap）。  fastutil类库为原始类型提供了适当的集合类型，可以兼容Java的标准库。  尽量避免包含大量小对象核指针的嵌套数据结构。  考虑使用数字类型的ID或者枚举对象来替代字符串行的key。  如果你的RAM不足32GB，可以通过设置JVM的-XX:+UseCompressedOops参数，修改指针默认占用8字节为4个字节。也可以讲这些参数加到spaspark-env.sh中。序列化的RDD存储通过以上的优化，你的对象仍然很大从而影响到高效的存储的话，一种更加简单的减少内存使用的方法是通过RDD的持久化API序列化StorageLevels，从而使他们以序列化的方式存储，比如MEMORY_ONLY_SER。Spark这时就可以将每个RDD作为一个大字节数组分区存储。唯一的不足是，序列化存储的数据每次读取的时候会很慢，这个取决于每个对象的反序列化（on the fly）。我们强烈建议使用Kryo作为缓存序列化数据的方法，这样可以比Java序列化占用更少的空间（甚至是原始的Java对象）GC优化JVM 的GC可能会是一个问题，当你的程序在存储一个RDD方面存在一个很大的『churn』。（他不会是一个大问题，如果只是每次读取一个RDD，并多次操作）当Java需要逐渐的用新对象替换旧对象时，GC就会追踪你的所有Java对象，找到并替换掉。这里的重点指出的，GC的执行成本是与Java对象的个数成正比的，因此使用更少对象的数据结构可以大大降低GC的成本（比如一个Int类型的数组替换LinkedList的数组）。更好的方法是持久化被序列化之后的对象，如上述，在每个RDD分区里只会存在一个对象（一个字节数组）。在尝试其他技术之前，如果GC是一个问题，那么第一件事就是尝试序列化的缓存。你的任务的占用的活动内存核缓存在你节点上的RDD之间的干扰也会导致GC出现问题（需要执行任务所需的内容总量）。我们将会讨论怎样控制分配给RDD缓存的空间以减少这种影响。衡量GC的影响GC优化的第一步需要先手机统计数据：GC发生的频率和发费的时间。这个可以通过添加设置java参数-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps实现。（详情见传递Java参数到Spark任务的指南）下一次你的Spark任务执行时，你就可以在每个worker的日志中看到关于GC事件的信息。注意，这些日志时存放到worker节点上的，并不是在driver 程序上。缓冲大小优化GC一个重要的配置参数是分配给缓存RDD使用的内存总量。缺省情况下，Spark会使用配置给executor memory (spark.executor.memory) 60%的内存量缓冲RDD。也就是说40%的内存是用于在任务执行过程中任何其它的对象。当你的任务速度缓慢，并且发现JVM GC频率特别频繁或者出现内存溢出的问题时，降低这个值会对降低内存消耗提供帮助。修改这个值可以通过设置spark.storage.memoryFraction，比如修改成50%，则可以conf.set(“spark.storage.memoryFraction”, “0.5”)。结合使用序列化缓存，使用更少的缓存就可以充分的减轻大部分的GC问题。高级GC优化更进一步的GC优化，我们首先需要理解一些基本的JVM管理内存管理知识：  Java堆空间被划分为两个部分Young 和 Old。Young generation用来保存短周期的对象，同时Old generation是用来保持长周期对象。  Young generation被进一步划分为3部分：Eden, Survivor1, Survivor2。  一个简单的关于垃圾回收程序的描述：当Eden满了的时候，a minor GC is run on Eden and objects that are alive from Eden and Survivor1 are copied to Survivor2。Survivor部分是可以交换的。如果一个对象已经旧了或者Survivor2满了，他就会被移动到旧的部分。最后，当旧的部分接近满的时候，一个满的GC就会被唤起。Spark中GC优化的目标是确保只有需要长久存在的RDD才会被存储在Old generation，Young generation有足够的空间处处短周期的对象。这就可以帮助避免full GCs在任务执行过程中收集临时任务。这里的一些步骤可能会比较有用：  通过检查GC的状态检查是否存在多个垃圾回收。如果一个full GC在一个任务结束之前被多次唤醒，这就意味着没有足够的可用内存涌来执行任务。  In the GC stats that are printed，如果OldGen接近满的状态的话，就减少缓存的内存总量。这个可以通过设置spark.storage.memoryFraction property来达成。缓存更少的对象总比减慢任务执行速度要好的多！  如果存在很多的次要collections而不是很多的主要的GCs，给Eden分配更多的内存会有所帮助。你可以设置Eden稍微高些的内存给每个任务。如果Eden表示成E，那么可以通过设置Young generation的参数大小为-Xmn=4/3*E。（The scaling up by 4/3 is to account for space used by survivor regions as well.）  举个例子，如果你的任务正在从HDFS中读取数据，任务使用的内存被标记为数据的块大小。  注意，解压后的数据块大约是之前数据的2～3倍。因此我们希望有3或者4个任务的工作空间，并且HDFS块的大小是64M，我们就可以估算出Eden大小大约是4364MB。  通过修改新的设置来监测垃圾回收的频率和时间我的经验得出GC优化的改进取决于你的程序和可用的内存总量。在网上有很多优化选项的描述，但是再进一步，管理full GC的频率可以有助于降低顶层限制。其它需要考虑的事情并行的水平节点不会被充分利用，除非喂每个操作设置了足够高的并行水平。Spark会自动设置“map”任务的数量执行每个文件以和他的大小保持一致（尽管你可以通过SparkContext.textFile的可选参数控制），并且对于分布式的“reduce”操作，比如groupByKey 和 reduceByKey，会使用最大的父RDD的分区数。你可以传递并行的level作为第二个参数，活着设置spark.default.parallelism配置属性改变默认值。通常情况下，我们建议在节点上每个CPU内核执行2～3个任务。降低任务的内存使用有时，你可能得到OutOfMemoryError错误，但是这并不是因为你的Rdd不适合你的内存，而是因为任务中的其中一个的设置，比如其中一个reduce任务执行groupByKey时太大。Spark的shuffle操作（sortByKey, groupByKey, reduceByKey, join等）会创建一个哈希表并且每个任务会执行grouping操作，这个通常情况下也会很大。此时最简单的解决办法是增加并行的水平，以降低任务的输入集大小。Spark可以高效的支持任务，因为它可以重复利用一个executor JVM，并且有很低的任务加载消耗，所以你可以安全的增加并发水平，甚至超过你节点上核心的数目。广播大变量使用SparkContext上可用的广播功能可以明显的减少每个序列化任务的大小，和通过节点加载任务的消耗。如果你的任务使用来自driver程序的大的对象（比如：静态查找表），试着转化成一个广播变量。Spark打印出主节点上每个任务的序列化后的大小，从而你可以通过这个决定你的任务是否过大；通常的任务大于20KB就值得优化了。数据的位置数据的位置可能是影响Spark作业的最主要的一个影响因素。如果数据和代码被放置在一起的话，计算速度会明显加快。但是如果代码和数据是分开的，不同的两块数据会被移动到一起。特别是，传输序列化的代码比数据块要快很多，这是因为代码的大小比数据小很多。Spark构建调度的主要原则就是由数据的位置决定的。数据的位置到底距离操作他的代码多近的距离才合适呢？这里存在多个级别（自近及远）：  PROCESS_LOCAL  数据和代码在同一个JVM中，这种方式是最好的一种情况  NODE_LOCAL 数据在同一个节点。比如说在HDFS的同一个节点，或者相同节点的不同executor。  这种方式比PROCESS_LOCAL稍微慢些，因为这种情况下，数据需要在不同进程中传输  NO_PREF data is accessed equally quickly from anywhere and has no locality preference  RACK_LOCAL 数据在同一个服务器机架。数据在同一个机架的不同的服务器上的话，数据需要通过网络进行传输，typically through a single switch  ANY 数据分布在网络的各个地方，并且不在同一个机架Spark更倾向于调度最好的locality级别的所有任务，但是这并不是经常发生的。在这种情况下，不在处理数据的任何空闲executor，Spark都会选择更低的locality级别。这有两种选择：  等待同一个服务器上的任务，直到对应的CPU闲下来才执行  立即执行一个新任务，并通过移动数据到更远的节点执行Spark通常会稍微等一下，以等待CPU执行完毕。一旦超时，他就会将远端的数据传递给空闲的CPU。不同级别的等待超时回退可以单独配置，或者在一个参数中集中修改；前往配置页查看spark.locality参数详情。如果你的任务are long and see poor locality，你也可以修改设置，但是缺省值是完全可以满足要求的。总结这是一个简短的介绍，其中指出了你可能需要了解的优化Spark应用的最主要的要点，数据序列化和内存优化。对于大多数的程序来说，选择Kryo序列化，并且以序列化形式保存数据可以解决大部分一般的性能问题。Feel free to ask on the Spark mailing list about other tuning best practices.]]></content>
      <categories>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Python关键字:yield]]></title>
      <url>/2016/01/02/Python-keyword-yield/</url>
      <content type="text"><![CDATA[注：以下代码实验环境均为➜  ~  pythonPython 2.7.10 (default, Oct 23 2015, 18:05:06)[GCC 4.2.1 Compatible Apple LLVM 7.0.0 (clang-700.0.59.5)] on darwinType "help", "copyright", "credits" or "license" for more information.Yield关键字理解yield关键字之前需要先明白什么是迭代。可迭代对象建立一个列表之后，可以逐项的读取列表中的元素，这就是一个可迭代的对象：&gt;&gt;&gt; list = [1, 2, 3, 4]&gt;&gt;&gt; for i in list:...     print i...1234&gt;&gt;&gt;使用列表生成器建立一个列表，同样也是创建了一个可迭代的对象：&gt;&gt;&gt; list = [x * 2 for x in range(3)]&gt;&gt;&gt; list[0, 2, 4]&gt;&gt;&gt; for i in list:...     print i...024&gt;&gt;&gt;可以使用for .. in .. 的方式处理：链表，字符串等，这就叫做一个迭代器。但是这样会将数据存放到内存中，如果数据量过大的话，就非常不适合了。生成器生成器是可以迭代的，但是你 只可以读取它一次 ，因为它并不把所有的值放在内存中，它是实时地生成数据&gt;&gt;&gt; g = (x * 2 for x in range(3))&gt;&gt;&gt; g&lt;generator object &lt;genexpr&gt; at 0x10978ca00&gt;&gt;&gt;&gt; for i in g:...     print i...024&gt;&gt;&gt; for i in g:...     print i...&gt;&gt;&gt;&gt;&gt;&gt;yield关键字yield 是一个类似return的关键字，但是这个函数返回的是生成器。同时我们可以利用 isgeneratorfunction 判断一个特殊的 generator 函数&gt;&gt;&gt; mg = createGenerator()&gt;&gt;&gt; print (mg)&lt;generator object createGenerator at 0x10978c9b0&gt;&gt;&gt;&gt; for i in mg:...     print i...024&gt;&gt;&gt; from inspect import isgeneratorfunction&gt;&gt;&gt; isgeneratorfunction(mg)False&gt;&gt;&gt; isgeneratorfunction(createGenerator)True&gt;&gt;&gt;需要注意的是：当你调用这个函数的时候，函数内部的代码并不立马执行 ，而是返回一个生成器对象。只有当使用for进行迭代的时候，函数内的代码才会执行。控制资源访问&gt;&gt;&gt; class Bank():...     crisis = False...     def createAtm(self):...             while not self.crisis:...                     yield "$100"...&gt;&gt;&gt;&gt;&gt;&gt; bank = Bank()&gt;&gt;&gt; atm = bank.createAtm()&gt;&gt;&gt; print(atm.next())$100&gt;&gt;&gt; print(atm.next())$100&gt;&gt;&gt; print(atm.next())$100&gt;&gt;&gt; print(atm.next())$100&gt;&gt;&gt; atm.crisis = TrueTraceback (most recent call last):  File "&lt;stdin&gt;", line 1, in &lt;module&gt;AttributeError: 'generator' object has no attribute 'crisis'&gt;&gt;&gt; bank.crisis = True&gt;&gt;&gt; print (atm.next())Traceback (most recent call last):  File "&lt;stdin&gt;", line 1, in &lt;module&gt;StopIteration&gt;&gt;&gt;如果不使用yield的话，这个类执行createAtm之后就会导致系统资源耗尽（或者说是无限循环）使用yield可以是程序非常优美，虽然python程序本身就会很优美生成斐波那契（Fibonacci）数列许多初学者都会这么写&gt;&gt;&gt; def fab(max):...     n, a, b = 0, 0, 1...     while n &lt; max:...             print b...             a, b = b, a + b...             n = n + 1...&gt;&gt;&gt; fab(5)11235&gt;&gt;&gt;使用yield改写一个版本&gt;&gt;&gt; def fab(max):...     n, a, b = 0, 0, 1...     while n &lt; max:...             yield b...             a, b = b, a + b...             n = n + 1...&gt;&gt;&gt;&gt;&gt;&gt; for n in fab(5):...     print n...11235&gt;&gt;&gt;简单描述下for 语句在碰到生成器 generator 的时候，调用generator.__next__()获取生成器的返回值。__next__()以Fibonacci为例：for每次调用，可以理解为执行了一次generator() 执行到 yield 的时候，生成器返回了n的值并停止。这就像普通函数碰到 return 时一样，剩下的代码都被忽略了。不同的地方在于，python 会记录这个停止的位置。 当再次执行generator()的时候，python 从这个停止位置开始执行而不是开头， 也就是说这次返回了1。再执行generator()，则返回2，当执行到返回5的时候，已经没有 yield 语句了， 就抛出了 StopIteration 。这和其他迭代器是类似的，当然在for中是不会抛出异常的。进阶在PEP 342中加入了将值传给生成器的支持。PEP 342加入了新的特性，能让生成器在单一语句中实现，生成一个值（像从前一样），接受一个值，或同时生成一个值并接受一个值。我们用前面那个关于素数的函数来展示如何将一个值传给生成器。这一次，我们不再简单地生成比某个数大的素数，而是找出比某个数的等比级数大的最小素数（例如10， 我们要生成比10，100，1000，10000 … 大的最小素数）。&gt;&gt;&gt; import math&gt;&gt;&gt; def getPrimes(number):...     while True:...             if isPrime(number):&gt;&gt;&gt;                     '''...                     yield关键字返回number的值，...                     而other = yield foo "返回foo的值，...                     这个值返回给调用者的同时，将other的值也设置为那个值"。...                     可以通过send方法来将一个值”发送“给生成器。&gt;&gt;&gt;                     '''...                     number = yield number...             number += 1...&gt;&gt;&gt; def printSuccessivePrimes(iterations, base=10):...     printGenerator = getPrimes(base)&gt;&gt;&gt;     '''...     用send来“启动”一个生成器时（就是从生成器函数的第一行代码执行到第一个yield语句的位置），必须发送None。因为此时生成器还没有走到第一个yield语句，如果send一个真实的值，这时是没有人去“接收”它的。一旦生成器启动了，我们就可以像上面那样发送数据了&gt;&gt;&gt;     '''...     printGenerator.send(None)...     for power in range(iterations):&gt;&gt;&gt;             '''...             打印的是generator.send的结果，send在发送数据给生成器的同时还返回生成器通过yield生成的值&gt;&gt;&gt;             '''...             print(printGenerator.send(base ** power))...&gt;&gt;&gt; def isPrime(number):...     if number &gt; 1:...             if number == 2:...                     return True...             if number % 2 == 0:...                     return False...             for current in range(3, int(math.sqrt(number) + 1), 2):...                     if number % current == 0:...                             return False...             return True...     return False...&gt;&gt;&gt;&gt;&gt;&gt; printSuccessivePrimes(10)2111011009100071000031000003100000191000000071000000007&gt;&gt;&gt;Python 使用yield的关键思想  generator是用来产生一系列值的  yield则像是generator函数的返回结果  yield唯一所做的另一件事就是保存一个generator函数的状态  generator就是一个特殊类型的迭代器（iterator）  和迭代器相似，我们可以通过使用next()来从generator中获取下一个值  通过隐式地调用next()来忽略一些值参考文献  http://pyzh.readthedocs.org/en/latest/the-python-yield-keyword-explained.html  http://www.dabeaz.com/coroutines/index.html  https://docs.python.org/2/tutorial/classes.html  http://stackoverflow.com/questions/231767/what-does-the-yield-keyword-do-in-python  http://www.pydanny.com/python-yields-are-fun.html  http://dhcmrlchtdj.github.io/sia/post/2012-11-20/python_yield.html  http://www.oschina.net/translate/improve-your-python-yield-and-generators-explained  http://www.pythonclub.org/python-basic/yield  https://www.ibm.com/developerworks/cn/opensource/os-cn-python-yield/  https://www.jeffknupp.com/blog/2013/04/07/improve-your-python-yield-and-generators-explained/  http://pythontips.com/2013/09/29/the-python-yield-keyword-explained/  http://blog.jobbole.com/28506/]]></content>
      <categories>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Oryx2 简介]]></title>
      <url>/2015/12/21/Oryx2-Overview/</url>
      <content type="text"><![CDATA[简介Oryx2是专注于进行大规模，实时机器学习框架，遵循lambda规则，基于Apache Spark和Apache Kafka构建。Oryx 不仅是构建应用程序的框架，而且包含 协同过滤，分类，回归和聚类的打包的端到端的应用。包含3层  lambda层          批量处理      快速处理      服务        ML抽象层  端到端实现层从另一个角度，可以看成是一系列链接的元素  批处理层：依据历史数据进行离线处理  实时处理层：通过增量数据流，实时更新结果  服务层：通过模型的传递实现异步查询API  数据传输层：在外部数据源与处理层之间传输数据The project  may be reused tier by tier for example, the packaged app tier can be ignored, and it can be a framework for building new ML applications.  It can be reused layer by layer too: for example, the Speed Layer can be omitted if a deployment does not need incremental updates.  It can be modified piece-by-piece too: the collaborative filtering application’s model-building batch layer could be swapped for a custom implementation based on a new algorithm outside Spark MLlib while retaining the serving and speed layer implementations.Lambda层实现数据传输数据传输机制其实就是一个Kafka的Topic。任何一个进程（包含且不局限与服务层）都可以向topic中写入数据，并通过实时处理和批处理层查看。  Kafka Topic也可以用来模型和模型之间的更新，并被实时处理层和服务层消费。批处理层批处理层是以Spark Streaming进程的方式实现的，运行在Hadoop Cluster节点上，并读取来自Kafka topic的输入数据。 Streaming 进程会有一个很长的运行周期-若干小时甚至一天。会使用Spark存储当前会话数据到HDFS中，然后合并HDFS上的所有历史数据，之后重新初始化构建新的结果数据。并将新的结果重新写入HDFS，同时发不到Kafka更新topic中。实时处理层实时处理层也是由Spark Streaming进程实现的，同样读取Kafka topic输入数据。但是他存在比较短的运行周期，比如秒级别。会持续消费更新topic中的新模型，并生产新的模型。也会回写更新topic。服务层服务层监听更新topic上的模型以及模型更新。在内存中持久化模型状态。  会暴露顶层方法的 HTTP REST API 用于查询内存中的模型。大部分接口都支持大规模的部署。  每个接口都可以接收新的数据并写入Kafka，以此在实时处理层和批处理层可见。配置和部署程序是基于Java实现的，依赖    * Spark 1.3.x+    * Hadoop 2.6.x+    * Tomcat 8.x+    * Kafka 0.8.2+    * Zookeeper 等。配置文件通过 Typesafe Config 的方式实现整个系统的部署配置。  包括： 批处理，实时处理，服务层逻辑关键的接口类的实现每个层的二进制形式分开进行打包和部署的，每个都是以可执行的Java的jar包的形式存在并包含所有必须的服务。ML层实现ML层对上述通用接口方法做了简单的专一话的实现，实现了通用ML需求，并且对应用暴露了机器学习特有的接入接口。举个例子，实现了批量处理层，用于自动更新测试集和训练集进程。可以调用应用提供的函数来评估测试机模型。通过尝试不同的超参数值，选择出最佳结果。通过PMML管理模型的序列号。端到端应用实现除了作为一种框架，Oryx2 包含完整的三中机器学习需要的批处理层，实时处理层，服务层。  开箱即用，或者作为自定义程序的基础：* 基于最小二乘法的协同过滤/推荐* 基于k-means的聚类* 基于随机决策森林的分类和回归参考文献  http://oryx.io/index.html  http://www.ivanopt.com/oryx-document%E7%BF%BB%E8%AF%91/  http://jameskinley.tumblr.com/post/37398560534/the-lambda-architecture-principles-for  http://dmg.org/pmml/v4-1/GeneralStructure.html  http://blog.csdn.net/nxcjh321/article/details/24796879  http://youngfor.me/post/recsys/oryx-tui-jian-xi-tong-chu-ti-yan  https://github.com/OryxProject/oryx]]></content>
      <categories>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Oryx2 管理员文档]]></title>
      <url>/2015/12/21/Oryx2-Admin-Docs/</url>
      <content type="text"><![CDATA[Oryx 2.1.0 系统要求  Java 7 or later (JRE only is required)  A Hadoop cluster running the following components:          Apache Hadoop 2.6.0 or later      Apache Zookeeper 3.4.5 or later      Apache Kafka 0.8.2 or later (in 0.8.x line)      Apache Spark 1.5.0 or later      服务Hadoop cluster 服务  HDFS  YARN  Zookeeper  Kafka  Spark (on YARN)Note that for CDH, Kafka is available as a parcel from Cloudera Labs.Kafka brokers 需要配置在集群中，根据实例，需要注意hosts和端口。端口一般会设置为9092，此处和ZK server的端口保持一致，缺省设置为2181。缺省端口会在随后的例子中使用。多个hosts需要通过逗号分割，并需要提供host:port 这种方式，比如 ： your-zk-1:2181,your-zk-2:2181。同时需要注意，你的ZK实例是否使用了chroot path。这是一个简单的路径前缀，比如 your-zk:2181/your-chroot/kafka 是经常用到作为前缀的。如果没有使用chroot，就可以忽略这个。注意：如果存在多个ZK server，和一个chroot，只需要在最后添加一次chroot即可，比如 your-zk-1:2181,your-zk-2:2181/kafka配置KafkaOryx 使用2个Kafka topics 做数据传输。  一个传输输入数据到批量处理，和实时计算层  另一个同步模型更新到服务层缺省的topics的名称分别为OryxInput 和 OryxUpdate，只有Oryx服务启动后才能创建这两个topics。输入topic的分区数目会影响消费数据的Spark Streaming 作业的分区数，甚至是并发数。比如，批量处理层读取HDFS上的历史数据分区和Kafka数据。如果输入topic只有一个分区，且在每个时间间隔内有大量的数据涌入，这时Kafka基于的输入分区相对的需要很长的时间去处理。一个比较合适的经验值是选择一些topic分区，在一个批处理时间间隔内到达的大量数据，大约是一个HDFS块大小，缺省值是128MB。提供的oryx-run.sh kafka设置脚本，缺省设置为4个分区，当然了，这个值之后是可以修改的。必须注意不能设置更新topic多于1个分区。重复因子可以设置为任何值，但是建议最少是2。注意：重复因子数目不能超过Kafka brokers在集群上的数目。所以提供的设置脚本里缺省设置重复因子为1. 之后你可以通过kafka-topics –zookeeper … –alter –topic … –replication-factor N 等修改这些缺省值。你需要为其中一个或者两个topic配置持续时间。尤其重要的是需要限制更新topic的持续时间，因为实时计算层和服务层需要从启动开始的起点获取整个topic。这个机制并不如输入数据重要，输入数据不会再次从头读取数据。设置这个值为批量处理层更新间隔的2倍是比较合适。比如设置该值为1天（24 * 60 * 60 * 1000 = 86400000 ms），设置topic的为86400000ms。这个可以通过oryx-run.sh设置脚本自动设置。上述两个topics会包含大量信息；尤其是更新topic包含整个序列化的PMML模型。很有可能他会超过Kafka缺省最大消息的大小（kafka消息最大1Mib）。如果有更大的数据则需要设置topic’s max.message.bytes。oryx-run.sh Kafka设置脚本设置更新topic缺省为16Mib。这也是Oryx试图吸入更新topic里的模型的最大值默认；更大的模型只会以文件的形式保存到HDFS中的路径中。请查看属性oryx.update-topic.message.max-size。Kafka代理的message.max.bytes属性可以控制这个，但是设置这个值会影响到代理管理的所有的topics，甚至包括不良状态的topics。可以通过查看性能和资源部分了解更多。尤其是需要注意的，必须设置代理的replica.fetch.max.bytes属性，以防止重复任何非常大的消息。  There is no per-topic equivalent to this.Kafka配置自动设置提供的oryx-run.sh脚本可以打印ZK的当前配置，列出已经存在的Kafka中的topics，如果需要，会创建配置好的输入topics和更新topics。你需要先创建Oryx配置文件，或者可以拷贝conf/als-example.conf。需要按照要求修改Kafka和ZK的配置文件，比如topic名称。oryx.conf文件需要和每个层的JAR文件放在同一个目录下，然后执行：./oryx-run.sh kafka-setupInput  ZK:    your-zk:2181Input  Kafka: your-kafka:9092Input  topic: OryxInputUpdate ZK:    your-zk:2181Update Kafka: your-kafka:9092Update topic: OryxUpdateAll available topics:Input topic OryxInput does not exist. Create it? yCreating topic OryxInputCreated topic "OryxInput".Status of topic OryxInput:Topic:OryxInput	PartitionCount:4	ReplicationFactor:1	Configs:	Topic: OryxInput	Partition: 0	Leader: 120	Replicas: 120,121	Isr: 120,121	Topic: OryxInput	Partition: 1	Leader: 121	Replicas: 121,120	Isr: 121,120	Topic: OryxInput	Partition: 2	Leader: 120	Replicas: 120,121	Isr: 120,121	Topic: OryxInput	Partition: 3	Leader: 121	Replicas: 121,120	Isr: 121,120Update topic OryxUpdate does not exist. Create it? yCreating topic OryxUpdateCreated topic "OryxUpdate".Updated config for topic "OryxUpdate".Status of topic OryxUpdate:Topic:OryxUpdate	PartitionCount:1	ReplicationFactor:1	Configs:retention.ms=86400000,max.message.bytes=16777216	Topic: OryxUpdate	Partition: 0	Leader: 120	Replicas: 120,121	Isr: 120,121查看发送到输入和更新topic，监控应用的动作，可以执行：./oryx-run.sh kafka-tailInput  ZK:    your-zk:2181Input  Kafka: your-kafka:9092Input  topic: OryxInputUpdate ZK:    your-zk:2181Update Kafka: your-kafka:9092Update topic: OryxUpdate...output...接着在另外一个窗口，可以接受输入数据，比如将来自终端用户的文档data.csv加入到输入队列，并验证：./oryx-run.sh kafka-input --input-file data.csv如果以上全部成功了，可以关闭这些进程。集群至此已经准备好运行Oryx了。HDFS和数据层在Oryx中，Kafka是数据传输的途径，因此数据在Kafka中只是需要暂时存放的。然而输入数据也会持久化到HDFS中以备之后使用。同样的，模型和更新用来为Kafka的更新topic提供数据，模型也会持久化到HDFS以备之后引用。oryx.batch.storage.data-dir定义了输入数据存放在HDFS中的位置。在这个目录下，子目录标题会以oryx-[timestamp].data形式创建，每一个会通过Spark Straming在批量处理层执行。在这里，时间戳格式格式与Unix相同，且以毫秒为单位。实际上，和大多数Hadoop中分布式进程输出的『文件』一样，存在这样的一个子目录，包含了很多以part-开头的文件。每个文件都是序列化文件，通过Writable类序列化Kafka输入topics的键值，Writable类实现自oryx.batch.storage.key-writable-class 和 oryx.batch.storage.message-writable-class类。默认情况下，这是TextWritable，并且keys和消息是以字符串形式被记录下来的。该目录下的数据会被删除。也就不会再次被批处理层计算使用。尤其是，设置oryx.batch.storage.max-age-data-hours为一个非负数，将会使批处理层自动删除大于给定时间的数据。同样的，在每个批处理间隔内被批处理层选中的模型会被原始的机器学习应用（扩展子MLUpdate）输出。也会被持久化到oryx.batch.storage.model-dir定义的目录下的子目录中。在这个目录下，子目录的命名都是以时间戳形式实现的，同Unix毫秒形式。子目录下的内容取决于应用，但是一般会包含以model.pmml命名的PMML模块，并且和模块一起存在的可选的追加文件。这个目录之所以存在是因为需要记录PMML模块用来归档用或者被其他工具使用。也可按照规则删除其内容。捕获错误最后，你可能希望停止其中一个或者几个层的运行，或者重启。服务也可能挂鸟。这到底发生了神马？为啥会这样捏？数据丢失历史数据存放在HDFS中，理论上会存放多个副本。HDFS会确保数据被靠谱的存放着。当设置了副本，Kafka也被设计为采用副本方式应对故障。这并没有啥鸟用，这样不能确保数据不会丢失，只能依靠HDFS和Kafka能正常可用罢了。服务器挂鸟通常情况下，所有的三层服务进程应该会持续的工作，如果不得不停止或者挂鸟的话，服务自己会立即重启。这个可以通过初始化脚本完美的完成或者类似机制（尚未实现鸟）服务层服务层是无状态的。启动后，他会读取更新topics中的所有的模型和可用更新。当首个可用的模型就绪后，就可以开始应答请求。基于这个原因，需要适当的限制更新topic的持续时间。服务层的操作不是分布式的，每个实例是独立的，启动和停止不会影响到其他部分。实时计算层实时计算层同样也不存在状态，也会在读取全部的模型和更新topic的更新。只要存在合法的模型，就可以生成更新。同时，从最后一次偏移位置开始读取输入topics。实时计算层使用Spark和Spark Streaming 做计算。Spark会响应计算过程中失败情况，并重试任务。Spark Streaming的Kafka集成模块在某些情况下可以恢复接收的故障。如果是整个进程死掉并被重新启动，oryx.id的值被设定以后，系统会自动从上一次Kafka记录的偏移地址开始读取。（否则，将会从上次偏移地址开始，这就意味着实时计算层没有运行的时候，到达的数据就不会生成任何更新。），同样的，如果实时计算层的模型还没有准备好的话，收到的数据也会被忽略。It effectively adopts “at most once” semantics.由于实时计算层的作用是为最后发布的模型提供approximate, “best effort”的更新。这种行为由于其间接性一般是没有问题，且令人满意的。批处理层批处理层是最复杂的，因为他并生成某些状态：  历史数据，总是持久化到HDFS  如果应用选择的话，模型的扩展状态和topics都可以被持久化到HDFS上对于多次或者根本不读取数据是非常敏感的，因为他本来就是生产官方下一代模型的组件与实时计算层一起，Spark和Spark Streaming在计算过程中可以捕获很多错误情况。也可以管理存储到HDFS中的数据，负责避免两次写入相同数据。应用负责回复各自的「状态」，一般情况下，建立在Oryx ML层的应用会将状态写入唯一的子目录中，并且重启后会在新的目录简单的产生一个新状态。前一个状态如果存在的话，也会被完整写入或者被完全忽视。批处理层也和实时计算层一样，符合『至多一次』的规则。综上，如果整个进程死掉或者被重启，oryx.id被设置的话，则会从Kafka记录的最后一次偏移重新读取，否则会在最后一次偏移处重新读取数据。]]></content>
      <categories>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Apache Flume-ng Structure]]></title>
      <url>/2015/12/10/apache-flume-ng-structure/</url>
      <content type="text"><![CDATA[Apache-flume NG 配置简介Flume NG是一个分布式、可靠、可用的系统，它能够将不同数据源的海量日志数据进行高效收集、聚合、移动，最后存储到一个中心化数据存储系统中。由原来的Flume OG到现在的Flume NG，进行了架构重构，并且现在NG版本完全不兼容原来的OG版本。经过架构重构后，Flume NG更像是一个轻量的小工具，非常简单，容易适应各种方式日志收集，并支持failover和负载均衡。架构设计要点核心概念  Event：一个数据单元，带有一个可选的消息头  Flow：Event从源点到达目的点的迁移的抽象  Client：操作位于源点处的Event，将其发送到Flume Agent  Agent：一个独立的Flume进程，包含组件Source、Channel、Sink  Source：用来消费传递到该组件的Event  Channel：中转Event的一个临时存储，保存有Source组件传递过来的Event  Sink：从Channel中读取并移除Event，将Event传递到Flow Pipeline中的下一个Agent（如果有的话）架构图基本流程外部系统产生日志，直接通过Flume的Agent的Source组件将事件（如日志行）发送到中间临时的channel组件，最后传递给Sink组件，HDFS Sink组件可以直接把数据存储到HDFS集群上。单Agent一个最基本Flow的配置，格式如下：  # list the sources, sinks and channels for the agent  &lt;Agent&gt;.sources = &lt;Source1&gt; &lt;Source2&gt;  &lt;Agent&gt;.sinks = &lt;Sink1&gt; &lt;Sink2&gt;  &lt;Agent&gt;.channels = &lt;Channel1&gt; &lt;Channel2&gt;  # set channel for source  &lt;Agent&gt;.sources.&lt;Source1&gt;.channels = &lt;Channel1&gt; &lt;Channel2&gt; ...  &lt;Agent&gt;.sources.&lt;Source2&gt;.channels = &lt;Channel1&gt; &lt;Channel2&gt; ...  # set channel for sink  &lt;Agent&gt;.sinks.&lt;Sink1&gt;.channel = &lt;Channel1&gt;  &lt;Agent&gt;.sinks.&lt;Sink2&gt;.channel = &lt;Channel2&gt;尖括号里面的，我们可以根据实际需求或业务来修改名称。下面详细说明：       表示配置一个Agent的名称，一个Agent肯定有一个名称。        和是Agent的Source组件的名称，消费传递过来的Event。        和是Agent的Channel组件的名称。        与是Agent的Sink组件的名称，从Channel中消费（移除）Event。  上面配置内容中  第一组中配置Source、Sink、Channel，它们的值可以有1个或者多个；  第二组中配置Source将把数据存储（Put）到哪一个Channel中，可以存储到1个或多个Channel中，同一个Source将数据存储到多个Channel中，实际上是Replication；  第三组中配置Sink从哪一个Channel中取（Task）数据，一个Sink只能从一个Channel中取数据。多个Agent顺序连接可以将多个Agent顺序连接起来，将最初的数据源经过收集，存储到最终的存储系统中。这是最简单的情况，一般情况下，应该控制这种顺序连接的Agent的数量，因为数据流经的路径变长了，如果不考虑failover的话，出现故障将影响整个Flow上的Agent收集服务。多个Agent的数据汇聚到同一个Agent这种情况应用的场景比较多，比如要收集Web网站的用户行为日志，Web网站为了可用性使用的负载均衡的集群模式，每个节点都产生用户行为日志，可以为每个节点都配置一个Agent来单独收集日志数据，然后多个Agent将数据最终汇聚到一个用来存储数据存储系统，如HDFS上。多路（Multiplexing）Agent这种模式，有两种方式  一种是用来复制（Replication）                  Replication方式，可以将最前端的数据源复制多份，分别传递到多个channel中，每个channel接收到的数据都是相同的，配置格式          # List the sources, sinks and channels for the agent  &lt;Agent&gt;.sources = &lt;Source1&gt;  &lt;Agent&gt;.sinks = &lt;Sink1&gt; &lt;Sink2&gt;  &lt;Agent&gt;.channels = &lt;Channel1&gt; &lt;Channel2&gt;  # set list of channels for source (separated by space)  &lt;Agent&gt;.sources.&lt;Source1&gt;.channels = &lt;Channel1&gt; &lt;Channel2&gt;  # set channel for sinks  &lt;Agent&gt;.sinks.&lt;Sink1&gt;.channel = &lt;Channel1&gt;  &lt;Agent&gt;.sinks.&lt;Sink2&gt;.channel = &lt;Channel2&gt;  &lt;Agent&gt;.sources.&lt;Source1&gt;.selector.type = replicating                使用的Replication方式，Source1会将数据分别存储到Channel1和Channel2，这两个channel里面存储的数据是相同的，然后数据被传递到Sink1和Sink2。              另一种是用来分流（Multiplexing）                  Multiplexing方式，selector可以根据header的值来确定数据传递到哪一个channel          # Mapping for multiplexing selector  &lt;Agent&gt;.sources.&lt;Source1&gt;.selector.type = multiplexing  &lt;Agent&gt;.sources.&lt;Source1&gt;.selector.header = &lt;someHeader&gt;  &lt;Agent&gt;.sources.&lt;Source1&gt;.selector.mapping.&lt;Value1&gt; = &lt;Channel1&gt;  &lt;Agent&gt;.sources.&lt;Source1&gt;.selector.mapping.&lt;Value2&gt; = &lt;Channel1&gt; &lt;Channel2&gt;  &lt;Agent&gt;.sources.&lt;Source1&gt;.selector.mapping.&lt;Value3&gt; = &lt;Channel2&gt;  #...  &lt;Agent&gt;.sources.&lt;Source1&gt;.selector.default = &lt;Channel2&gt;                上面selector的type的值为multiplexing，同时配置selector的header信息，还配置了多个selector的mapping的值，即header的值：如果header的值为Value1、Value2，数据从Source1路由到Channel1；如果header的值为Value2、Value3，数据从Source1路由到Channel2。            实现load balance功能Load balancing Sink Processor能够实现load balance功能，上图Agent1是一个路由节点，  负责将Channel暂存的Event均衡到对应的多个Sink组件上，而每个Sink组件分别连接到一个独立的Agent上    a1.sinkgroups = g1    a1.sinkgroups.g1.sinks = k1 k2 k3    a1.sinkgroups.g1.processor.type = load_balance    a1.sinkgroups.g1.processor.backoff = true    a1.sinkgroups.g1.processor.selector = round_robin    a1.sinkgroups.g1.processor.selector.maxTimeOut=10000实现failover能Failover Sink Processor能够实现failover功能，具体流程类似load balance，  但是内部处理机制与load balance完全不同：Failover Sink Processor维护一个优先级Sink组件列表，只要有一个Sink组件可用，  Event就被传递到下一个组件。如果一个Sink能够成功处理Event，则会加入到一个Pool中，否则会被移出Pool并计算失败次数，设置一个惩罚因子    a1.sinkgroups = g1    a1.sinkgroups.g1.sinks = k1 k2 k3    a1.sinkgroups.g1.processor.type = failover    a1.sinkgroups.g1.processor.priority.k1 = 5    a1.sinkgroups.g1.processor.priority.k2 = 7    a1.sinkgroups.g1.processor.priority.k3 = 6    a1.sinkgroups.g1.processor.maxpenalty = 20000安装配置# 下载二进制包[mofun_mining@i-tev02vc1 ~]$ wget "http://apache.arvixe.com/flume/1.6.0/apache-flume-1.6.0-bin.tar.gz"[mofun_mining@i-tev02vc1 ~]$ tar xvzf apache-flume-1.6.0-bin.tar.gz[mofun_mining@i-tev02vc1 ~]$ mv apache-flume-1.6.0-bin /usr/local/# 修改配置文件[mofun_mining@i-qe32ajmq conf]$ pwd/usr/local/apache-flume-1.6.0-bin/conf[mofun_mining@i-qe32ajmq conf]$ sudo cp flume-conf.properties.template flume-conf.properties采用 Avro Source+Memory Channel+HDFS Sink 方式  服务器（日志汇总服务器agent）端配置文件  [mofun_mining@i-tev02vc1 ~]$ cd /usr/local/apache-flume-1.6.0-bin/conf/  [mofun_mining@i-tev02vc1 conf]$ ls  flume-conf.properties  flume-conf.properties.template  flume-env.ps1.template  flume-env.sh  flume-env.sh.template  log4j.properties  [mofun_mining@i-tev02vc1 conf]$ pwd  /usr/local/apache-flume-1.6.0-bin/conf  [mofun_mining@i-tev02vc1 conf]$ sudo vim flume-conf.properties  # example.conf: A single-node Flume configuration  # Name the components on this agent  agent1.sources = r1  agent1.sinks = k1  agent1.channels = c1  # Describe/configure the source  agent1.sources.r1.type = avro  agent1.sources.r1.bind = 192.168.1.33  agent1.sources.r1.port = 41414  agent1.sources.r1.channels = c1  # Describe the sink  agent1.sinks.k1.type = hdfs  agent1.sinks.k1.channel = c1  agent1.sinks.k1.hdfs.fileType = DataStream  agent1.sinks.k1.hdfs.useLocalTimeStamp = true  agent1.sinks.k1.hdfs.path = /flume/events/%Y-%m-%d  #agent1.sinks.k1.hdfs.round = true  #agent1.sinks.k1.hdfs.roundValue = 10  #agent1.sinks.k1.hdfs.roundUnit = minute  agent1.sinks.k1.hdfs.rollCount = 5000  agent1.sinks.k1.hdfs.rollSize = 0  agent1.sinks.k1.hdfs.rollInterval= 0  # Use a channel which buffers events in memory  agent1.channels.c1.type = memory  agent1.channels.c1.capacity = 10000  agent1.channels.c1.transactionCapacity = 1000  客户端（日志收集agent）  [reason@i-qunray9x conf]$ cd /usr/local/apache-flume-1.6.0-bin/conf/  [reason@i-qunray9x conf]$ pwd  /usr/local/apache-flume-1.6.0-bin/conf  [reason@i-qunray9x conf]$ sudo vim flume-conf.properties  # example.conf: A single-node Flume configuration  # Name the components on this agent  agent1.sources = r1  agent1.sinks = k1  agent1.channels = c1  # Describe/configure the source  agent1.sources.r1.type = exec  agent1.sources.r1.command = tail -n 0 -F /home/reason/1.txt  agent1.sources.r1.channels = c1  # Describe the sink  agent1.sinks.k1.type = avro  agent1.sinks.k1.channel = c1  agent1.sinks.k1.hdfs.useLocalTimeStamp = true  agent1.sinks.k1.hdfs.path = /flume/events/%Y-%m-%d  agent1.sinks.k1.hostname=192.168.1.33  agent1.sinks.k1.port = 41414  # Use a channel which buffers events in memory  agent1.channels.c1.type = memory  agent1.channels.c1.capacity = 5000  agent1.channels.c1.transactionCapacity = 500  启动服务器    [mofun_mining@i-tev02vc1 conf]$    /usr/local/apache-flume-1.6.0-bin/bin/flume-ng agent -c ./conf/ -f /usr/local/apache-flume-1.6.0-bin/conf/flume-conf.properties -n agent1 -Dflume.root.logger=INFO,console  启动客户端    [reason@i-qunray9x conf]$    /usr/local/apache-flume-1.6.0-bin/bin/flume-ng agent -c conf -f /usr/local/apache-flume-1.6.0-bin/conf/flume-conf.properties -n agent1  测试  [mofun_mining@i-r6cuv8iq ~]$ hdfs dfs -ls /flume/events/2015-12-15  Found 40 items  -rw-r--r--   2 mofun_mining supergroup      34844 2015-12-15 17:39 /flume/events/2015-12-15/FlumeData.1450172340281  -rw-r--r--   2 mofun_mining supergroup      34850 2015-12-15 17:39 /flume/events/2015-12-15/FlumeData.1450172340282  -rw-r--r--   2 mofun_mining supergroup      34850 2015-12-15 17:39 /flume/events/2015-12-15/FlumeData.1450172340283  -rw-r--r--   2 mofun_mining supergroup      34850 2015-12-15 17:39 /flume/events/2015-12-15/FlumeData.1450172340284  -rw-r--r--   2 mofun_mining supergroup      34850 2015-12-15 17:39 /flume/events/2015-12-15/FlumeData.1450172340285  -rw-r--r--   2 mofun_mining supergroup      34850 2015-12-15 17:39 /flume/events/2015-12-15/FlumeData.1450172340286  -rw-r--r--   2 mofun_mining supergroup      34850 2015-12-15 17:39 /flume/events/2015-12-15/FlumeData.1450172340287  -rw-r--r--   2 mofun_mining supergroup      34850 2015-12-15 17:39 /flume/events/2015-12-15/FlumeData.1450172340288  -rw-r--r--   2 mofun_mining supergroup      34850 2015-12-15 17:39 /flume/events/2015-12-15/FlumeData.1450172340289  -rw-r--r--   2 mofun_mining supergroup      34850 2015-12-15 17:39  ...此时，通过nginx实时产生的日志，即可实时插入到hdfs中了。参考文献  http://shiyanjun.cn/archives/915.html  http://my.oschina.net/leejun2005/blog/288136  http://tech.meituan.com/mt-log-system-optimization.html  http://www.ixirong.com/2015/05/18/how-to-install-flume-ng/  https://flume.apache.org/FlumeUserGuide.html#setting-up-an-agent  http://m.blog.csdn.net/blog/xueliang1029/24039459]]></content>
      <categories>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Apache Kafka Structure]]></title>
      <url>/2015/12/08/apache-kafka-structure/</url>
      <content type="text"><![CDATA[Apache Kafka消息服务      参考地址 http://kafka.apache.org/documentation.html        消息队列的分类                  点对点        生产者生产消息发送到Queue中，消费者消费Queue中的消息，其中：                  Queue中不再存储已经被消费的消息          Queue支持多个消费者，但是同一个消息，只能被一个消费者消费                            发布/订阅        生产者（生产）将消息发布到topic中，同时多个消费者（消费）订阅该消息。和点对点方式不同的是，发布到topic的消息会被所有订阅者消费                  简介    背景 Kafka使用Scala语言编写，是一个分布式，分区的，支持多副本，多订阅者的日志系统。    目前支持Java，Python，C++， PHP等          总体结构            名词解释                  Producer        消息生产者，就是向kafka broker发消息的客户端                    Consumer        消息消费者，向kafka broker取消息的客户端                    Topic        是一个消息队列？                    Consumer Group （CG）                  这是Kafka用来实现一个Topic消息的广播（发给所有的Consumer）和单播（发给任意一个Consumer）的手段          一个Topic可以有多个CG          Topic的消息会复制（不是真的复制，是概念上的）到所有的CG，但每个CG只会把消息发给该CG中的一个consumer          如果需要实现广播，只要每个Consumer有一个独立的CG就可以了          要实现单播只要所有的Consumer在同一个CG                      用CG还可以将Consumer进行自由的分组而不需要多次发送消息到不同的topic                                Broker                          一台Kafka服务器就是一个Broker              一个集群由多个Broker组成。一个Broker可以容纳多个Topic                                            Partition            为了实现扩展性，一个非常大的Topic可以分布到多个Broker（即服务器）上，一个Topic可以分为多个Partition，每个Partition是一个有序的队列。Prtition中的每条消息都会被分配一个有序的id（Offset）。Kafka只保证按一个Partition中的顺序将消息发给Consumer，不保证一个Topic的整体（多个Partition间）的顺序。                                Offset                          kafka的存储文件都是按照offset.kafka来命名，用offset做名字的好处是方便查找              例如你想找位于2049的位置，只要找到2048.kafka的文件即可，当然the first offset就是00000000000.kafka                                                特性          通过O(1)的磁盘数据结构提供消息的持久化，这种结构对于即使数以TB的消息存储也能够保持长时间的稳定性能      高吞吐量：即使是非常普通的硬件Kafka也可以支持每秒数十万的消息      支持 同步 和 异步 复制两种HA      Consumer客户端                  pull          随机读          利用sendfile系统调用          zero-copy          批量拉数据                    消费状态保存在客户端      消息存储顺序写      数据迁移、扩容对用户透明      支持Hadoop并行数据加载      支持online和offline的场景      持久化：通过将数据持久化到硬盘以及replication防止数据丢失      scale out：无需停机即可扩展机器      定期删除机制，支持设定partitions的segment file保留时间            可靠性（一致性)    传统的MQ系统通常都是通过broker和consumer间的确认（ack）机制实现的，并在broker保存消息分发的状态，即使这样一致性也是很难保证的。    Kafka的做法是由consumer自己保存状态，也不要任何确认。这样虽然consumer负担更重，但其实更灵活了。因为不管consumer上任何原因导致需要重新处理消息，都可以再次从broker获得。        可扩展性    Kafka 使用Zookeeper实现动态的集群扩展，不需要更改客户端（生产者和消费者）的配置。broker会在ZK注册并保持相关的元数据更新。而客户端会在ZK上注册相关的watcher，一旦ZK发生变化，客户端能及时做出相应调整。这样可以保证变更broker时，各个broker之间能自动实现负载均衡。        设计目标    高吞吐量          数据磁盘持久化：消息不在内存中cache，直接写入到磁盘，充分利用磁盘的顺序读写性能      zero-copy：减少IO操作步骤      支持数据批量发送和拉取      支持数据压缩      Topic划分为多个partition，提高并行处理能力        Producer负载均衡和HA机制          producer根据用户指定的算法，将消息发送到指定的partition。      存在多个partiiton，每个partition有自己的replica，每个replica分布在不同的Broker节点上。      多个partition需要选取出lead partition，lead partition负责读写，并由zookeeper负责fail over。      通过zookeeper管理broker与consumer的动态加入与离开。            Consumer的pull机制    由于broker会持久化数据，broker没有cache压力，因此，consumer比较适合才去pull的方式消费数据：          简化kafka设计，降低了难度      Consumer根据消费能力自主控制消息拉取速度      Consumer根据自身情况自主选择消费模式，例如批量，重复消费，从制定partition或位置(offset)开始消费等            Consumer与Topic关系以及机制    每个group包含多个consumer。对于topic中的一条特定消息，只会被订阅此Topic每个group中的一个consumer消费，那么一个group中的所有consumer将会交错的消费整个Topic。    如果所有的consumer都具有相同的group（类似JMS queue），消息将有所有的consumer负载均衡    如果所有的consumer都具有不同的group，那么这就是『发布-订阅』，消息将会广播给所有消费者    在Kafka中，一个partition中的消息只会被group中的一个consumer消费（同一时刻）；每个group中consumer消息消费互相独立；一个group是一个『订阅』者，一个Topic中的每个partition只会被一个『订阅』者中的一个consumer消费，但是一个consumer可以同事消费多个partitions中的消息。    Kafka只能保证一个partition中的消息被某个consumer消费是顺序的，但是从Topic角度，当有多个partitions时，消息仍不是全局有序的    一个group中包含多个consumer，这样的话不仅能提高topic中消息的并发消费能力，还能提高『故障容错』性，如果group中的某个consumer失效，那么其消费的partition将会被其他consumer接管    Kafka的设计原理决定，对于一个Topic，同一个group中不能有多于partition个数的consumer同时消费，否则将意味着某些consumer将无法得到消息        Producer均衡算法    Kafka集群中的任何一个broker，都可以向producer提供metadata，这些metadata中包含『集群中存货的servers/partition leaders』，当producer获取到metadata后，会和topic下所有的partition leader保持socker连接；消息由producer直接通过socker发送到broker          中间不会经过任何『路由层』，即，消息被路由到哪个partition上，是有producer决定的在producer端的配置文件中，可以指定partition的路由方式：『random』，『key-hash』等            Consumer均衡算法    当一个group中，有consumer加入或者离开时，会触发partitions均衡。均衡的最终目的，是提升topic的并发消费能力。          假如topic1,具有如下partitions: P0,P1,P2,P3      加入group中,有如下consumer: C0,C1      首先根据partition索引号对partitions排序: P0,P1,P2,P3      根据consumer.id排序: C0,C1      计算倍数: M = [P0,P1,P2,P3].size / [C0,C1].size,本例值M=2(向上取整)      然后依次分配partitions: C0 = [P0,P1],C1=[P2,P3],即Ci = [P(i * M),P((i + 1) * M -1)]            Broker集群内broker之间replica机制    replication策略是基于partiton，而不是topic          kafka将每个partition复制到多个server上任何一个partition有一个leader和任意数量的follower备份的数量可以由broker配置文件设定leader处理所有的read-write请求，负责跟踪所有的follower状态，如果follower『落后』太多或者失效，leader会把它从replicas同步列表中删除follower需要和leader保持同步，follower就像一个consumer，消费信息并保存在本地日志中当所有的follower都将一个消息保存成功，此消息才能被认为是『committed』，此时consumer才能消费它，这种策略要求leader和follower之间保持良好的网络环境只要ZK集群存活，即使只存活一个replica，仍可以保证消息的正常发送和接收              Kafka判定一个follower存活的条件                  和ZK保持良好的链接          及时跟进leader，不能落后太多                            如果此replicas落后太多，它会继续在leader中fetch数据，然后加入同步列表中，Kafka不会更换宿主，只有这样才能保证replicas足够快，才能保证producer发布消息时接收ACK的延迟较小              当leader失效，需要考虑负载均衡，partition leader较少的broker更有可能成为新的leader，因为                  不能采用『投票多数派』的算法，因为这种算法对于『网络稳定性/投票参与者数量』要求较高          Kafka集群设计中，容忍N-1个replicas失效          每个partiton中所有的replica信息都可以在ZK中获得，那么选择leader是非常简单的          选择follower时需要注意：避免新的leader server上承载的partiton leader的个数过多，否则此server将承受更多的IO压力                          总结          Producer端直接连接broker列表，从列表中返回TopicMetadataResponse，该Metadata包含Topic下每个partition leader建立socket连接并发送消息。      Broker端使用ZK用来注册broker信息，以及监控partition leader存活性。      Consumer端使用ZK用来注册consumer信息，其中包括consumer消费的partition列表等，同时也用来发现broker列表，并和partition leader建立socket连接，并获取消息。      Kafka在Zookeeper中存储结构  结构图Kafka 安装和配置参考文献  http://blog.csdn.net/zhongwen7710/article/details/41252649  http://kafka.apache.org/documentation.html#brokerconfigs]]></content>
      <categories>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Linux shell 获得以前日期]]></title>
      <url>/2015/12/05/linux-shell-generate-date/</url>
      <content type="text"><![CDATA[在linux shell里，我想获得以前的日期1、比如，去年的上个月的昨天的日期：（今天是2009年2月2日，也就是2008年1月1日）reasonpun@reasonpun:~$ logRecordDate="`date -d "-1 year -1 month -1 day" "+%Y_%m_%d"`"reasonpun@reasonpun:~$ echo $logRecordDate2008_01_01reasonpun@reasonpun:~$2、上个月的今天：reasonpun@reasonpun:~$ logRecordDate="`date -d "-1 month" "+%Y_%m_%d"`"reasonpun@reasonpun:~$ echo $logRecordDate2009_01_023、去年的今天：reasonpun@reasonpun:~$ logRecordDate="`date -d "-1 year" "+%Y_%m_%d"`"reasonpun@reasonpun:~$ echo $logRecordDate2008_02_024、上个月的昨天：reasonpun@reasonpun:~$ logRecordDate="`date -d "-1 month -1 day" "+%Y_%m_%d"`"reasonpun@reasonpun:~$ echo $logRecordDate2009_01_015、根据时间戳转换成日期[mofun_mining@i-qe32ajmq ~]$ date -d @1434847028 "+%Y-%m-%d"2015-06-21其他的类推～～呵呵，还是希望大家给测测其他日期会不会出错呵呵。多谢～～～鼓捣之环境：ubuntu8.04update @ 2016-12-19 15:48PHP获取后一天日期的写法$date = "04-15-2013";$date1 = str_replace('-', '/', $date);$tomorrow = date('m-d-Y', strtotime($date1 . "+1 days"));echo $tomorrow;另附上windows下获得前一天的日期：@echo offset td=%date:~2,2%%date:~5,2%%date:~8,2%set dy=%date:~0,4%set dm=%date:~5,2%set dd=%date:~8,2%set da=%date:~8,2%if %dm%%dd%==0101 goto L01if %dm%%dd%==0201 goto L02if %dm%%dd%==0301 goto L07if %dm%%dd%==0401 goto L02if %dm%%dd%==0501 goto L04if %dm%%dd%==0601 goto L02if %dm%%dd%==0701 goto L04if %dm%%dd%==0801 goto L02if %dm%%dd%==0901 goto L02if %dm%%dd%==1001 goto L05if %dm%%dd%==1101 goto L03if %dm%%dd%==1201 goto L06if %dd%==02 goto L10if %dd%==03 goto L10if %dd%==04 goto L10if %dd%==05 goto L10if %dd%==06 goto L10if %dd%==07 goto L10if %dd%==08 goto L10if %dd%==09 goto L10if %dd%==10 goto L11set /A dd=dd-1set dt=%dy%-%dm%-%dd%goto END:L10set /A dd=%dd:~1,1%-1set dt=%dy%-%dm%-0%dd%goto END:L11set dt=%dy%-%dm%-09goto END:L02set /A dm=%dm:~1,1%-1set dt=%dy%-0%dm%-31goto END:L04set /A dm=dm-1set dt=%dy%-0%dm%-30goto END:L05set dt=%dy%-09-30goto END:L03set dt=%dy%-10-31goto END:L06set dt=%dy%-11-30goto END:L01set /A dy=dy-1set dt=%dy%-12-31goto END:L07set /A "dd=dy%%4"if not %dd%==0 goto L08set /A "dd=dy%%100"if not %dd%==0 goto L09set /A "dd=dy%%400"if %dd%==0 goto L09:L08set dt=%dy%-02-28goto END:L09set dt=%dy%-02-29goto END:ENDset dateTime=20%dt:~2,2%%dt:~5,2%%dt:~8,2%]]></content>
      <categories>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Docker中搭建Spark计算框架]]></title>
      <url>/2015/12/04/spark-in-docker/</url>
      <content type="text"><![CDATA[安装好Docker之后先拉取一个官方的基本镜像ubuntudocker pull ubuntu我们将在这个基础镜像上运行容器，将这个容器当成一个普通的ubuntu虚拟机来操作部署spark，最后将配置好的容器commit为一个镜像，之后就可以通过这个镜像运行n个节点来完成集群的搭建下载完ubuntu镜像之后运行[reason@localhost ~]$ docker imagesREPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZEmofun/spark         v2.0                b2dacac3e132        About an hour ago   1.508 GBmofun/spark         v1.3                4d2f33ca61ee        18 hours ago        1.506 GBubuntu              latest              0a17decee413        6 days ago          188.3 MBdocker/whalesay     latest              ded5e192a685        4 months ago        247 MB[reason@localhost ~]$运行ubuntu容器[reason@localhost ~]$ docker run -v /home/docker/software/:/software -it ubunturoot@f4c0a9d42852:/#在容器中安装ssh[reason@localhost ~]$ docker run -v /home/docker/software/:/software -it ubunturoot@3970c1e5466e:/# apt-get install sshReading package lists... DoneBuilding dependency treeReading state information... Done# ssh默认配置root无法登陆root@3970c1e5466e:~/.ssh# vim /etc/ssh/sshd_configroot@3970c1e5466e:~/.ssh## 将 /etc/ssh/sshd_config中PermitRootLogin no/without_passwd 改为yes# 生成访问密钥root@3970c1e5466e:~# ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsaGenerating public/private rsa key pair.Created directory '/root/.ssh'.Your identification has been saved in /root/.ssh/id_rsa.Your public key has been saved in /root/.ssh/id_rsa.pub.The key fingerprint is:b5:d1:e1:dd:98:7d:be:cc:55:69:c6:e7:67:80:d8:d3 root@3970c1e5466eThe key's randomart image is:+--[ RSA 2048]----+|            .    ||           = =.=.||          + * E=*||         . o .o++||        S .     *||              o.+||               + ||                 ||                 |+-----------------+root@3970c1e5466e:~#root@3970c1e5466e:~# cd ~/.ssh/root@3970c1e5466e:~/.ssh# cat id_rsa.pub &gt;&gt; authorized_keysroot@3970c1e5466e:~/.ssh#root@3970c1e5466e:~/.ssh# vim ~/.bashrc#加入/usr/sbin/sshd#如果在启动容器的时候还是无法启动ssh的话，# 在/etc/rc.local文件中也加入root@3970c1e5466e:~/.ssh# vim /etc/rc.local#加入root@3970c1e5466e:~/.ssh# /usr/sbin/sshdMissing privilege separation directory: /var/run/sshdroot@3970c1e5466e:~/.ssh# mkdir /var/run/sshdroot@3970c1e5466e:~/.ssh# /usr/sbin/sshdroot@3970c1e5466e:~/.ssh## 开启ssh服务后验证是否可以使用，打印出当前时间root@3970c1e5466e:~/.ssh# ssh localhost dateThe authenticity of host 'localhost (::1)' can't be established.ECDSA key fingerprint is ab:43:27:e6:1c:44:be:2c:f1:17:27:90:6d:2c:68:86.Are you sure you want to continue connecting (yes/no)? yesWarning: Permanently added 'localhost' (ECDSA) to the list of known hosts.Mon Oct 19 05:57:44 UTC 2015root@3970c1e5466e:~/.ssh## ssh安装完毕安装JDK可以使用apt-get方式直接下载安装jdk（不推荐，下载速度慢，有可能还会失败）这里选择从网上下载完jdk-8u60-linux-xx.bin之后将其传到Ubuntu宿主机中，在运行容器的时候使用-v参数将宿主机上的目录映射到容器中，这样在容器中就可以访问到宿主机中的文件了如果提示不能安装.bin文件，使用以下命令即可解决root@3970c1e5466e:~/.ssh# apt-get updateroot@3970c1e5466e:~/.ssh# apt-getinstall g++-multilib安装Zookeeper将下载好的zookeeper-3.4.6.tar.gz上传root@3970c1e5466e:~/.ssh# mv /software/zookeeper-3.4.6.tar.gz /usr/local/zookeeper-3.4.6root@3970c1e5466e:~/.ssh# tar -zxvf zookeeper-3.4.6.tar.gzroot@3970c1e5466e:~/.ssh# cd /usr/local/zookeeper-3.4.6/conf/root@3970c1e5466e:~/.ssh# cp zoo_sample.cfgzoo.cfgvim zoo.cfgroot@3970c1e5466e:~/.ssh# vim zoo.cfg#修改：dataDir=/root/zookeeper/tmp#在最后添加：server.1=cloud4:2888:3888server.2=cloud5:2888:3888server.3=cloud6:2888:3888#保存退出，然后创建一个tmp文件夹root@3970c1e5466e:~/.ssh# mkdir /data/zookeeper/tmp#再创建一个空文件root@3970c1e5466e:~/.ssh# touch /data/zookeeper/tmp/myid#最后向该文件写入IDroot@3970c1e5466e:~/.ssh# echo 1&gt; /data/zookeeper/tmp/myid安装Hadooproot@3970c1e5466e:~/.ssh# mv /software/hadoop-2.6.1.tar.gz /usr/local/root@3970c1e5466e:~/.ssh# tar -zxvf hadoop-2.2.0-64bit.tar.gzroot@3970c1e5466e:~/.ssh# cd /usr/local/hadoop/etc/hadoop更改hadoop-env.shroot@3970c1e5466e:/data/test# cd /usr/local/hadoop-2.6.1/root@3970c1e5466e:/usr/local/hadoop-2.6.1# lsLICENSE.txt  NOTICE.txt  README.txt  bin  etc  include  lib  libexec  logs  sbin  shareroot@3970c1e5466e:/usr/local/hadoop-2.6.1# cd etc/hadoop/root@3970c1e5466e:/usr/local/hadoop-2.6.1/etc/hadoop# vim hadoop-env.sh#加入java环境变量export JAVA_HOME=/usr/local/jdk1.8.0_60修改core-site.xmlroot@3970c1e5466e:/usr/local/hadoop-2.6.1/etc/hadoop# vim core-site.xml&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;&lt;configuration&gt;&lt;property&gt;&lt;name&gt;fs.defaultFS&lt;/name&gt;&lt;value&gt;hdfs://ns1&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;&lt;value&gt;/data/hadoop/tmp&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;&lt;value&gt;cloud4:2181,cloud5:2182,cloud6:2183&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt;修改hdfs-site.xml, mapred-site.xml, yarn-site.xmlroot@3970c1e5466e:/usr/local/hadoop-2.6.1/etc/hadoop# vim hdfs-site.xml&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;&lt;configuration&gt;&lt;property&gt;&lt;name&gt;dfs.nameservices&lt;/name&gt;&lt;value&gt;ns1&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;dfs.ha.namenodes.ns1&lt;/name&gt;&lt;value&gt;nn1,nn2&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;dfs.namenode.rpc-address.ns1.nn1&lt;/name&gt;&lt;value&gt;cloud1:9000&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;dfs.namenode.http-address.ns1.nn1&lt;/name&gt;&lt;value&gt;cloud1:50070&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;dfs.namenode.rpc-address.ns1.nn2&lt;/name&gt;&lt;value&gt;cloud2:9000&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;dfs.namenode.http-address.ns1.nn2&lt;/name&gt;&lt;value&gt;cloud2:50070&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt;&lt;value&gt;qjournal://cloud4:8485;cloud5:8485;cloud6:8485/ns1&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt;&lt;value&gt;/data/hadoop/journal&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;dfs.client.failover.proxy.provider.ns1&lt;/name&gt;&lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;&lt;value&gt;sshfence&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt;&lt;value&gt;/root/.ssh/id_rsa&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;dfs.ha.fencing.ssh.connect-timeout&lt;/name&gt;&lt;value&gt;30000&lt;/value&gt;&lt;/property&gt;&lt;property&gt;        &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;        &lt;value&gt;file:///data/hadoop/workspace/hdfs/name&lt;/value&gt;&lt;/property&gt;&lt;property&gt;        &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;        &lt;value&gt;file:///data/hadoop/workspace/hdfs/data&lt;/value&gt;&lt;/property&gt;&lt;property&gt;       &lt;name&gt;dfs.replication&lt;/name&gt;       &lt;value&gt;2&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt;root@3970c1e5466e:/usr/local/hadoop-2.6.1/etc/hadoop# mv mapred-site.xml.template mapred-site.xmlroot@3970c1e5466e:/usr/local/hadoop-2.6.1/etc/hadoop# vim mapred-site.xml&lt;?xml version="1.0"?&gt;&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;&lt;configuration&gt;&lt;property&gt;&lt;name&gt;mapreduce.framework.name&lt;/name&gt;&lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt;root@3970c1e5466e:/usr/local/hadoop-2.6.1/etc/hadoop# vim yarn-site.xml&lt;?xml version="1.0"?&gt;&lt;configuration&gt;&lt;property&gt;&lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;&lt;value&gt;cloud3&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;&lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt;root@3970c1e5466e:/usr/local/hadoop-2.6.1/etc/hadoop# vim slavescloud1cloud2cloud3cloud4cloud5cloud6安装Sparkroot@3970c1e5466e:~/.ssh# mv /software/scala-2.11.7 /usr/local/root@3970c1e5466e:~/.ssh# tar -zxvf scala-2.11.7.tgzroot@3970c1e5466e:~/.ssh# vim ~/.bashrcexport JAVA_HOME=/usr/local/jdk1.8.0_60export HADOOP_HOME=/usr/local/hadoop-2.6.1export SCALA_HOME=/usr/local/scala-2.11.7export SPARK_HOME=/usr/local/spark-1.5.1-bin-hadoop2.6export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$SCALA_HOME/bin:$SPARK_HOME/binroot@3970c1e5466e:~/.ssh# mv /software/spark-1.5.1-bin-hadoop2.6.tgz /usr/local/root@3970c1e5466e:~/.ssh# tar -zxvf spark-1.5.1-bin-hadoop2.6编辑配置文件root@3970c1e5466e:~/.ssh# cd /usr/local/spark-1.5.1-bin-hadoop2.6/root@3970c1e5466e:/usr/local/spark-1.5.1-bin-hadoop2.6# cd conf/root@3970c1e5466e:/usr/local/spark-1.5.1-bin-hadoop2.6# vim slavescloud1cloud2cloud3cloud4cloud5cloud6root@3970c1e5466e:/usr/local/spark-1.5.1-bin-hadoop2.6# mv spark-env.sh.template spark-env.shroot@3970c1e5466e:/usr/local/spark-1.5.1-bin-hadoop2.6# vim ~/spark/conf/spark-env.shexport SPARK_MASTER_IP=cloud1export SPARK_WORKER_MEMORY=128mexport JAVA_HOME=/usr/local/jdk1.8.0_60export SCALA_HOME=/usr/local/scala-2.11.7export SPARK_HOME=/usr/local/spark-1.5.1-bin-hadoop2.6export HADOOP_CONF_DIR=/usr/local/hadoop-2.6.1/etc/hadoopexport SPARK_LIBRARY_PATH=$$SPARK_HOME/libexport SCALA_LIBRARY_PATH=$SPARK_LIBRARY_PATHexport SPARK_WORKER_CORES=1export SPARK_WORKER_INSTANCES=1export SPARK_MASTER_PORT=7077此时配置已经基本完成，所以需要__保存镜像__[reason@localhost ~]$ sudo docker commit -m "mofun spark first commit" -a "reason" cloud1 mofun/spark:v1.0查看执行过的镜像[reason@localhost ~]$ sudo docker ps -aCONTAINER ID        IMAGE                 COMMAND                  CREATED             STATUS                         PORTS               NAMESd4e581ba6af8        mofun/spark:v1.0   "/bin/bash"              11 seconds ago      Exited (0) 7 seconds ago                           hungry_pasteur启动容器# 后台模式运行[reason@localhost ~]$ docker run -d --name cloud2 -h cloud2 -it mofun/spark:v2.0 /bin/bash需要使用刚才制作的镜像启动6个容器[reason@localhost ~]$ docker run --name cloud1 -h cloud1 -it mofun/spark:v2.0 /bin/bash[reason@localhost ~]$ docker run --name cloud2 -h cloud2 -it mofun/spark:v2.0 /bin/bash[reason@localhost ~]$ docker run --name cloud3 -h cloud3 -it mofun/spark:v2.0 /bin/bash[reason@localhost ~]$ docker run --name cloud4 -h cloud4 -it mofun/spark:v2.0 /bin/bash[reason@localhost ~]$ docker run --name cloud5 -h cloud5 -it mofun/spark:v2.0 /bin/bash[reason@localhost ~]$ docker run --name cloud6 -h cloud6 -it mofun/spark:v2.0 /bin/bash做最后的修改#在cloud5~cloud6中分别手动修改myidroot@cloud5:~# echo 2 &gt;  /data/zookeeper/myidroot@cloud5:~# echo 2 &gt; /usr/local/zookeeper-3.4.6/tmp/myidroot@cloud6:~# echo 3 &gt;  /data/zookeeper/myidroot@cloud6:~# echo 3 &gt; /usr/local/zookeeper-3.4.6/tmp/myid启动zookeeper集群# 启动zookeeper集群（分别在cloud4、cloud5、cloud6上启动zk）# 全部节点启动后，再执行zkServer.sh status# 返回正常# root@cloud6:/usr/local/zookeeper-3.4.6/bin# ./zkServer.sh status# JMX enabled by default# Using config: /usr/local/zookeeper-3.4.6/bin/../conf/zoo.cfg# Mode: followerroot@cloud5:~# /usr/local/zookeeper-3.4.6/bin/zkServer.sh start# 当3个节点服务正常启动后# 使用status查看是否启动root@cloud5:~# /usr/local/zookeeper-3.4.6/bin/zkServer.sh statusJMX enabled by defaultUsing config: /usr/local/zookeeper-3.4.6/bin/../conf/zoo.cfgMode: followerroot@cloud5:~#进入cloud1，开启hadoop和spark服务# 启动journalnode（在cloud1上启动所有journalnode，注意：是调用的hadoop-daemons.sh这个脚本，注意是复数s的那个脚本）# 运行jps命令检验，cloud4、cloud5、cloud6上多了JournalNode进程root@cloud1:/usr/local/hadoop-2.6.1/sbin# pwd/usr/local/hadoop-2.6.1/sbinroot@cloud1:/usr/local/hadoop-2.6.1/sbin#root@cloud1:/usr/local/hadoop-2.6.1/sbin# hadoop-daemons.sh start journalnode# 格式化ZK(在cloud1上执行即可，在bin目录下)root@cloud1:/usr/local/hadoop-2.6.1/bin# hdfs zkfc -formatZK# 进入节点cloud4，查看zookeeper信息root@cloud4:/usr/local/zookeeper-3.4.6/bin# pwd/usr/local/zookeeper-3.4.6/binroot@cloud4:/usr/local/zookeeper-3.4.6/bin# lsREADME.txt  zkCleanup.sh  zkCli.cmd  zkCli.sh  zkEnv.cmd  zkEnv.sh  zkServer.cmd  zkServer.sh  zookeeper.outroot@cloud4:/usr/local/zookeeper-3.4.6/bin#root@cloud4:/usr/local/zookeeper-3.4.6/bin# ./zkCli.shConnecting to localhost:21812015-10-21 09:34:34,485 [myid:] - INFO  [main:Environment@100] - Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT2015-10-21 09:34:34,487 [myid:] - INFO  [main:Environment@100] - Client environment:host.name=cloud42015-10-21 09:34:34,487 [myid:] - INFO  [main:Environment@100] - Client environment:java.version=1.8.0_......[zk: localhost:2181(CONNECTED) 2] ls /hadoop-ha[ns1][zk: localhost:2181(CONNECTED) 3]# 格式化HDFS(在bin目录下),在cloud1上执行命令:root@cloud1:/usr/local/hadoop-2.6.1/bin# hdfs namenode -format# 首先启动active节点，执行如下命令(在cloud1上执行)root@cloud1:/usr/local/hadoop-2.6.1/sbin# hadoop-daemon.sh start namenode  # 进入cloud2，需要启动standy模式root@cloud2:/usr/local/hadoop-2.6.1/bin# pwd/usr/local/hadoop-2.6.1/binroot@cloud2:/usr/local/hadoop-2.6.1/bin# ./hdfs namenode -bootstrapStandbyroot@cloud2:/usr/local/hadoop-2.6.1/sbin# pwd/usr/local/hadoop-2.6.1/sbin# 这条命令可以等到hadoop-daemons.sh start zkfc 成功以后执行# 貌似是zkfc的启动慢导致standby模式启动出错？root@cloud1:/usr/local/spark-1.5.1-bin-hadoop2.6/bin# jps1522 NameNode2546 Jps1859 DFSZKFailoverController1109 Worker104 JournalNode426 DataNode558 NodeManager943 Master# 仔细观察DFSZKFailoverController 这个进程的存在情况root@cloud2:/usr/local/hadoop-2.6.1/sbin# ./hadoop-daemon.sh start namenode# 重新进入cloud1 ，启动datanoderoot@cloud4:/usr/local/hadoop-2.6.1/sbin# pwd/usr/local/hadoop-2.6.1/sbinroot@cloud4:/usr/local/hadoop-2.6.1/sbin# ./hadoop-daemons.sh start datanode# 在cloud3上执行start-yarn.shroot@cloud3:/usr/local/hadoop-2.6.1/sbin# start-yarn.sh# 启动ZKFCroot@cloud1:/usr/local/hadoop-2.6.1/sbin# ./hadoop-daemons.sh start zkfc# 启动spark集群root@cloud1:/usr/local/hadoop-2.6.1/sbin# cd /usr/local/spark-1.5.1-bin-hadoop2.6/sbin/root@cloud1:/usr/local/spark-1.5.1-bin-hadoop2.6/sbin# start-all.sh此时可以通过CURL访问服务了，如果宿主机中的hosts文件没有配置docker容器的主机名和IP地址映射关系的话要换成用IP访问[reason@localhost ~]$ curl http://172.17.0.56:50070[reason@localhost ~]$其他# 删除镜像[reason@localhost ~]$ docker imagesREPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZEmofun/spark         v2.0                b2dacac3e132        3 hours ago         1.508 GBmofun/spark         v1.3                4d2f33ca61ee        21 hours ago        1.506 GBubuntu              latest              0a17decee413        6 days ago          188.3 MBdocker/whalesay     latest              ded5e192a685        4 months ago        247 MB[reason@localhost ~]$ sudo docker rmi 4d2f# 删除容器[reason@localhost ~]$ docker ps -aCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS                        PORTS               NAMES8258875263be        ubuntu              "/bin/bash"         3 minutes ago       Exited (127) 3 minutes ago                        mad_mestorf[reason@localhost ~]$ sudo docker rm 8258875263be8258875263be[reason@localhost ~]$# 删除(NULL) 容器root@iZ28ikebrg6Z:/var/run# docker images  REPOSITORY             TAG                 IMAGE ID            CREATED             VIRTUAL SIZE  &lt;none&gt;                 &lt;none&gt;              def2e0b08cbc        About an hour ago   1.37 GB  sameersbn/redmine      latest              f0bec095f291        2 hours ago         614.6 MB  root@iZ28ikebrg6Z:/var/run# docker ps -a  CONTAINER ID        IMAGE                    COMMAND                CREATED             STATUS                     PORTS               NAMES  5d6373cb79e6        224b40d4b89f             "/bin/sh -c 'apt-get   25 hours ago        Exited (0) 25 hours ago                        distracted_blackwell    root@iZ28ikebrg6Z:/var/run# docker rm 5d63  5d63# 镜像导出[reason@localhost ~]$ sudo docker ps -aCONTAINER ID        IMAGE                 COMMAND                  CREATED             STATUS                         PORTS               NAMESd4e581ba6af8        mofun/spark_rc:v1.0   "/bin/bash"              11 seconds ago      Exited (0) 7 seconds ago                           hungry_pasteur[reason@localhost ~]$ sudo docker export d4e581ba6af8 &gt; mofunspark_v1.0.tar# 和导入恢复[reason@localhost ~]$ cat mofunspark_v1.0.tar | sudo docker import - mofun/spark:v1.0执行Scala交互模式时出错时，需要检查root@cloud1:/usr/local/jdk1.8.0_60/jre/lib/ext# ls -latotal 25632drwxr-xr-x.  3  501 staff     4096 Oct 19 02:35 .drwxr-xr-x. 15  501 staff     4096 Oct 18 03:34 ..-rw-r--r--.  1  501 staff  3860522 Aug  4 19:29 cldrdata.jar-rw-r--r--.  1  501 staff     8286 Aug  4 19:29 dnsns.jar-rw-r--r--.  1  501 staff    44516 Aug  4 19:29 jaccess.jar-rwxr-xr-x.  1  501 staff 18464934 Aug  3 17:58 jfxrt.jar-rw-r--r--.  1  501 staff  1178935 Aug  4 19:29 localedata.jar-rw-r--r--.  1  501 staff     1269 Aug  4 19:29 meta-index-rw-r--r--.  1  501 staff  2014239 Aug  4 19:29 nashorn.jar-rw-r--r--.  1  501 staff    39771 Aug  4 19:29 sunec.jar-rw-r--r--.  1  501 staff   278680 Aug  4 19:29 sunjce_provider.jar-rw-r--r--.  1  501 staff   250826 Aug  4 19:29 sunpkcs11.jardrwxr-xr-x.  2 root root      4096 Oct 19 02:35 tmp-rw-r--r--.  1  501 staff    68848 Aug  4 19:29 zipfs.jarroot@cloud1:/usr/local/jdk1.8.0_60/jre/lib/ext## 该目录下出现了很多类似._jfxrt.jar 的包，直接予以删除即可。参考文献  http://eksliang.iteye.com/blog/2226986  http://dockerpool.com/static/books/docker_practice/container/daemon.html  http://dockerpool.com/static/books/docker_practice/install/ubuntu.html  http://dockerpool.com/static/books/docker_practice/image/create.html  http://dockerpool.com/static/books/docker_practice/image/save_load.html  http://dockerpool.com/static/books/docker_practice/container/rm.html  http://blog.csdn.net/qq1010885678/article/details/46353101  http://cn.soulmachine.me/blog/20131027/  http://my.oschina.net/zjzhai/blog/225112  http://blog.csdn.net/minimicall/article/details/40188251  http://www.scala-lang.org/documentation/  https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/streaming/NetworkWordCount.scala  http://spark.apache.org/docs/latest/  https://docs.sigmoidanalytics.com/index.php/Error:_Failed_to_initialize_compiler:_object_scala_not_found.  http://docs.docker.com/linux/step_one/  http://blog.sequenceiq.com/blog/2015/01/09/spark-1-2-0-docker/]]></content>
      <categories>
        
      </categories>
      <tags>
        
          <tag> Spark </tag>
        
          <tag> Docker </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Nginx Logrotate]]></title>
      <url>/2015/12/04/nginx-logrotate/</url>
      <content type="text"><![CDATA[Centos Logrotate关于logrotate is  designed to ease administration of systems that generatelarge numbers of log files.  It allows automatic rotation, compression,removal, and mailing of log files.  Each log file may be handled daily,weekly, monthly, or when it grows too large. (Logrotate man page)配置文件内容示例[reason@online_http:/etc/logrotate.d]$ cat nginx.logrotate.d## author: reason@mofunsky.com# date:   2015-03-31 10:15# cron:#       59 23 * * * /usr/sbin/logrotate /etc/logrotate.conf &gt; /dev/null## location:#       /etc/logrotate.d/nginx.logrotate.d#/local/nginx*.log {    daily    rotate 10    missingok    compress    notifempty    sharedscripts    postrotate        /bin/kill -USR1 $(cat /var/nginx.pid 2&gt;/dev/null) 2&gt;/dev/null || :    endscript}存放位置[reason@online_http:/etc/logrotate.d]$ pwd/etc/logrotate.d[reason@online_http:/etc/logrotate.d]$执行方式和时间# add by reason @ 2015-04-05 20:41# logrotate nginx log daily59      23      *       *       *       root /usr/sbin/logrotate /etc/logrotate.conf &gt;/dev/null 2&gt;&amp;1执行效果# 会按照日期，在每天的23：59分将改天日志转储压缩为*.gz文件[reason@online_http:/local/nginx_access_log]$ ls -lshtotal 5.1G427M -rwxrwxrwx 1 nginx nginx 427M Oct 11 12:01 nginx_me_access.log259M -rwxrwxrwx 1 nginx nginx 259M Oct  1 23:59 nginx_me_access.log-20151001.gz233M -rwxrwxrwx 1 nginx nginx 233M Oct  2 23:59 nginx_me_access.log-20151002.gz228M -rwxrwxrwx 1 nginx nginx 228M Oct  3 23:59 nginx_me_access.log-20151003.gz225M -rwxrwxrwx 1 nginx nginx 225M Oct  4 23:59 nginx_me_access.log-20151004.gz232M -rwxrwxrwx 1 nginx nginx 232M Oct  5 23:59 nginx_me_access.log-20151005.gz另外一种执行方式logrotate 是linux系统的缺省安装命令，初始化状态下即存在缺省的配置信息，其中包括执行文件和时间[reason@online_http:/local/nginx_access_log]$ cd /etc/logrotate.d/[reason@online_http:/etc/logrotate.d]$ lsnginx.logrotate.d[reason@online_http:/etc/logrotate.d]$ pwd/etc/logrotate.d[reason@online_http:/etc/logrotate.d]$比如在/etc/logrotate.d目录下会缺省放置一批需要转储的日志服务对应的配置文件，比如httpd，exim等，logrotate会按照配置文件信息按照既定时间对其产生的日志数据进行转储操作。[reason@online_http:/etc/logrotate.d]$ pwd/etc/logrotate.d[reason@online_http:/etc/logrotate.d]$ cat yum/var/log/yum.log {    missingok    notifempty    size 30k    yearly    create 0600 root root}[reason@online_http:/etc/logrotate.d]$具体参数涵义可参考Logrotate man page缺省状态下，logrotate的自动执行分别被放入了如下几个文件中  cron.daily  cron.weekly  cron.monthly而对于不同的linux发行版本以上脚本可能被放置在/etc/crontab中或者[root@i-bntub2bp logrotate.d]# cat /etc/anacrontab# /etc/anacrontab: configuration file for anacron# See anacron(8) and anacrontab(5) for details.SHELL=/bin/shPATH=/sbin:/bin:/usr/sbin:/usr/binMAILTO=root# the maximal random delay added to the base delay of the jobsRANDOM_DELAY=45# the jobs will be started during the following hours onlySTART_HOURS_RANGE=3-22#period in days   delay in minutes   job-identifier   command1       5       cron.daily              nice run-parts /etc/cron.daily7       25      cron.weekly             nice run-parts /etc/cron.weekly@monthly 45     cron.monthly            nice run-parts /etc/cron.monthly[root@i-bntub2bp logrotate.d]#针对[root@i-bntub2bp logrotate.d]# cat /etc/redhat-releaseCentOS release 6.6 (Final)[root@i-bntub2bp logrotate.d]#而言，可以直接在/etc/anacrontab中找到如上信息。大体解释下anacrontab的部分参数涵义START_HOURS_RANGE=3-22  # 执行时间为3点到22点之间RANDOM_DELAY=45 # 时间处于可执行区间内随机延迟45分钟之内任意时间1       5       cron.daily              nice run-parts /etc/cron.daily# 执行时间为3点到22点之间执行/etc/cron.daily脚本 (after reboot and after the machine has been up for 5 minutes^^), 如果没有重启服务，则会在3：05之后执行。参考文献  http://huoding.com/2013/04/21/246  http://serverfault.com/questions/135906/when-does-cron-daily-run  http://www.cyberciti.biz/faq/linux-when-does-cron-daily-weekly-monthly-run/  http://linuxcommand.org/man_pages/logrotate8.html  https://www.centos.org/docs/2/rhl-cg-en-7.2/anacron.html]]></content>
      <categories>
        
      </categories>
      <tags>
        
          <tag> nginx </tag>
        
          <tag> logrotate </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
</search>
