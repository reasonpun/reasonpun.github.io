<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[Tensor Cell]]></title>
  <subtitle><![CDATA[还要再走500里]]></subtitle>
  <link href="/atom.xml" rel="self"/>
  <link href="http://reasonpun.com/"/>
  <updated>2015-12-25T03:28:01.000Z</updated>
  <id>http://reasonpun.com/</id>
  
  <author>
    <name><![CDATA[reasono]]></name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title><![CDATA[Oryx2 Admin Docs]]></title>
    <link href="http://reasonpun.com/2015/12/21/Oryx2-Admin-Docs/"/>
    <id>http://reasonpun.com/2015/12/21/Oryx2-Admin-Docs/</id>
    <published>2015-12-21T08:53:46.000Z</published>
    <updated>2015-12-25T03:28:01.000Z</updated>
    <content type="html"><![CDATA[<!--  -->
<!--  -->
<!--  _ __    __     __      ____    ___     ___   _____   __  __    ___     -->
<!-- /\`'__\/'__`\ /'__`\   /',__\  / __`\ /' _ `\/\ '__`\/\ \/\ \ /' _ `\   -->
<!-- \ \ \//\  __//\ \L\.\_/\__, `\/\ \L\ \/\ \/\ \ \ \L\ \ \ \_\ \/\ \/\ \  -->
<!--  \ \_\\ \____\ \__/.\_\/\____/\ \____/\ \_\ \_\ \ ,__/\ \____/\ \_\ \_\ -->
<!--   \/_/ \/____/\/__/\/_/\/___/  \/___/  \/_/\/_/\ \ \/  \/___/  \/_/\/_/ -->
<!--                                                 \ \_\                   -->
<!--                                                  \/_/                   -->
<!--  -->
<h3 id="Oryx_2-1-0__u7CFB_u7EDF_u8981_u6C42"><a href="#Oryx_2-1-0__u7CFB_u7EDF_u8981_u6C42" class="headerlink" title="Oryx 2.1.0 系统要求"></a>Oryx 2.1.0 系统要求</h3><ul>
<li>Java 7 or later (JRE only is required)</li>
<li>A Hadoop cluster running the following components:<ul>
<li>Apache Hadoop 2.6.0 or later</li>
<li>Apache Zookeeper 3.4.5 or later</li>
<li>Apache Kafka 0.8.2 or later (in 0.8.x line)</li>
<li>Apache Spark 1.5.0 or later</li>
</ul>
</li>
</ul>
<h3 id="u670D_u52A1"><a href="#u670D_u52A1" class="headerlink" title="服务"></a>服务</h3><p>Hadoop cluster 服务</p>
<ul>
<li>HDFS</li>
<li>YARN</li>
<li>Zookeeper</li>
<li>Kafka</li>
<li>Spark (on YARN)</li>
</ul>
<p>Note that for CDH, Kafka is available as a parcel from <a href="http://www.cloudera.com/content/cloudera/en/developers/home/cloudera-labs/apache-kafka.html" target="_blank" rel="external">Cloudera Labs</a>.</p>
<p>Kafka brokers 需要配置在集群中，根据实例，需要注意hosts和端口。端口一般会设置为9092，此处和ZK server的端口保持一致，缺省设置为2181。缺省端口会在随后的例子中使用。</p>
<p>多个hosts需要通过都好分割，并需要提供host:port 这种方式，比如 ： your-zk-1:2181,your-zk-2:2181。</p>
<p>同时需要注意，你的ZK实例是否使用了chroot path。这是一个简单的路径前缀，比如 your-zk:2181/your-chroot<br>/kafka 是经常用到作为前缀的。如果没有使用chroot，就可以忽略这个。注意：如果存在多个ZK server，和一个chroot，只需要在最后添加一次chroot即可，比如 your-zk-1:2181,your-zk-2:2181/kafka</p>
<h3 id="u914D_u7F6EKafka"><a href="#u914D_u7F6EKafka" class="headerlink" title="配置Kafka"></a>配置Kafka</h3><p>Oryx 使用2个Kafka topics 做数据传输。</p>
<ul>
<li>一个传输输入数据到批量处理，和实时计算层</li>
<li>另一个同步模型更新到服务层</li>
</ul>
<p>缺省的topics的名称分别为OryxInput 和 OryxUpdate，只有Oryx服务启动后才能创建这两个topics。</p>
<p>输入topic的分区数目会影响消费数据的Spark Streaming 作业的分区数，甚至是并发数。比如，批量处理层读取HDFS上的历史数据分区和Kafka数据。</p>
<p>如果输入topic只有一个分区，且在每个时间间隔内有大量的数据涌入，这是Kafka基于的输入分区相对的需要很长的时间去处理。一个比较合适的经验值是选择一些topic分区，在一个批处理时间间隔内到达的大量数据，大约是一个HDFS 块大小，缺省值是128MB。</p>
<p>提供的oryx-run.sh kafka设置脚本，缺省设置为4个分区，当然了，这个值之后是可以修改的。必须注意不能设置更新topic多于1个分区。</p>
<p>重复因子可以设置为任何值，但是建议最少是2。注意：重复因子数目不能超过Kafka brokers在集群上的数目。所以提供的设置脚本里缺省设置重复因子为1. 之后你可以通过kafka-topics –zookeeper … –alter –topic … –replication-factor N 等修改这些缺省值。</p>
<p>你需要为其中一个或者两个topic配置持续时间。尤其重要的是需要限制更新topic的持续时间，因为实时计算层和服务层需要从启动开始的起点获取整个topic。这个机制并如不输入数据重要，输入数据不会再次从头读取数据。</p>
<p>设置这个值为批量处理层更新间隔的2倍是比较合适。比如设置该值为1天（24 <em> 60 </em> 60 * 1000 = 86400000 ms），设置topic的为86400000ms。这个可以通过oryx-run.sh设置脚本自动设置。</p>
<p>上述两个topics会包含大量信息；尤其是更新topic包含整个序列化的PMML模型。很有可能他会超过Kafka缺省最大消息的大小（kafka消息最大1Mib）。如果有更大的数据则需要设置topic’s max.message.bytes。oryx-run.sh Kafka设置脚本设置更新topic缺省为16Mib。这也是Oryx试图吸入更新topic里的模型默认最大值；更大的模型只会保存文件在HDFS中的路径。请查看属性oryx.update-topic.message.max-size。</p>
<p>Kafka代理的message.max.bytes属性可以控制这个，但是设置这个值会影响到代理管理的所有的topics，甚至包括不良状态的topics。可以查看性能和资源部分了解更多。尤其是，需要注意的是必须设置代理的replica.fetch.max.bytes属性，以防止重复任何非常大的消息。</p>
<blockquote>
<p>There is no per-topic equivalent to this.</p>
</blockquote>
<h3 id="Kafka_u914D_u7F6E_u81EA_u52A8_u8BBE_u7F6E"><a href="#Kafka_u914D_u7F6E_u81EA_u52A8_u8BBE_u7F6E" class="headerlink" title="Kafka配置自动设置"></a>Kafka配置自动设置</h3><p>提供的oryx-run.sh脚本可以打印ZK的当前配置，列出已经存在的Kafka中的topics，如果需要，会创建配置好的输入topics和更新topics。</p>
<p>你需要先创建Oryx配置文件，或者可以拷贝conf/als-example.conf。需要按照要求修改Kafka和ZK的配置文件，比如topic名称。</p>
<p>oryx.conf文件需要和每个层的JAR文件放在同一个目录下，然后执行：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">./oryx-run.sh kafka-setup</span><br><span class="line"></span><br><span class="line">Input  ZK:    your-zk:<span class="number">2181</span></span><br><span class="line">Input  Kafka: your-kafka:<span class="number">9092</span></span><br><span class="line">Input  topic: OryxInput</span><br><span class="line">Update ZK:    your-zk:<span class="number">2181</span></span><br><span class="line">Update Kafka: your-kafka:<span class="number">9092</span></span><br><span class="line">Update topic: OryxUpdate</span><br><span class="line"></span><br><span class="line">All available topics:</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Input topic OryxInput does not exist. Create it? y</span><br><span class="line">Creating topic OryxInput</span><br><span class="line">Created topic <span class="string">"OryxInput"</span>.</span><br><span class="line">Status of topic OryxInput:</span><br><span class="line">Topic:OryxInput	PartitionCount:<span class="number">4</span>	ReplicationFactor:<span class="number">1</span>	Configs:</span><br><span class="line">	Topic: OryxInput	Partition: <span class="number">0</span>	Leader: <span class="number">120</span>	Replicas: <span class="number">120</span>,<span class="number">121</span>	Isr: <span class="number">120</span>,<span class="number">121</span></span><br><span class="line">	Topic: OryxInput	Partition: <span class="number">1</span>	Leader: <span class="number">121</span>	Replicas: <span class="number">121</span>,<span class="number">120</span>	Isr: <span class="number">121</span>,<span class="number">120</span></span><br><span class="line">	Topic: OryxInput	Partition: <span class="number">2</span>	Leader: <span class="number">120</span>	Replicas: <span class="number">120</span>,<span class="number">121</span>	Isr: <span class="number">120</span>,<span class="number">121</span></span><br><span class="line">	Topic: OryxInput	Partition: <span class="number">3</span>	Leader: <span class="number">121</span>	Replicas: <span class="number">121</span>,<span class="number">120</span>	Isr: <span class="number">121</span>,<span class="number">120</span></span><br><span class="line"></span><br><span class="line">Update topic OryxUpdate does not exist. Create it? y</span><br><span class="line">Creating topic OryxUpdate</span><br><span class="line">Created topic <span class="string">"OryxUpdate"</span>.</span><br><span class="line">Updated config <span class="keyword">for</span> topic <span class="string">"OryxUpdate"</span>.</span><br><span class="line">Status of topic OryxUpdate:</span><br><span class="line">Topic:OryxUpdate	PartitionCount:<span class="number">1</span>	ReplicationFactor:<span class="number">1</span>	Configs:retention.ms=<span class="number">86400000</span>,max.message.bytes=<span class="number">16777216</span></span><br><span class="line">	Topic: OryxUpdate	Partition: <span class="number">0</span>	Leader: <span class="number">120</span>	Replicas: <span class="number">120</span>,<span class="number">121</span>	Isr: <span class="number">120</span>,<span class="number">121</span></span><br></pre></td></tr></table></figure>
<p>查看发送到输入和更新topic，监控应用的动作，可以执行：</p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">./oryx-run.sh kafka-tail</span><br><span class="line">Input  <span class="string">ZK:</span>    your-<span class="string">zk:</span><span class="number">2181</span></span><br><span class="line">Input  <span class="string">Kafka:</span> your-<span class="string">kafka:</span><span class="number">9092</span></span><br><span class="line">Input  <span class="string">topic:</span> OryxInput</span><br><span class="line">Update <span class="string">ZK:</span>    your-<span class="string">zk:</span><span class="number">2181</span></span><br><span class="line">Update <span class="string">Kafka:</span> your-<span class="string">kafka:</span><span class="number">9092</span></span><br><span class="line">Update <span class="string">topic:</span> OryxUpdate</span><br><span class="line"></span><br><span class="line">...output...</span><br></pre></td></tr></table></figure>
<p>接着在另外一个窗口，可以接受输入数据，比如将来自终端用户的文档data.csv加入到输入队列，并验证：</p>
<figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./oryx-<span class="keyword">run</span>.<span class="keyword">sh</span> kafka-<span class="keyword">input</span> --<span class="keyword">input</span>-<span class="keyword">file</span> data.csv</span><br></pre></td></tr></table></figure>
<p>如果以上全部成功了，可以关闭这些进程。集群至此已经准备好运行Oryx了。</p>
<h3 id="HDFS_u548C_u6570_u636E_u5C42"><a href="#HDFS_u548C_u6570_u636E_u5C42" class="headerlink" title="HDFS和数据层"></a>HDFS和数据层</h3><p>在Oryx中，Kafka是数据传输的途径，因此数据在Kafka中只是需要暂时存放的。然而输入数据也会持久化到HDFS中以备之后使用。同样的，模型和更新用来为Kafka的更新topic提供数据，模型也会持久化到HDFS以备之后引用。</p>
<p>oryx.batch.storage.data-dir定义了输入数据存放在HDFS中的位置。在这个目录下，子目录标题会以oryx-[timestamp].data形式创建，每一个会通过Spark Straming在批量处理层执行。在这里，时间戳格式格式与Unix相同，且以毫秒为单位。</p>
<p>实际上，和大多数Hadoop中分布式式进程输出的『文件』一样，存在这样的一个子目录，包含了很多以part-开头的文件。每个文件都是序列化文件，通过Writable类序列化Kafka输入topics的键值，Writable类实现自oryx.batch.storage.key-writable-class 和 oryx.batch.storage.message-writable-class类。默认情况下，这是TextWritable，并且keys和消息是以字符串形式被记录下来的。</p>
<p>该目录下的数据会被删除。也就不会再次被批处理层计算使用。尤其是，设置oryx.batch.storage.max-age-data-hours为一个非负数，将会使批处理层自动删除大于给定时间的数据。</p>
<p>同样的，在每个批处理间隔内被批处理层选中的模型会被原始的机器学习应用（扩展子MLUpdate）输出。也会被持久化到oryx.batch.storage.model-dir定义的目录下的子目录中。在这个目录下，子目录的命名都是以时间戳形式实现的，同Unix毫秒形式。</p>
<p>子目录下的内容取决于应用，但是一般会包含以model.pmml命名的PMML模块，并且和模块一起存在的可选的追加文件。</p>
<p>这个目录之所以存在是因为需要记录PMML模块用来归档用或者被其他工具使用。也可按照规则删除其内容。</p>
<h3 id="u6355_u83B7_u9519_u8BEF"><a href="#u6355_u83B7_u9519_u8BEF" class="headerlink" title="捕获错误"></a>捕获错误</h3><p>最后，你可能希望停止其中一个或者几个层的运行，或者重启。服务也可能挂鸟。这到底发生了神马？为啥会这样捏？</p>
<h4 id="u6570_u636E_u4E22_u5931"><a href="#u6570_u636E_u4E22_u5931" class="headerlink" title="数据丢失"></a>数据丢失</h4><p>历史数据存放在HDFS中，理论上会存放多个副本。HDFS会确保数据被靠谱的存放着。当设置了副本，Kafka也被设计为采用副本方式应对故障。</p>
<p>这并没有啥鸟用，这样不能确保数据不会丢失，只能依靠HDFS和Kafka能正常可用罢了。</p>
<h4 id="u670D_u52A1_u5668_u6302_u9E1F"><a href="#u670D_u52A1_u5668_u6302_u9E1F" class="headerlink" title="服务器挂鸟"></a>服务器挂鸟</h4><p>通常情况下，所有的三层服务进程应该会持续的工作，如果不得不停止或者挂鸟的话，服务自己会立即重启。这个可以通过初始化脚本完美完成或者类似机制（尚未实现鸟）</p>
<h5 id="u670D_u52A1_u5C42"><a href="#u670D_u52A1_u5C42" class="headerlink" title="服务层"></a>服务层</h5><p>服务层是无状态的。启动后，他会读取更新topics中的所有的模型和可用更新。当首个可用的模型就绪后，就可以开始应答请求。基于这个原因，需要适当的限制更新topic的持续时间。</p>
<p>服务层的操作不是分布式的，每个实例是独立的，启动和停止不会影响到其他部分。</p>
<h5 id="u5B9E_u65F6_u8BA1_u7B97_u5C42"><a href="#u5B9E_u65F6_u8BA1_u7B97_u5C42" class="headerlink" title="实时计算层"></a>实时计算层</h5><p>实时计算层同样也不存在状态，也会在读取全部的模型和更新topic的更新。只要存在合法的模型，就可以生成更新。同时，从最后一次偏移位置开始读取输入topics。</p>
<p>实时计算层使用Spark和Spark Streaming 做计算。Spark会响应计算过程中失败情况，并重试任务。</p>
<p>Spark Streaming的Kafka集成模块在某些情况下可以恢复接收的故障。<br>如果是整个进程死掉并被重新启动，oryx.id的值被设定以后，系统会自动从上一次Kafka记录的偏移地址开始读取。（否则，将会从上次偏移地址开始，这就意味着实时计算层没有运行的时候，到达的数据就不会生成任何更新。），同样的，如果实时计算层的模型还没有准备好的话，收到的数据也会被忽略。It effectively adopts “at most once” semantics.</p>
<p>由于实时计算层的作用是为最后发布的模型提供approximate, “best effort”的更新。这种行为由于其间接性一般是没有问题，且令人满意的。</p>
<h5 id="u6279_u5904_u7406_u5C42"><a href="#u6279_u5904_u7406_u5C42" class="headerlink" title="批处理层"></a>批处理层</h5><p>批处理层是最复杂的，因为他并生成某些状态：</p>
<ul>
<li>历史数据，总是持久化到HDFS</li>
<li>如果应用选择的话，模型的扩展状态和topics都可以被持久化到HDFS上</li>
</ul>
<p>对于多次或者根本不读取数据是非常敏感的，因为他本来就是生产官方下一代模型的组件</p>
<p>与实时计算层一起，Spark和Spark Streaming在计算过程中可以捕获很多错误情况。也可以管理存储到HDFS中的数据，负责避免两次写入相同数据。</p>
<p>应用负责回复各自的「状态」，一般情况下，建立在Oryx ML层的应用会将状态写入唯一的子目录中，并且在重启后会在新的目录简单的产生一个新状态。前一个状态如果存在的话，也会被完整写入或者被完全忽视。</p>
<p>批处理层也和实时计算层一样，符合『至多一次』的规则。综上，如果整个进程死掉或者被重启，oryx.id被设置的话，则会从Kafka记录的最后一次偏移重新读取，否则会在最后一次偏移处重新读取数据。</p>
]]></content>
    <summary type="html">
    <![CDATA[<!--  -->
<!--  -->
<!--  _ __    __     __      ____    ___     ___   _____   __  __    ___     -->
<!-- /\`'__\/'__`\ /'__`\   /',__\  / _]]>
    </summary>
    
      <category term="Admin" scheme="http://reasonpun.com/tags/Admin/"/>
    
      <category term="Hadoop" scheme="http://reasonpun.com/tags/Hadoop/"/>
    
      <category term="Oryx2" scheme="http://reasonpun.com/tags/Oryx2/"/>
    
      <category term="Spark" scheme="http://reasonpun.com/tags/Spark/"/>
    
      <category term="Spark-Streaming" scheme="http://reasonpun.com/tags/Spark-Streaming/"/>
    
      <category term="documentation" scheme="http://reasonpun.com/tags/documentation/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Oryx2 Overview]]></title>
    <link href="http://reasonpun.com/2015/12/21/Oryx2-Overview/"/>
    <id>http://reasonpun.com/2015/12/21/Oryx2-Overview/</id>
    <published>2015-12-21T03:37:05.000Z</published>
    <updated>2015-12-25T03:30:40.000Z</updated>
    <content type="html"><![CDATA[<!--  -->
<!--  -->
<!--  _ __    __     __      ____    ___     ___   _____   __  __    ___     -->
<!-- /\`'__\/'__`\ /'__`\   /',__\  / __`\ /' _ `\/\ '__`\/\ \/\ \ /' _ `\   -->
<!-- \ \ \//\  __//\ \L\.\_/\__, `\/\ \L\ \/\ \/\ \ \ \L\ \ \ \_\ \/\ \/\ \  -->
<!--  \ \_\\ \____\ \__/.\_\/\____/\ \____/\ \_\ \_\ \ ,__/\ \____/\ \_\ \_\ -->
<!--   \/_/ \/____/\/__/\/_/\/___/  \/___/  \/_/\/_/\ \ \/  \/___/  \/_/\/_/ -->
<!--                                                 \ \_\                   -->
<!--                                                  \/_/                   -->
<!--  -->
<h3 id="u7B80_u4ECB"><a href="#u7B80_u4ECB" class="headerlink" title="简介"></a>简介</h3><img src="/images/oryx/OryxLogoMedium.png" title="oryx2">
<p>Oryx2是专注于进行大规模，实时机器学习框架，遵循lambda规则，基于Apache Spark和Apache Kafka构建。</p>
<p>Oryx 不仅是构建应用程序的框架，而且包含 协同过滤，分类，回归和聚类的打包的端到端的应用。</p>
<p>包含3层</p>
<ul>
<li>lambda层<ul>
<li>批量处理</li>
<li>快速处理</li>
<li>服务</li>
</ul>
</li>
<li>ML抽象层</li>
<li>端到端实现层</li>
</ul>
<p>从另一个角度，可以看成是一系列链接的元素</p>
<ul>
<li>批处理层：依据历史数据进行离线处理</li>
<li>实时处理层：通过增量数据流，实时更新结果</li>
<li>服务层：通过模型的传递实现异步查询API</li>
<li>数据传输层：在外部数据源与处理层之间传输数据</li>
</ul>
<p>The project</p>
<ul>
<li>may be reused tier by tier for example, the packaged app tier can be ignored, and it can be a framework for building new ML applications.</li>
<li>It can be reused layer by layer too: for example, the Speed Layer can be omitted if a deployment does not need incremental updates.</li>
<li>It can be modified piece-by-piece too: the collaborative filtering application’s model-building batch layer could be swapped for a custom implementation based on a new algorithm outside Spark MLlib while retaining the serving and speed layer implementations.</li>
</ul>
<img src="/images/oryx/Architecture.png" title="Architecture">
<h3 id="Lambda_u5C42_u5B9E_u73B0"><a href="#Lambda_u5C42_u5B9E_u73B0" class="headerlink" title="Lambda层实现"></a>Lambda层实现</h3><h4 id="u6570_u636E_u4F20_u8F93"><a href="#u6570_u636E_u4F20_u8F93" class="headerlink" title="数据传输"></a>数据传输</h4><p>  数据传输机制其实就是一个Kafka的Topic。任何一个进程（包含且不局限与服务层）都可以向topic中写入数据，并通过实时处理和批处理层查看。<br>  Kafka Topic也可以用来模型和模型之间的更新，并被实时处理层和服务层消费。</p>
<h4 id="u6279_u5904_u7406_u5C42"><a href="#u6279_u5904_u7406_u5C42" class="headerlink" title="批处理层"></a>批处理层</h4><p>  批处理层是以Spark Streaming进程的方式实现的，运行在Hadoop Cluster节点上，并读取来自Kafka topic的输入数据。 Streaming 进程会有一个很长的运行周期-若干小时甚至一天。会使用Spark存储当前会话数据到HDFS中，然后合并HDFS上的所有历史数据，之后重新初始化构建新的结果数据。并将新的结果重新写入HDFS，同时发不到Kafka更新topic中。</p>
<h4 id="u5B9E_u65F6_u5904_u7406_u5C42"><a href="#u5B9E_u65F6_u5904_u7406_u5C42" class="headerlink" title="实时处理层"></a>实时处理层</h4><p>  实时处理层也是由Spark Streaming进程实现的，同样读取Kafka topic输入数据。但是他存在比较短的运行周期，比如秒级别。会持续消费更新topic中的新模型，并生产新的模型。也会回写更新topic。</p>
<h4 id="u670D_u52A1_u5C42"><a href="#u670D_u52A1_u5C42" class="headerlink" title="服务层"></a>服务层</h4><p>  服务层监听更新topic上的模型以及模型更新。在内存中持久化模型状态。<br>  会暴露顶层方法的 HTTP REST API 用于查询内存中的模型。大部分接口都支持大规模的部署。<br>  每个接口都可以接收新的数据并写入Kafka，以此在实时处理层和批处理层可见。</p>
<h4 id="u914D_u7F6E_u548C_u90E8_u7F72"><a href="#u914D_u7F6E_u548C_u90E8_u7F72" class="headerlink" title="配置和部署"></a>配置和部署</h4><p>  程序是基于Java实现的，依赖</p>
<pre><code>* Spark 1.3.x+
* Hadoop 2.6.x+
* Tomcat 8.x+
* Kafka 0.8.2+
* Zookeeper 等。
</code></pre><p>  配置文件通过 <a href="https://github.com/typesafehub/config" target="_blank" rel="external">Typesafe Config</a> 的方式实现整个系统的部署配置。<br>  包括： 批处理，实时处理，服务层逻辑关键的接口类的实现</p>
<p>  每个层的二进制形式分开进行打包和部署的，每个都是以可执行的Java的jar包的形式存在并包含所有必须的服务。</p>
<h3 id="ML_u5C42_u5B9E_u73B0"><a href="#ML_u5C42_u5B9E_u73B0" class="headerlink" title="ML层实现"></a>ML层实现</h3><p>  ML层对上述通用接口方法做了简单的专一话的实现，实现了通用ML需求，并且对应用暴露了机器学习特有的接入接口。</p>
<p>  举个例子，实现了批量处理层，用于自动更新测试集和训练集进程。可以调用应用提供的函数来评估测试机模型。通过尝试不同的超参数值，选择出最佳结果。通过PMML管理模型的序列号。</p>
<h3 id="u7AEF_u5230_u7AEF_u5E94_u7528_u5B9E_u73B0"><a href="#u7AEF_u5230_u7AEF_u5E94_u7528_u5B9E_u73B0" class="headerlink" title="端到端应用实现"></a>端到端应用实现</h3><p>  除了作为一种框架，Oryx2 包含完整的三中机器学习需要的批处理层，实时处理层，服务层。<br>  开箱即用，或者作为自定义程序的基础：</p>
<pre><code>* 基于最小二乘法的协同过滤/推荐
* 基于k-means的聚类
* 基于随机决策森林的分类和回归
</code></pre><h3 id="u53C2_u8003_u6587_u732E"><a href="#u53C2_u8003_u6587_u732E" class="headerlink" title="参考文献"></a>参考文献</h3><ul>
<li><a href="http://oryx.io/index.html" target="_blank" rel="external">http://oryx.io/index.html</a></li>
<li><a href="http://www.ivanopt.com/oryx-document%E7%BF%BB%E8%AF%91/" target="_blank" rel="external">http://www.ivanopt.com/oryx-document%E7%BF%BB%E8%AF%91/</a></li>
<li><a href="http://jameskinley.tumblr.com/post/37398560534/the-lambda-architecture-principles-for" target="_blank" rel="external">http://jameskinley.tumblr.com/post/37398560534/the-lambda-architecture-principles-for</a></li>
<li><a href="http://dmg.org/pmml/v4-1/GeneralStructure.html" target="_blank" rel="external">http://dmg.org/pmml/v4-1/GeneralStructure.html</a></li>
<li><a href="http://blog.csdn.net/nxcjh321/article/details/24796879" target="_blank" rel="external">http://blog.csdn.net/nxcjh321/article/details/24796879</a></li>
<li><a href="http://youngfor.me/post/recsys/oryx-tui-jian-xi-tong-chu-ti-yan" target="_blank" rel="external">http://youngfor.me/post/recsys/oryx-tui-jian-xi-tong-chu-ti-yan</a></li>
<li><a href="https://github.com/OryxProject/oryx" target="_blank" rel="external">https://github.com/OryxProject/oryx</a></li>
</ul>
]]></content>
    <summary type="html">
    <![CDATA[<!--  -->
<!--  -->
<!--  _ __    __     __      ____    ___     ___   _____   __  __    ___     -->
<!-- /\`'__\/'__`\ /'__`\   /',__\  / _]]>
    </summary>
    
      <category term="Hadoop" scheme="http://reasonpun.com/tags/Hadoop/"/>
    
      <category term="Java" scheme="http://reasonpun.com/tags/Java/"/>
    
      <category term="Oryx2" scheme="http://reasonpun.com/tags/Oryx2/"/>
    
      <category term="Recommendation" scheme="http://reasonpun.com/tags/Recommendation/"/>
    
      <category term="Spark" scheme="http://reasonpun.com/tags/Spark/"/>
    
      <category term="Spark-Streaming" scheme="http://reasonpun.com/tags/Spark-Streaming/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Genetic Algorithm]]></title>
    <link href="http://reasonpun.com/2015/12/14/Genetic-Algorithm/"/>
    <id>http://reasonpun.com/2015/12/14/Genetic-Algorithm/</id>
    <published>2015-12-14T08:25:10.000Z</published>
    <updated>2015-12-25T03:29:45.000Z</updated>
    <content type="html"><![CDATA[<!--  -->
<!--  -->
<!--  _ __    __     __      ____    ___     ___   _____   __  __    ___     -->
<!-- /\`'__\/'__`\ /'__`\   /',__\  / __`\ /' _ `\/\ '__`\/\ \/\ \ /' _ `\   -->
<!-- \ \ \//\  __//\ \L\.\_/\__, `\/\ \L\ \/\ \/\ \ \ \L\ \ \ \_\ \/\ \/\ \  -->
<!--  \ \_\\ \____\ \__/.\_\/\____/\ \____/\ \_\ \_\ \ ,__/\ \____/\ \_\ \_\ -->
<!--   \/_/ \/____/\/__/\/_/\/___/  \/___/  \/_/\/_/\ \ \/  \/___/  \/_/\/_/ -->
<!--                                                 \ \_\                   -->
<!--                                                  \/_/                   -->
<!--  -->
]]></content>
    <summary type="html">
    <![CDATA[<!--  -->
<!--  -->
<!--  _ __    __     __      ____    ___     ___   _____   __  __    ___     -->
<!-- /\`'__\/'__`\ /'__`\   /',__\  / _]]>
    </summary>
    
      <category term="Genetic Algorithm" scheme="http://reasonpun.com/tags/Genetic-Algorithm/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[apache-flume-ng-structure]]></title>
    <link href="http://reasonpun.com/2015/12/10/apache-flume-ng-structure/"/>
    <id>http://reasonpun.com/2015/12/10/apache-flume-ng-structure/</id>
    <published>2015-12-10T07:01:57.000Z</published>
    <updated>2015-12-25T03:30:08.000Z</updated>
    <content type="html"><![CDATA[<!--  -->
<!--  -->
<!--  _ __    __     __      ____    ___     ___   _____   __  __    ___     -->
<!-- /\`'__\/'__`\ /'__`\   /',__\  / __`\ /' _ `\/\ '__`\/\ \/\ \ /' _ `\   -->
<!-- \ \ \//\  __//\ \L\.\_/\__, `\/\ \L\ \/\ \/\ \ \ \L\ \ \ \_\ \/\ \/\ \  -->
<!--  \ \_\\ \____\ \__/.\_\/\____/\ \____/\ \_\ \_\ \ ,__/\ \____/\ \_\ \_\ -->
<!--   \/_/ \/____/\/__/\/_/\/___/  \/___/  \/_/\/_/\ \ \/  \/___/  \/_/\/_/ -->
<!--                                                 \ \_\                   -->
<!--                                                  \/_/                   -->
<!--  -->
<h2 id="Apache-flume_NG__u914D_u7F6E"><a href="#Apache-flume_NG__u914D_u7F6E" class="headerlink" title="Apache-flume NG 配置"></a>Apache-flume NG 配置</h2><h3 id="u7B80_u4ECB"><a href="#u7B80_u4ECB" class="headerlink" title="简介"></a>简介</h3><p>  Flume NG是一个分布式、可靠、可用的系统，它能够将不同数据源的海量日志数据进行高效收集、聚合、移动，最后存储到一个中心化数据存储系统中。</p>
<p>  由原来的Flume OG到现在的Flume NG，进行了架构重构，并且现在NG版本完全不兼容原来的OG版本。</p>
<p>  经过架构重构后，Flume NG更像是一个轻量的小工具，非常简单，容易适应各种方式日志收集，并支持failover和负载均衡。</p>
<h3 id="u67B6_u6784_u8BBE_u8BA1_u8981_u70B9"><a href="#u67B6_u6784_u8BBE_u8BA1_u8981_u70B9" class="headerlink" title="架构设计要点"></a>架构设计要点</h3><h4 id="u6838_u5FC3_u6982_u5FF5"><a href="#u6838_u5FC3_u6982_u5FF5" class="headerlink" title="核心概念"></a>核心概念</h4><ul>
<li>Event：一个数据单元，带有一个可选的消息头</li>
<li>Flow：Event从源点到达目的点的迁移的抽象</li>
<li>Client：操作位于源点处的Event，将其发送到Flume Agent</li>
<li>Agent：一个独立的Flume进程，包含组件Source、Channel、Sink</li>
<li>Source：用来消费传递到该组件的Event</li>
<li>Channel：中转Event的一个临时存储，保存有Source组件传递过来的Event</li>
<li>Sink：从Channel中读取并移除Event，将Event传递到Flow Pipeline中的下一个Agent（如果有的话）</li>
</ul>
<h4 id="u67B6_u6784_u56FE"><a href="#u67B6_u6784_u56FE" class="headerlink" title="架构图"></a>架构图</h4><pre><code><img src="/images/flume-ng/flume-ng-architecture.png" title="flume-ng总体结构图">
</code></pre><h4 id="u57FA_u672C_u6D41_u7A0B"><a href="#u57FA_u672C_u6D41_u7A0B" class="headerlink" title="基本流程"></a>基本流程</h4><p>外部系统产生日志，直接通过Flume的Agent的Source组件将事件（如日志行）发送到中间临时的channel组件，最后传递给Sink组件，HDFS Sink组件可以直接把数据存储到HDFS集群上。</p>
<h4 id="u5355Agent"><a href="#u5355Agent" class="headerlink" title="单Agent"></a>单Agent</h4><p>  一个最基本Flow的配置，格式如下：</p>
  <figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># list the sources, sinks and channels for the agent</span></span><br><span class="line"><span class="variable">&lt;Agent&gt;</span>.sources = <span class="variable">&lt;Source1&gt;</span> <span class="variable">&lt;Source2&gt;</span></span><br><span class="line"><span class="variable">&lt;Agent&gt;</span>.sinks = <span class="variable">&lt;Sink1&gt;</span> <span class="variable">&lt;Sink2&gt;</span></span><br><span class="line"><span class="variable">&lt;Agent&gt;</span>.channels = <span class="variable">&lt;Channel1&gt;</span> <span class="variable">&lt;Channel2&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># set channel for source</span></span><br><span class="line"><span class="variable">&lt;Agent&gt;</span>.sources.<span class="variable">&lt;Source1&gt;</span>.channels = <span class="variable">&lt;Channel1&gt;</span> <span class="variable">&lt;Channel2&gt;</span> ...</span><br><span class="line"><span class="variable">&lt;Agent&gt;</span>.sources.<span class="variable">&lt;Source2&gt;</span>.channels = <span class="variable">&lt;Channel1&gt;</span> <span class="variable">&lt;Channel2&gt;</span> ...</span><br><span class="line"></span><br><span class="line"><span class="comment"># set channel for sink</span></span><br><span class="line"><span class="variable">&lt;Agent&gt;</span>.sinks.<span class="variable">&lt;Sink1&gt;</span>.channel = <span class="variable">&lt;Channel1&gt;</span></span><br><span class="line"><span class="variable">&lt;Agent&gt;</span>.sinks.<span class="variable">&lt;Sink2&gt;</span>.channel = <span class="variable">&lt;Channel2&gt;</span></span><br></pre></td></tr></table></figure>
<p>  尖括号里面的，我们可以根据实际需求或业务来修改名称。</p>
<p>  下面详细说明：</p>
<ul>
<li><agent> 表示配置一个Agent的名称，一个Agent肯定有一个名称。</agent></li>
<li><source1>和<source2>是Agent的Source组件的名称，消费传递过来的Event。</source2></source1></li>
<li><channel1>和<channel2>是Agent的Channel组件的名称。</channel2></channel1></li>
<li><p><sink1>与<sink2>是Agent的Sink组件的名称，从Channel中消费（移除）Event。</sink2></sink1></p>
<p>上面配置内容中</p>
</li>
<li><p>第一组中配置Source、Sink、Channel，它们的值可以有1个或者多个；</p>
</li>
<li>第二组中配置Source将把数据存储（Put）到哪一个Channel中，可以存储到1个或多个Channel中，<br>同一个Source将数据存储到多个Channel中，实际上是Replication；</li>
<li>第三组中配置Sink从哪一个Channel中取（Task）数据，一个Sink只能从一个Channel中取数据。</li>
</ul>
<h4 id="u591A_u4E2AAgent_u987A_u5E8F_u8FDE_u63A5"><a href="#u591A_u4E2AAgent_u987A_u5E8F_u8FDE_u63A5" class="headerlink" title="多个Agent顺序连接"></a>多个Agent顺序连接</h4>  <img src="/images/flume-ng/flume-multiseq-agents.png" title="flume-ng 多个Agent顺序连接">
<p>  可以将多个Agent顺序连接起来，将最初的数据源经过收集，存储到最终的存储系统中。这是最简单的情况，一般情况下，应该控制这种顺序连接的Agent的数量，因为数据流经的路径变长了，如果不考虑failover的话，出现故障将影响整个Flow上的Agent收集服务。</p>
<h4 id="u591A_u4E2AAgent_u7684_u6570_u636E_u6C47_u805A_u5230_u540C_u4E00_u4E2AAgent"><a href="#u591A_u4E2AAgent_u7684_u6570_u636E_u6C47_u805A_u5230_u540C_u4E00_u4E2AAgent" class="headerlink" title="多个Agent的数据汇聚到同一个Agent"></a>多个Agent的数据汇聚到同一个Agent</h4>  <img src="/images/flume-ng/flume-join-agent.png" title="flume-ng 多个Agent的数据汇聚到同一个Agent">
<p>  这种情况应用的场景比较多，比如要收集Web网站的用户行为日志，Web网站为了可用性使用的负载均衡的集群模式，每个节点都产生用户行为日志，可以为每个节点都配置一个Agent来单独收集日志数据，然后多个Agent将数据最终汇聚到一个用来存储数据存储系统，如HDFS上。</p>
<h4 id="u591A_u8DEF_uFF08Multiplexing_uFF09Agent"><a href="#u591A_u8DEF_uFF08Multiplexing_uFF09Agent" class="headerlink" title="多路（Multiplexing）Agent"></a>多路（Multiplexing）Agent</h4>  <img src="/images/flume-ng/flume-multiplexing-agent.png" title="flume-ng 多路（Multiplexing）Agent">
<p>  这种模式，有两种方式</p>
<ul>
<li><p>一种是用来复制（Replication）</p>
<ul>
<li><p>Replication方式，可以将最前端的数据源复制多份，分别传递到多个channel中，每个channel接收到的数据都是相同的，配置格式</p>
<figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># List the sources, sinks and channels for the agent</span></span><br><span class="line"><span class="variable">&lt;Agent&gt;</span>.sources = <span class="variable">&lt;Source1&gt;</span></span><br><span class="line"><span class="variable">&lt;Agent&gt;</span>.sinks = <span class="variable">&lt;Sink1&gt;</span> <span class="variable">&lt;Sink2&gt;</span></span><br><span class="line"><span class="variable">&lt;Agent&gt;</span>.channels = <span class="variable">&lt;Channel1&gt;</span> <span class="variable">&lt;Channel2&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># set list of channels for source (separated by space)</span></span><br><span class="line"><span class="variable">&lt;Agent&gt;</span>.sources.<span class="variable">&lt;Source1&gt;</span>.channels = <span class="variable">&lt;Channel1&gt;</span> <span class="variable">&lt;Channel2&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># set channel for sinks</span></span><br><span class="line"><span class="variable">&lt;Agent&gt;</span>.sinks.<span class="variable">&lt;Sink1&gt;</span>.channel = <span class="variable">&lt;Channel1&gt;</span></span><br><span class="line"><span class="variable">&lt;Agent&gt;</span>.sinks.<span class="variable">&lt;Sink2&gt;</span>.channel = <span class="variable">&lt;Channel2&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="variable">&lt;Agent&gt;</span>.sources.<span class="variable">&lt;Source1&gt;</span>.selector.type = replicating</span><br></pre></td></tr></table></figure>
<p>使用的Replication方式，Source1会将数据分别存储到Channel1和Channel2，这两个channel里面存储的数据是相同的，然后数据被传递到Sink1和Sink2。</p>
</li>
</ul>
</li>
<li><p>另一种是用来分流（Multiplexing）</p>
<ul>
<li><p>Multiplexing方式，selector可以根据header的值来确定数据传递到哪一个channel</p>
<figure class="highlight pf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Mapping for multiplexing selector</span></span><br><span class="line"><span class="variable">&lt;Agent&gt;</span>.sources.<span class="variable">&lt;Source1&gt;</span>.selector.type = multiplexing</span><br><span class="line"><span class="variable">&lt;Agent&gt;</span>.sources.<span class="variable">&lt;Source1&gt;</span>.selector.header = <span class="variable">&lt;someHeader&gt;</span></span><br><span class="line"><span class="variable">&lt;Agent&gt;</span>.sources.<span class="variable">&lt;Source1&gt;</span>.selector.mapping.<span class="variable">&lt;Value1&gt;</span> = <span class="variable">&lt;Channel1&gt;</span></span><br><span class="line"><span class="variable">&lt;Agent&gt;</span>.sources.<span class="variable">&lt;Source1&gt;</span>.selector.mapping.<span class="variable">&lt;Value2&gt;</span> = <span class="variable">&lt;Channel1&gt;</span> <span class="variable">&lt;Channel2&gt;</span></span><br><span class="line"><span class="variable">&lt;Agent&gt;</span>.sources.<span class="variable">&lt;Source1&gt;</span>.selector.mapping.<span class="variable">&lt;Value3&gt;</span> = <span class="variable">&lt;Channel2&gt;</span></span><br><span class="line"><span class="comment">#...</span></span><br><span class="line"></span><br><span class="line"><span class="variable">&lt;Agent&gt;</span>.sources.<span class="variable">&lt;Source1&gt;</span>.selector.<span class="keyword">default</span> = <span class="variable">&lt;Channel2&gt;</span></span><br></pre></td></tr></table></figure>
<p>上面selector的type的值为multiplexing，同时配置selector的header信息，还配置了多个selector的mapping的值，即header的值：如果header的值为Value1、Value2，数据从Source1路由到Channel1；如果header的值为Value2、Value3，数据从Source1路由到Channel2。</p>
</li>
</ul>
</li>
</ul>
<h4 id="u5B9E_u73B0load_balance_u529F_u80FD"><a href="#u5B9E_u73B0load_balance_u529F_u80FD" class="headerlink" title="实现load balance功能"></a>实现load balance功能</h4>  <img src="/images/flume-ng/flume-load-balance-agents.png" title="实现load balance功能">
<p>  Load balancing Sink Processor能够实现load balance功能，上图Agent1是一个路由节点，<br>  负责将Channel暂存的Event均衡到对应的多个Sink组件上，而每个Sink组件分别连接到一个独立的Agent上</p>
  <figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a1<span class="class">.sinkgroups</span> = g1</span><br><span class="line">a1<span class="class">.sinkgroups</span><span class="class">.g1</span><span class="class">.sinks</span> = k1 k2 k3</span><br><span class="line">a1<span class="class">.sinkgroups</span><span class="class">.g1</span><span class="class">.processor</span><span class="class">.type</span> = load_balance</span><br><span class="line">a1<span class="class">.sinkgroups</span><span class="class">.g1</span><span class="class">.processor</span><span class="class">.backoff</span> = true</span><br><span class="line">a1<span class="class">.sinkgroups</span><span class="class">.g1</span><span class="class">.processor</span><span class="class">.selector</span> = round_robin</span><br><span class="line">a1<span class="class">.sinkgroups</span><span class="class">.g1</span><span class="class">.processor</span><span class="class">.selector</span><span class="class">.maxTimeOut</span>=<span class="number">10000</span></span><br></pre></td></tr></table></figure>
<h4 id="u5B9E_u73B0failover_u80FD"><a href="#u5B9E_u73B0failover_u80FD" class="headerlink" title="实现failover能"></a>实现failover能</h4><p>  Failover Sink Processor能够实现failover功能，具体流程类似load balance，<br>  但是内部处理机制与load balance完全不同：Failover Sink Processor维护一个优先级Sink组件列表，只要有一个Sink组件可用，<br>  Event就被传递到下一个组件。如果一个Sink能够成功处理Event，则会加入到一个Pool中，否则会被移出Pool并计算失败次数，设置一个惩罚因子</p>
  <figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a1<span class="class">.sinkgroups</span> = g1</span><br><span class="line">a1<span class="class">.sinkgroups</span><span class="class">.g1</span><span class="class">.sinks</span> = k1 k2 k3</span><br><span class="line">a1<span class="class">.sinkgroups</span><span class="class">.g1</span><span class="class">.processor</span><span class="class">.type</span> = failover</span><br><span class="line">a1<span class="class">.sinkgroups</span><span class="class">.g1</span><span class="class">.processor</span><span class="class">.priority</span><span class="class">.k1</span> = <span class="number">5</span></span><br><span class="line">a1<span class="class">.sinkgroups</span><span class="class">.g1</span><span class="class">.processor</span><span class="class">.priority</span><span class="class">.k2</span> = <span class="number">7</span></span><br><span class="line">a1<span class="class">.sinkgroups</span><span class="class">.g1</span><span class="class">.processor</span><span class="class">.priority</span><span class="class">.k3</span> = <span class="number">6</span></span><br><span class="line">a1<span class="class">.sinkgroups</span><span class="class">.g1</span><span class="class">.processor</span><span class="class">.maxpenalty</span> = <span class="number">20000</span></span><br></pre></td></tr></table></figure>
<h3 id="u5B89_u88C5_u914D_u7F6E"><a href="#u5B89_u88C5_u914D_u7F6E" class="headerlink" title="安装配置"></a>安装配置</h3><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下载二进制包</span></span><br><span class="line">[mofun_mining<span class="variable">@i</span>-tev02vc1 ~]<span class="variable">$ </span>wget <span class="string">"http://www.gtlib.gatech.edu/pub/apache/flume/1.6.0/apache-flume-1.6.0-bin.tar.gz"</span></span><br><span class="line">[mofun_mining<span class="variable">@i</span>-tev02vc1 ~]<span class="variable">$ </span>tar xvzf apache-flume-<span class="number">1.6</span>.<span class="number">0</span>-bin.tar.gz</span><br><span class="line">[mofun_mining<span class="variable">@i</span>-tev02vc1 ~]<span class="variable">$ </span>mv apache-flume-<span class="number">1.6</span>.<span class="number">0</span>-bin /usr/local/</span><br><span class="line"><span class="comment"># 修改配置文件</span></span><br><span class="line">[mofun_mining<span class="variable">@i</span>-qe32ajmq conf]<span class="variable">$ </span>pwd</span><br><span class="line">/usr/local/apache-flume-<span class="number">1.6</span>.<span class="number">0</span>-bin/conf</span><br><span class="line">[mofun_mining<span class="variable">@i</span>-qe32ajmq conf]<span class="variable">$ </span>sudo cp flume-conf.properties.template flume-conf.properties</span><br></pre></td></tr></table></figure>
<p>采用 Avro Source+Memory Channel+HDFS Sink 方式</p>
<ul>
<li><p>服务器（日志汇总服务器agent）端配置文件</p>
<figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">[mofun_mining@i-tev02vc1 ~]$ <span class="keyword">cd</span> /usr/<span class="keyword">local</span>/apache-flume-1.6.0-bin/<span class="keyword">conf</span>/</span><br><span class="line">[mofun_mining@i-tev02vc1 <span class="keyword">conf</span>]$ <span class="keyword">ls</span></span><br><span class="line">flume-<span class="keyword">conf</span>.properties  flume-<span class="keyword">conf</span>.properties.template  flume-env.ps1.template  flume-env.<span class="keyword">sh</span>  flume-env.<span class="keyword">sh</span>.template  log4j.properties</span><br><span class="line">[mofun_mining@i-tev02vc1 <span class="keyword">conf</span>]$ <span class="keyword">pwd</span></span><br><span class="line">/usr/<span class="keyword">local</span>/apache-flume-1.6.0-bin/<span class="keyword">conf</span></span><br><span class="line">[mofun_mining@i-tev02vc1 <span class="keyword">conf</span>]$ sudo vim flume-<span class="keyword">conf</span>.properties</span><br><span class="line"># example.<span class="keyword">conf</span>: A single-node Flume configuration</span><br><span class="line"></span><br><span class="line"># Name the components <span class="keyword">on</span> this agent</span><br><span class="line">agent1.sources = r1</span><br><span class="line">agent1.sinks = k1</span><br><span class="line">agent1.channels = c1</span><br><span class="line"></span><br><span class="line"># <span class="keyword">Describe</span>/configure the source</span><br><span class="line">agent1.sources.r1.<span class="keyword">type</span> = avro</span><br><span class="line">agent1.sources.r1.bind = 192.168.1.33</span><br><span class="line">agent1.sources.r1.port = 41414</span><br><span class="line">agent1.sources.r1.channels = c1</span><br><span class="line"></span><br><span class="line"># <span class="keyword">Describe</span> the sink</span><br><span class="line">agent1.sinks.k1.<span class="keyword">type</span> = hdfs</span><br><span class="line">agent1.sinks.k1.channel = c1</span><br><span class="line">agent1.sinks.k1.hdfs.fileType = DataStream</span><br><span class="line">agent1.sinks.k1.hdfs.useLocalTimeStamp = true</span><br><span class="line">agent1.sinks.k1.hdfs.path = /flume/events/%Y-%<span class="keyword">m</span>-%<span class="literal">d</span></span><br><span class="line">#agent1.sinks.k1.hdfs.round = true</span><br><span class="line">#agent1.sinks.k1.hdfs.roundValue = 10</span><br><span class="line">#agent1.sinks.k1.hdfs.roundUnit = minute</span><br><span class="line">agent1.sinks.k1.hdfs.rollCount = 5000</span><br><span class="line">agent1.sinks.k1.hdfs.rollSize = 0</span><br><span class="line">agent1.sinks.k1.hdfs.rollInterval= 0</span><br><span class="line"></span><br><span class="line"># <span class="keyword">Use</span> a channel <span class="keyword">which</span> buffers events <span class="keyword">in</span> <span class="keyword">memory</span></span><br><span class="line">agent1.channels.c1.<span class="keyword">type</span> = <span class="keyword">memory</span></span><br><span class="line">agent1.channels.c1.capacity = 10000</span><br><span class="line">agent1.channels.c1.transactionCapacity = 1000</span><br></pre></td></tr></table></figure>
</li>
<li><p>客户端（日志收集agent）</p>
<figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">[reason@i-qunray9x <span class="keyword">conf</span>]$ <span class="keyword">cd</span> /usr/<span class="keyword">local</span>/apache-flume-1.6.0-bin/<span class="keyword">conf</span>/</span><br><span class="line">[reason@i-qunray9x <span class="keyword">conf</span>]$ <span class="keyword">pwd</span></span><br><span class="line">/usr/<span class="keyword">local</span>/apache-flume-1.6.0-bin/<span class="keyword">conf</span></span><br><span class="line">[reason@i-qunray9x <span class="keyword">conf</span>]$ sudo vim flume-<span class="keyword">conf</span>.properties</span><br><span class="line"></span><br><span class="line"># example.<span class="keyword">conf</span>: A single-node Flume configuration</span><br><span class="line"></span><br><span class="line"># Name the components <span class="keyword">on</span> this agent</span><br><span class="line">agent1.sources = r1</span><br><span class="line">agent1.sinks = k1</span><br><span class="line">agent1.channels = c1</span><br><span class="line"></span><br><span class="line"># <span class="keyword">Describe</span>/configure the source</span><br><span class="line">agent1.sources.r1.<span class="keyword">type</span> = exec</span><br><span class="line">agent1.sources.r1.command = tail -<span class="keyword">n</span> 0 -F /home/reason/1.txt</span><br><span class="line">agent1.sources.r1.channels = c1</span><br><span class="line"></span><br><span class="line"># <span class="keyword">Describe</span> the sink</span><br><span class="line">agent1.sinks.k1.<span class="keyword">type</span> = avro</span><br><span class="line">agent1.sinks.k1.channel = c1</span><br><span class="line">agent1.sinks.k1.hdfs.useLocalTimeStamp = true</span><br><span class="line">agent1.sinks.k1.hdfs.path = /flume/events/%Y-%<span class="keyword">m</span>-%<span class="literal">d</span></span><br><span class="line">agent1.sinks.k1.hostname=192.168.1.33</span><br><span class="line">agent1.sinks.k1.port = 41414</span><br><span class="line"></span><br><span class="line"># <span class="keyword">Use</span> a channel <span class="keyword">which</span> buffers events <span class="keyword">in</span> <span class="keyword">memory</span></span><br><span class="line">agent1.channels.c1.<span class="keyword">type</span> = <span class="keyword">memory</span></span><br><span class="line">agent1.channels.c1.capacity = 5000</span><br><span class="line">agent1.channels.c1.transactionCapacity = 500</span><br></pre></td></tr></table></figure>
</li>
<li><p>启动服务器</p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[mofun_mining<span class="annotation">@i</span>-tev02vc1 conf]$</span><br><span class="line"><span class="regexp">/usr/</span>local<span class="regexp">/apache-flume-1.6.0-bin/</span>bin<span class="regexp">/flume-ng agent -c ./</span>conf<span class="regexp">/ -f /</span>usr<span class="regexp">/local/</span>apache-flume-<span class="number">1.6</span><span class="number">.0</span>-bin<span class="regexp">/conf/</span>flume-conf.properties -n agent1 -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>
</li>
<li><p>启动客户端</p>
<figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[reason@i-qunray9x <span class="keyword">conf</span>]$</span><br><span class="line">/usr/<span class="keyword">local</span>/apache-flume-1.6.0-bin/bin/flume-ng agent -c <span class="keyword">conf</span> -f /usr/<span class="keyword">local</span>/apache-flume-1.6.0-bin/<span class="keyword">conf</span>/flume-<span class="keyword">conf</span>.properties -<span class="keyword">n</span> agent1</span><br></pre></td></tr></table></figure>
</li>
<li><p>测试</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[mofun_mining@i-r6cuv8iq ~]$ hdfs dfs -ls /flume/events/<span class="number">2015</span>-<span class="number">12</span>-<span class="number">15</span></span><br><span class="line">Found <span class="number">40</span> items</span><br><span class="line">-rw-r--r--   <span class="number">2</span> mofun_mining supergroup      <span class="number">34844</span> <span class="number">2015</span>-<span class="number">12</span>-<span class="number">15</span> <span class="number">17</span>:<span class="number">39</span> /flume/events/<span class="number">2015</span>-<span class="number">12</span>-<span class="number">15</span>/FlumeData<span class="number">.1450172340281</span></span><br><span class="line">-rw-r--r--   <span class="number">2</span> mofun_mining supergroup      <span class="number">34850</span> <span class="number">2015</span>-<span class="number">12</span>-<span class="number">15</span> <span class="number">17</span>:<span class="number">39</span> /flume/events/<span class="number">2015</span>-<span class="number">12</span>-<span class="number">15</span>/FlumeData<span class="number">.1450172340282</span></span><br><span class="line">-rw-r--r--   <span class="number">2</span> mofun_mining supergroup      <span class="number">34850</span> <span class="number">2015</span>-<span class="number">12</span>-<span class="number">15</span> <span class="number">17</span>:<span class="number">39</span> /flume/events/<span class="number">2015</span>-<span class="number">12</span>-<span class="number">15</span>/FlumeData<span class="number">.1450172340283</span></span><br><span class="line">-rw-r--r--   <span class="number">2</span> mofun_mining supergroup      <span class="number">34850</span> <span class="number">2015</span>-<span class="number">12</span>-<span class="number">15</span> <span class="number">17</span>:<span class="number">39</span> /flume/events/<span class="number">2015</span>-<span class="number">12</span>-<span class="number">15</span>/FlumeData<span class="number">.1450172340284</span></span><br><span class="line">-rw-r--r--   <span class="number">2</span> mofun_mining supergroup      <span class="number">34850</span> <span class="number">2015</span>-<span class="number">12</span>-<span class="number">15</span> <span class="number">17</span>:<span class="number">39</span> /flume/events/<span class="number">2015</span>-<span class="number">12</span>-<span class="number">15</span>/FlumeData<span class="number">.1450172340285</span></span><br><span class="line">-rw-r--r--   <span class="number">2</span> mofun_mining supergroup      <span class="number">34850</span> <span class="number">2015</span>-<span class="number">12</span>-<span class="number">15</span> <span class="number">17</span>:<span class="number">39</span> /flume/events/<span class="number">2015</span>-<span class="number">12</span>-<span class="number">15</span>/FlumeData<span class="number">.1450172340286</span></span><br><span class="line">-rw-r--r--   <span class="number">2</span> mofun_mining supergroup      <span class="number">34850</span> <span class="number">2015</span>-<span class="number">12</span>-<span class="number">15</span> <span class="number">17</span>:<span class="number">39</span> /flume/events/<span class="number">2015</span>-<span class="number">12</span>-<span class="number">15</span>/FlumeData<span class="number">.1450172340287</span></span><br><span class="line">-rw-r--r--   <span class="number">2</span> mofun_mining supergroup      <span class="number">34850</span> <span class="number">2015</span>-<span class="number">12</span>-<span class="number">15</span> <span class="number">17</span>:<span class="number">39</span> /flume/events/<span class="number">2015</span>-<span class="number">12</span>-<span class="number">15</span>/FlumeData<span class="number">.1450172340288</span></span><br><span class="line">-rw-r--r--   <span class="number">2</span> mofun_mining supergroup      <span class="number">34850</span> <span class="number">2015</span>-<span class="number">12</span>-<span class="number">15</span> <span class="number">17</span>:<span class="number">39</span> /flume/events/<span class="number">2015</span>-<span class="number">12</span>-<span class="number">15</span>/FlumeData<span class="number">.1450172340289</span></span><br><span class="line">-rw-r--r--   <span class="number">2</span> mofun_mining supergroup      <span class="number">34850</span> <span class="number">2015</span>-<span class="number">12</span>-<span class="number">15</span> <span class="number">17</span>:<span class="number">39</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>此时，通过nginx实时产生的日志，即可实时插入到hdfs中了。</p>
</li>
</ul>
<h3 id="u53C2_u8003_u6587_u732E"><a href="#u53C2_u8003_u6587_u732E" class="headerlink" title="参考文献"></a>参考文献</h3><ul>
<li><a href="http://shiyanjun.cn/archives/915.html" target="_blank" rel="external">http://shiyanjun.cn/archives/915.html</a></li>
<li><a href="http://my.oschina.net/leejun2005/blog/288136" target="_blank" rel="external">http://my.oschina.net/leejun2005/blog/288136</a></li>
<li><a href="http://tech.meituan.com/mt-log-system-optimization.html" target="_blank" rel="external">http://tech.meituan.com/mt-log-system-optimization.html</a></li>
<li><a href="http://www.ixirong.com/2015/05/18/how-to-install-flume-ng/" target="_blank" rel="external">http://www.ixirong.com/2015/05/18/how-to-install-flume-ng/</a></li>
<li><a href="https://flume.apache.org/FlumeUserGuide.html#setting-up-an-agent" target="_blank" rel="external">https://flume.apache.org/FlumeUserGuide.html#setting-up-an-agent</a></li>
<li><a href="http://m.blog.csdn.net/blog/xueliang1029/24039459" target="_blank" rel="external">http://m.blog.csdn.net/blog/xueliang1029/24039459</a></li>
</ul>
]]></content>
    <summary type="html">
    <![CDATA[<!--  -->
<!--  -->
<!--  _ __    __     __      ____    ___     ___   _____   __  __    ___     -->
<!-- /\`'__\/'__`\ /'__`\   /',__\  / _]]>
    </summary>
    
      <category term="Apache" scheme="http://reasonpun.com/tags/Apache/"/>
    
      <category term="Flume-NG" scheme="http://reasonpun.com/tags/Flume-NG/"/>
    
      <category term="HDFS" scheme="http://reasonpun.com/tags/HDFS/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[apache_kafka_structure]]></title>
    <link href="http://reasonpun.com/2015/12/08/apache-kafka-structure/"/>
    <id>http://reasonpun.com/2015/12/08/apache-kafka-structure/</id>
    <published>2015-12-08T08:30:08.000Z</published>
    <updated>2015-12-25T03:29:55.000Z</updated>
    <content type="html"><![CDATA[<!--  -->
<!--  -->
<!--  _ __    __     __      ____    ___     ___   _____   __  __    ___     -->
<!-- /\`'__\/'__`\ /'__`\   /',__\  / __`\ /' _ `\/\ '__`\/\ \/\ \ /' _ `\   -->
<!-- \ \ \//\  __//\ \L\.\_/\__, `\/\ \L\ \/\ \/\ \ \ \L\ \ \ \_\ \/\ \/\ \  -->
<!--  \ \_\\ \____\ \__/.\_\/\____/\ \____/\ \_\ \_\ \ ,__/\ \____/\ \_\ \_\ -->
<!--   \/_/ \/____/\/__/\/_/\/___/  \/___/  \/_/\/_/\ \ \/  \/___/  \/_/\/_/ -->
<!--                                                 \ \_\                   -->
<!--                                                  \/_/                   -->
<!--  -->
<h4 id="Apache_Kafka_u6D88_u606F_u670D_u52A1"><a href="#Apache_Kafka_u6D88_u606F_u670D_u52A1" class="headerlink" title="Apache Kafka消息服务"></a>Apache Kafka消息服务</h4><ul>
<li><p>参考地址 <a href="http://kafka.apache.org/documentation.html#brokerconfigs" target="_blank" rel="external">http://kafka.apache.org/documentation.html</a></p>
</li>
<li><p>消息队列的分类</p>
<ul>
<li><p>点对点</p>
<p>生产者生产消息发送到Queue中，消费者消费Queue中的消息，其中：</p>
<ul>
<li>Queue中不再存储已经被消费的消息</li>
<li>Queue支持多个消费者，但是同一个消息，只能被一个消费者消费</li>
</ul>
</li>
<li><p>发布/订阅</p>
<p>生产者（生产）将消息发布到topic中，同时多个消费者（消费）订阅该消息。和点对点方式不同的是，发布到topic的消息会被所有订阅者消费</p>
</li>
</ul>
</li>
<li><p>简介</p>
<p>背景 Kafka使用Scala语言编写，是一个分布式，分区的，支持多副本，多订阅者的日志系统。</p>
<p>目前支持Java，Python，C++， PHP等</p>
<ul>
<li>总体结构</li>
</ul>
<img src="/images/kafka.0.9.0/structure.png" title="kafka总体结构图">
</li>
</ul>
<ul>
<li><p>名词解释</p>
<ul>
<li><p>Producer</p>
<p>  消息生产者，就是向kafka broker发消息的客户端</p>
</li>
<li><p>Consumer</p>
<p>  消息消费者，向kafka broker取消息的客户端</p>
</li>
<li><p>Topic</p>
<p>  是一个消息队列？</p>
</li>
<li><p>Consumer Group （CG）</p>
<ul>
<li>这是Kafka用来实现一个Topic消息的广播（发给所有的Consumer）和单播（发给任意一个Consumer）的手段</li>
<li>一个Topic可以有多个CG</li>
<li>Topic的消息会复制（不是真的复制，是概念上的）到所有的CG，但每个CG只会把消息发给该CG中的一个consumer</li>
<li>如果需要实现广播，只要每个Consumer有一个独立的CG就可以了</li>
<li>要实现单播只要所有的Consumer在同一个CG</li>
<li><p>用CG还可以将Consumer进行自由的分组而不需要多次发送消息到不同的topic</p>
</li>
<li><p>Broker</p>
<ul>
<li>一台Kafka服务器就是一个Broker</li>
<li>一个集群由多个Broker组成。一个Broker可以容纳多个Topic</li>
</ul>
</li>
<li><p>Partition</p>
<p>为了实现扩展性，一个非常大的Topic可以分布到多个Broker（即服务器）上，一个Topic可以分为多个Partition，每个Partition是一个有序的队列。Prtition中的每条消息都会被分配一个有序的id（Offset）。Kafka只保证按一个Partition中的顺序将消息发给Consumer，不保证一个Topic的整体（多个Partition间）的顺序。</p>
</li>
<li><p>Offset</p>
<ul>
<li>kafka的存储文件都是按照offset.kafka来命名，用offset做名字的好处是方便查找</li>
<li>例如你想找位于2049的位置，只要找到2048.kafka的文件即可，当然the first offset就是00000000000.kafka</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>特性</p>
<ul>
<li>通过O(1)的磁盘数据结构提供消息的持久化，这种结构对于即使数以TB的消息存储也能够保持长时间的稳定性能</li>
<li>高吞吐量：即使是非常普通的硬件Kafka也可以支持每秒数十万的消息</li>
<li>支持 <em>同步</em> 和 <em>异步</em> 复制两种HA</li>
<li>Consumer客户端<ul>
<li>pull</li>
<li>随机读</li>
<li>利用sendfile系统调用</li>
<li>zero-copy</li>
<li>批量拉数据</li>
</ul>
</li>
<li>消费状态保存在客户端</li>
<li>消息存储顺序写</li>
<li>数据迁移、扩容对用户透明</li>
<li>支持Hadoop并行数据加载</li>
<li>支持online和offline的场景</li>
<li>持久化：通过将数据持久化到硬盘以及replication防止数据丢失</li>
<li>scale out：无需停机即可扩展机器</li>
<li>定期删除机制，支持设定partitions的segment file保留时间</li>
</ul>
</li>
<li><p>可靠性（一致性)</p>
<p>传统的MQ系统通常都是通过broker和consumer间的确认（ack）机制实现的，并在broker保存消息分发的状态，即使这样一致性也是很难保证的。</p>
<p>Kafka的做法是由consumer自己保存状态，也不要任何确认。这样虽然consumer负担更重，但其实更灵活了。因为不管consumer上任何原因导致需要重新处理消息，都可以再次从broker获得。</p>
</li>
<li><p>可扩展性</p>
<p>Kafka 使用Zookeeper实现动态的集群扩展，不需要更改客户端（生产者和消费者）的配置。broker会在ZK注册并保持相关的元数据更新。而客户端会在ZK上注册相关的watcher，一旦ZK发生变化，客户端能及时做出相应调整。这样可以保证变更broker时，各个broker之间能自动实现负载均衡。</p>
</li>
<li><p>设计目标</p>
<p>高吞吐量</p>
<ul>
<li>数据磁盘持久化：消息不在内存中cache，直接写入到磁盘，充分利用磁盘的顺序读写性能</li>
<li>zero-copy：减少IO操作步骤</li>
<li>支持数据批量发送和拉取</li>
<li>支持数据压缩</li>
<li>Topic划分为多个partition，提高并行处理能力</li>
</ul>
</li>
<li><p>Producer负载均衡和HA机制</p>
<ul>
<li>producer根据用户指定的算法，将消息发送到指定的partition。</li>
<li>存在多个partiiton，每个partition有自己的replica，每个replica分布在不同的Broker节点上。</li>
<li>多个partition需要选取出lead partition，lead partition负责读写，并由zookeeper负责fail over。</li>
<li>通过zookeeper管理broker与consumer的动态加入与离开。</li>
</ul>
</li>
<li><p>Consumer的pull机制</p>
<p>由于broker会持久化数据，broker没有cache压力，因此，consumer比较适合才去pull的方式消费数据：</p>
<ul>
<li>简化kafka设计，降低了难度</li>
<li>Consumer根据消费能力自主控制消息拉取速度</li>
<li>Consumer根据自身情况自主选择消费模式，例如批量，重复消费，从制定partition或位置(offset)开始消费等</li>
</ul>
</li>
<li><p>Consumer与Topic关系以及机制</p>
<p>每个group包含多个consumer。对于topic中的一条特定消息，只会被订阅此Topic每个group中的一个consumer消费，那么一个group中的所有consumer将会交错的消费整个Topic。</p>
<p>如果所有的consumer都具有相同的group（类似JMS queue），消息将有所有的consumer负载均衡</p>
<p>如果所有的consumer都具有不同的group，那么这就是『发布-订阅』，消息将会广播给所有消费者</p>
<p>在Kafka中，一个partition中的消息只会被group中的一个consumer消费（同一时刻）；每个group中consumer消息消费互相独立；<br>一个group是一个『订阅』者，一个Topic中的每个partition只会被一个『订阅』者中的一个consumer消费，但是一个consumer可以同事消费多个partitions中的消息。</p>
<p>Kafka只能保证一个partition中的消息被某个consumer消费是顺序的，但是从Topic角度，当有多个partitions时，消息仍不是全局有序的</p>
<p>一个group中包含多个consumer，这样的话不仅能提高topic中消息的并发消费能力，还能提高『故障容错』性，如果group中的某个consumer失效，那么其消费的partition将会被其他consumer接管</p>
<p>Kafka的设计原理决定，对于一个Topic，同一个group中不能有多于partition个数的consumer同时消费，否则将意味着某些consumer将无法得到消息</p>
</li>
<li><p>Producer均衡算法</p>
<p>Kafka集群中的任何一个broker，都可以向producer提供metadata，这些metadata中包含『集群中存货的servers/partition leaders』，当producer获取到metadata后，会和topic下所有的partition leader保持socker连接；消息由producer直接通过socker发送到broker</p>
<blockquote>
<p>中间不会经过任何『路由层』，即，消息被路由到哪个partition上，是有producer决定的<br>在producer端的配置文件中，可以指定partition的路由方式：『random』，『key-hash』等</p>
</blockquote>
</li>
<li><p>Consumer均衡算法</p>
<p>当一个group中，有consumer加入或者离开时，会触发partitions均衡。均衡的最终目的，是提升topic的并发消费能力。</p>
<ul>
<li>假如topic1,具有如下partitions: P0,P1,P2,P3</li>
<li>加入group中,有如下consumer: C0,C1</li>
<li>首先根据partition索引号对partitions排序: P0,P1,P2,P3</li>
<li>根据consumer.id排序: C0,C1</li>
<li>计算倍数: M = [P0,P1,P2,P3].size / [C0,C1].size,本例值M=2(向上取整)</li>
<li>然后依次分配partitions: C0 = [P0,P1],C1=[P2,P3],即Ci = [P(i <em> M),P((i + 1) </em> M -1)]</li>
</ul>
</li>
<li><p>Broker集群内broker之间replica机制</p>
<p>replication策略是基于partiton，而不是topic</p>
<blockquote>
<p>kafka将每个partition复制到多个server上<br>任何一个partition有一个leader和任意数量的follower<br>备份的数量可以由broker配置文件设定<br>leader处理所有的read-write请求，负责跟踪所有的follower状态，<br>如果follower『落后』太多或者失效，leader会把它从replicas同步列表中删除<br>follower需要和leader保持同步，follower就像一个consumer，消费信息并保存在本地日志中<br>当所有的follower都将一个消息保存成功，此消息才能被认为是『committed』，<br>此时consumer才能消费它，这种策略要求leader和follower之间保持良好的网络环境<br>只要ZK集群存活，即使只存活一个replica，仍可以保证消息的正常发送和接收</p>
</blockquote>
<ul>
<li>Kafka判定一个follower存活的条件<ul>
<li>和ZK保持良好的链接</li>
<li>及时跟进leader，不能落后太多</li>
</ul>
</li>
</ul>
<blockquote>
<p>如果此replicas落后太多，它会继续在leader中fetch数据，然后加入同步列表中，<br>Kafka不会更换宿主，只有这样才能保证replicas足够快，才能保证producer发布消息时接收ACK的延迟较小</p>
</blockquote>
<ul>
<li>当leader失效，需要考虑负载均衡，partition leader较少的broker更有可能成为新的leader，因为<ul>
<li>不能采用『投票多数派』的算法，因为这种算法对于『网络稳定性/投票参与者数量』要求较高</li>
<li>Kafka集群设计中，容忍N-1个replicas失效</li>
<li>每个partiton中所有的replica信息都可以在ZK中获得，那么选择leader是非常简单的</li>
<li>选择follower时需要注意：避免新的leader server上承载的partiton leader的个数过多，否则此server将承受更多的IO压力</li>
</ul>
</li>
</ul>
</li>
<li><p>总结</p>
<ul>
<li>Producer端直接连接broker列表，从列表中返回TopicMetadataResponse，该Metadata包含Topic下每个partition leader建立socket连接并发送消息。</li>
<li>Broker端使用ZK用来注册broker信息，以及监控partition leader存活性。</li>
<li>Consumer端使用ZK用来注册consumer信息，其中包括consumer消费的partition列表等，同时也用来发现broker列表，并和partition leader建立socket连接，并获取消息。</li>
</ul>
</li>
</ul>
<h4 id="Kafka_u5728Zookeeper_u4E2D_u5B58_u50A8_u7ED3_u6784"><a href="#Kafka_u5728Zookeeper_u4E2D_u5B58_u50A8_u7ED3_u6784" class="headerlink" title="Kafka在Zookeeper中存储结构"></a>Kafka在Zookeeper中存储结构</h4><ul>
<li><p>结构图</p>
<img src="/images/kafka.0.9.0/kafka_in_zk.png" title="kafka在ZK中的存储结构图">
</li>
</ul>
<h4 id="Kafka__u5B89_u88C5_u548C_u914D_u7F6E"><a href="#Kafka__u5B89_u88C5_u548C_u914D_u7F6E" class="headerlink" title="Kafka 安装和配置"></a>Kafka 安装和配置</h4><h4 id="u53C2_u8003_u6587_u732E"><a href="#u53C2_u8003_u6587_u732E" class="headerlink" title="参考文献"></a>参考文献</h4><ul>
<li><a href="http://blog.csdn.net/zhongwen7710/article/details/41252649" target="_blank" rel="external">http://blog.csdn.net/zhongwen7710/article/details/41252649</a></li>
<li><a href="http://kafka.apache.org/documentation.html#brokerconfigs" target="_blank" rel="external">http://kafka.apache.org/documentation.html#brokerconfigs</a></li>
</ul>
]]></content>
    <summary type="html">
    <![CDATA[<!--  -->
<!--  -->
<!--  _ __    __     __      ____    ___     ___   _____   __  __    ___     -->
<!-- /\`'__\/'__`\ /'__`\   /',__\  / _]]>
    </summary>
    
      <category term="Apache" scheme="http://reasonpun.com/tags/Apache/"/>
    
      <category term="HDFS" scheme="http://reasonpun.com/tags/HDFS/"/>
    
      <category term="Kafka" scheme="http://reasonpun.com/tags/Kafka/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[kafka文档简述]]></title>
    <link href="http://reasonpun.com/2015/12/05/kafa-documentation/"/>
    <id>http://reasonpun.com/2015/12/05/kafa-documentation/</id>
    <published>2015-12-05T08:47:08.000Z</published>
    <updated>2015-12-25T03:28:28.000Z</updated>
    <content type="html"><![CDATA[<!--  -->
<!--  -->
<!--  _ __    __     __      ____    ___     ___   _____   __  __    ___     -->
<!-- /\`'__\/'__`\ /'__`\   /',__\  / __`\ /' _ `\/\ '__`\/\ \/\ \ /' _ `\   -->
<!-- \ \ \//\  __//\ \L\.\_/\__, `\/\ \L\ \/\ \/\ \ \ \L\ \ \ \_\ \/\ \/\ \  -->
<!--  \ \_\\ \____\ \__/.\_\/\____/\ \____/\ \_\ \_\ \ ,__/\ \____/\ \_\ \_\ -->
<!--   \/_/ \/____/\/__/\/_/\/___/  \/___/  \/_/\/_/\ \ \/  \/___/  \/_/\/_/ -->
<!--                                                 \ \_\                   -->
<!--                                                  \/_/                   -->
<!--  -->
<h3 id="u7FFB_u8BD1ing"><a href="#u7FFB_u8BD1ing" class="headerlink" title="翻译ing"></a>翻译ing</h3><h2 id="u672F_u8BED_u8868"><a href="#u672F_u8BED_u8868" class="headerlink" title="术语表"></a>术语表</h2><table>
<thead>
<tr>
<th>名词术语</th>
<th>解释</th>
</tr>
</thead>
<tbody>
<tr>
<td>topic</td>
<td>主题</td>
</tr>
<tr>
<td>producer</td>
<td>生产者</td>
</tr>
<tr>
<td>comsumer</td>
<td>消费者</td>
</tr>
<tr>
<td>broker</td>
<td>代理</td>
</tr>
<tr>
<td>partition</td>
<td>分区</td>
</tr>
<tr>
<td>offset</td>
<td>下标</td>
</tr>
<tr>
<td>round-robin</td>
<td>轮询</td>
</tr>
<tr>
<td>key</td>
<td>键</td>
</tr>
<tr>
<td>queuing</td>
<td>排队</td>
</tr>
<tr>
<td>publish-subscribe</td>
<td>发布-订阅</td>
</tr>
<tr>
<td>the consumer group</td>
<td>消费者组</td>
</tr>
<tr>
<td>subscriber</td>
<td>订阅者</td>
</tr>
<tr>
<td>guarantee</td>
<td>保证（?）</td>
</tr>
</tbody>
</table>
<h2 id="Kafka_0-9-0__u6587_u6863"><a href="#Kafka_0-9-0__u6587_u6863" class="headerlink" title="Kafka 0.9.0 文档"></a>Kafka 0.9.0 文档</h2><ol>
<li>开始<ul>
<li>1.1 简介</li>
<li>1.2 用例</li>
<li>1.3 快速入门</li>
<li>1.4 生态圈</li>
<li>1.5 升级</li>
</ul>
</li>
<li>API<ul>
<li>2.1 生产者 API</li>
<li>2.2 消费者 API<ul>
<li>2.2.1 Old High Level Consumer API</li>
<li>2.2.2 Old Simple Consumer API</li>
<li>2.2.3 New Consumer API</li>
</ul>
</li>
</ul>
</li>
<li>配置<ul>
<li>3.1 代理配置</li>
<li>3.2 生产者配置</li>
<li>3.3 消费者配置<ul>
<li>3.3.1 Old Consumer Configs</li>
<li>3.3.2 New Consumer Configs</li>
<li>3.4 Kafka 连接配置</li>
</ul>
</li>
</ul>
</li>
<li>设计<ul>
<li>4.1 Motivation</li>
<li>4.2 持久化</li>
<li>4.3 Efficiency</li>
<li>4.4 生产者</li>
<li>4.5 消费者</li>
<li>4.6 Message Delivery Semantics</li>
<li>4.7 Replication</li>
<li>4.8 Log Compaction</li>
<li>4.9 Quotas</li>
</ul>
</li>
<li>实现<ul>
<li>5.1 API 设计</li>
<li>5.2 网络层</li>
<li>5.3 消息</li>
<li>5.4 消息格式</li>
<li>5.5 日志</li>
<li>5.6 分布式</li>
</ul>
</li>
<li>Operations<ul>
<li>6.1 Basic Kafka Operations<ul>
<li>Adding and removing topics</li>
<li>Modifying topics</li>
<li>Graceful shutdown</li>
<li>Balancing leadership</li>
<li>Checking consumer position</li>
<li>Mirroring data between clusters</li>
<li>Expanding your cluster</li>
<li>Decommissioning brokers</li>
<li>Increasing replication factor</li>
</ul>
</li>
<li>6.2 Datacenters</li>
<li>6.3 Important Configs<ul>
<li>Important Server Configs</li>
<li>Important Client Configs</li>
<li>A Production Server Configs</li>
</ul>
</li>
<li>6.4 Java Version</li>
<li>6.5 Hardware and OS<ul>
<li>OS</li>
<li>Disks and Filesystems</li>
<li>Application vs OS Flush Management</li>
<li>Linux Flush Behavior</li>
<li>Ext4 Notes</li>
</ul>
</li>
<li>6.6 Monitoring</li>
<li>6.7 ZooKeeper<ul>
<li>Stable Version</li>
<li>Operationalization</li>
</ul>
</li>
</ul>
</li>
<li>安全性<ul>
<li>7.1 Security Overview</li>
<li>7.2 Encryption and Authentication using SSL</li>
<li>7.3 Authentication using SASL</li>
<li>7.4 Authorization and ACLs</li>
<li>7.5 ZooKeeper Authentication<ul>
<li>New Clusters</li>
<li>Migrating Clusters</li>
<li>Migrating the ZooKeeper Ensemble</li>
</ul>
</li>
</ul>
</li>
<li>Kafka Connect<ul>
<li>8.1 Overview</li>
<li>8.2 User Guide</li>
<li>8.3 Connector Development Guide</li>
</ul>
</li>
</ol>
<h2 id="1-__u5F00_u59CB"><a href="#1-__u5F00_u59CB" class="headerlink" title="1. 开始"></a>1. 开始</h2><h3 id="1-1__u7B80_u4ECB"><a href="#1-1__u7B80_u4ECB" class="headerlink" title="1.1 简介"></a>1.1 简介</h3><p>kafka是一个分布式的，分区的，复用的日志提交服务。它以一种独特的设计方式提供消息传递系统的功能。</p>
<p>这是什么意思呢？</p>
<p>First let’s review some basic messaging terminology:</p>
<ul>
<li>Kafka maintains feeds of messages in categories called topics.</li>
<li>We’ll call processes that publish messages to a Kafka topic producers.</li>
<li>We’ll call processes that subscribe to topics and process the feed of published messages consumers..</li>
<li>Kafka is run as a cluster comprised of one or more servers each of which is called a broker.</li>
</ul>
<p>So, at a high level, producers send messages over the network to the Kafka cluster which in turn serves them up to consumers like this:</p>
<img src="/images/kafka.0.9.0/producer_consumer.png" width="258" height="180" title="生产者消费者关系">
<p>Communication between the clients and the servers is done with a simple, high-performance, language agnostic TCP protocol. We provide a Java client for Kafka, but clients are available in many languages.</p>
<p>服务器端和客户端的通讯是通过一个简单的，高效的，TCP协议无关的语言实现的。不仅提供了Java客户端，还提供了其他很多语言的支持。</p>
<h4 id="Topics_and_Logs"><a href="#Topics_and_Logs" class="headerlink" title="Topics and Logs"></a>Topics and Logs</h4><p>Let’s first dive into the high-level abstraction Kafka provides—the topic.<br>A topic is a category or feed name to which messages are published. For each topic, the Kafka cluster maintains a partitioned log that looks like this:</p>
<img src="/images/kafka.0.9.0/producer_consumer.png" width="258" height="180" title="生产者消费者关系">
<p>Each partition is an ordered, immutable sequence of messages that is continually appended to—a commit log. The messages in the partitions are each assigned a sequential id number called the offset that uniquely identifies each message within the partition.<br>The Kafka cluster retains all published messages—whether or not they have been consumed—for a configurable period of time. For example if the log retention is set to two days, then for the two days after a message is published it is available for consumption, after which it will be discarded to free up space. Kafka’s performance is effectively constant with respect to data size so retaining lots of data is not a problem.</p>
<p>In fact the only metadata retained on a per-consumer basis is the position of the consumer in the log, called the “offset”. This offset is controlled by the consumer: normally a consumer will advance its offset linearly as it reads messages, but in fact the position is controlled by the consumer and it can consume messages in any order it likes. For example a consumer can reset to an older offset to reprocess.</p>
<p>This combination of features means that Kafka consumers are very cheap—they can come and go without much impact on the cluster or on other consumers. For example, you can use our command line tools to “tail” the contents of any topic without changing what is consumed by any existing consumers.</p>
<p>The partitions in the log serve several purposes. First, they allow the log to scale beyond a size that will fit on a single server. Each individual partition must fit on the servers that host it, but a topic may have many partitions so it can handle an arbitrary amount of data. Second they act as the unit of parallelism—more on that in a bit.</p>
<h4 id="Distribution"><a href="#Distribution" class="headerlink" title="Distribution"></a>Distribution</h4><p>The partitions of the log are distributed over the servers in the Kafka cluster with each server handling data and requests for a share of the partitions. Each partition is replicated across a configurable number of servers for fault tolerance.<br>Each partition has one server which acts as the “leader” and zero or more servers which act as “followers”. The leader handles all read and write requests for the partition while the followers passively replicate the leader. If the leader fails, one of the followers will automatically become the new leader. Each server acts as a leader for some of its partitions and a follower for others so load is well balanced within the cluster.</p>
<h4 id="Producers"><a href="#Producers" class="headerlink" title="Producers"></a>Producers</h4><p>Producers publish data to the topics of their choice. The producer is responsible for choosing which message to assign to which partition within the topic. This can be done in a round-robin fashion simply to balance load or it can be done according to some semantic partition function (say based on some key in the message). More on the use of partitioning in a second.</p>
<h4 id="Consumers"><a href="#Consumers" class="headerlink" title="Consumers"></a>Consumers</h4><p>Messaging traditionally has two models: queuing and publish-subscribe. In a queue, a pool of consumers may read from a server and each message goes to one of them; in publish-subscribe the message is broadcast to all consumers. Kafka offers a single consumer abstraction that generalizes both of these—the consumer group.<br>Consumers label themselves with a consumer group name, and each message published to a topic is delivered to one consumer instance within each subscribing consumer group. Consumer instances can be in separate processes or on separate machines.</p>
<p>If all the consumer instances have the same consumer group, then this works just like a traditional queue balancing load over the consumers.</p>
<p>If all the consumer instances have different consumer groups, then this works like publish-subscribe and all messages are broadcast to all consumers.</p>
<p>More commonly, however, we have found that topics have a small number of consumer groups, one for each “logical subscriber”. Each group is composed of many consumer instances for scalability and fault tolerance. This is nothing more than publish-subscribe semantics where the subscriber is cluster of consumers instead of a single process.</p>
<p>Kafka has stronger ordering guarantees than a tranditional messageing system, too.</p>
<img src="/images/kafka.0.9.0/kafka_cluster.png" title="kafka_cluster">
<p>A traditional queue retains messages in-order on the server, and if multiple consumers consume from the queue then the server hands out messages in the order they are stored. However, although the server hands out messages in order, the messages are delivered asynchronously to consumers, so they may arrive out of order on different consumers. This effectively means the ordering of the messages is lost in the presence of parallel consumption. Messaging systems often work around this by having a notion of “exclusive consumer” that allows only one process to consume from a queue, but of course this means that there is no parallelism in processing.</p>
<p>Kafka does it better. By having a notion of parallelism—the partition—within the topics, Kafka is able to provide both ordering guarantees and load balancing over a pool of consumer processes. This is achieved by assigning the partitions in the topic to the consumers in the consumer group so that each partition is consumed by exactly one consumer in the group. By doing this we ensure that the consumer is the only reader of that partition and consumes the data in order. Since there are many partitions this still balances the load over many consumer instances. Note however that there cannot be more consumer instances in a consumer group than partitions.</p>
<p>Kafka only provides a total order over messages within a partition, not between different partitions in a topic. Per-partition ordering combined with the ability to partition data by key is sufficient for most applications. However, if you require a total order over messages this can be achieved with a topic that has only one partition, though this will mean only one consumer process per consumer group.</p>
<h4 id="Guarantees"><a href="#Guarantees" class="headerlink" title="Guarantees"></a>Guarantees</h4><p>At a high-level Kafka gives the following guarantees:</p>
<ul>
<li>Messages sent by a producer to a particular topic partition will be appended in the order they are sent. That is, if a message M1 is sent by the same producer as a message M2, and M1 is sent first, then M1 will have a lower offset than M2 and appear earlier in the log.</li>
<li>A consumer instance sees messages in the order they are stored in the log.</li>
<li>For a topic with replication factor N, we will tolerate up to N-1 server failures without losing any messages committed to the log.<br>More details on these guarantees are given in the design section of the documentation.</li>
</ul>
<h3 id="1-2_Use_Cases"><a href="#1-2_Use_Cases" class="headerlink" title="1.2 Use Cases"></a>1.2 Use Cases</h3><p>Here is a description of a few of the popular use cases for Apache Kafka. For an overview of a number of these areas in action, see this blog post.</p>
<h4 id="Messaging"><a href="#Messaging" class="headerlink" title="Messaging"></a>Messaging</h4><p>Kafka works well as a replacement for a more traditional message broker. Message brokers are used for a variety of reasons (to decouple processing from data producers, to buffer unprocessed messages, etc). In comparison to most messaging systems Kafka has better throughput, built-in partitioning, replication, and fault-tolerance which makes it a good solution for large scale message processing applications.<br>In our experience messaging uses are often comparatively low-throughput, but may require low end-to-end latency and often depend on the strong durability guarantees Kafka provides.</p>
<p>In this domain Kafka is comparable to traditional messaging systems such as ActiveMQ or RabbitMQ.</p>
<h4 id="Website_Activity_Tracking"><a href="#Website_Activity_Tracking" class="headerlink" title="Website Activity Tracking"></a>Website Activity Tracking</h4><p>The original use case for Kafka was to be able to rebuild a user activity tracking pipeline as a set of real-time publish-subscribe feeds. This means site activity (page views, searches, or other actions users may take) is published to central topics with one topic per activity type. These feeds are available for subscription for a range of use cases including real-time processing, real-time monitoring, and loading into Hadoop or offline data warehousing systems for offline processing and reporting.<br>Activity tracking is often very high volume as many activity messages are generated for each user page view.</p>
<h4 id="Metrics"><a href="#Metrics" class="headerlink" title="Metrics"></a>Metrics</h4><p>Kafka is often used for operational monitoring data. This involves aggregating statistics from distributed applications to produce centralized feeds of operational data.</p>
<h4 id="Log_Aggregation"><a href="#Log_Aggregation" class="headerlink" title="Log Aggregation"></a>Log Aggregation</h4><p>Many people use Kafka as a replacement for a log aggregation solution. Log aggregation typically collects physical log files off servers and puts them in a central place (a file server or HDFS perhaps) for processing. Kafka abstracts away the details of files and gives a cleaner abstraction of log or event data as a stream of messages. This allows for lower-latency processing and easier support for multiple data sources and distributed data consumption. In comparison to log-centric systems like Scribe or Flume, Kafka offers equally good performance, stronger durability guarantees due to replication, and much lower end-to-end latency.</p>
<h4 id="Stream_Processing"><a href="#Stream_Processing" class="headerlink" title="Stream Processing"></a>Stream Processing</h4><p>Many users end up doing stage-wise processing of data where data is consumed from topics of raw data and then aggregated, enriched, or otherwise transformed into new Kafka topics for further consumption. For example a processing flow for article recommendation might crawl article content from RSS feeds and publish it to an “articles” topic; further processing might help normalize or deduplicate this content to a topic of cleaned article content; a final stage might attempt to match this content to users. This creates a graph of real-time data flow out of the individual topics. Storm and Samza are popular frameworks for implementing these kinds of transformations.</p>
<h4 id="Event_Sourcing"><a href="#Event_Sourcing" class="headerlink" title="Event Sourcing"></a>Event Sourcing</h4><p>Event sourcing is a style of application design where state changes are logged as a time-ordered sequence of records. Kafka’s support for very large stored log data makes it an excellent backend for an application built in this style.</p>
<h4 id="Commit_Log"><a href="#Commit_Log" class="headerlink" title="Commit Log"></a>Commit Log</h4><p>Kafka can serve as a kind of external commit-log for a distributed system. The log helps replicate data between nodes and acts as a re-syncing mechanism for failed nodes to restore their data. The log compaction feature in Kafka helps support this usage. In this usage Kafka is similar to Apache BookKeeper project.</p>
<h3 id="1-3_Quick_Start"><a href="#1-3_Quick_Start" class="headerlink" title="1.3 Quick Start"></a>1.3 Quick Start</h3><p>This tutorial assumes you are starting fresh and have no existing Kafka or ZooKeeper data.</p>
<h4 id="Step_1_3A_Download_the_code"><a href="#Step_1_3A_Download_the_code" class="headerlink" title="Step 1: Download the code"></a>Step 1: Download the code</h4><p>Download the 0.9.0.0 release and un-tar it.</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="tag">tar</span> <span class="tag">-xzf</span> <span class="tag">kafka_2</span><span class="class">.11-0</span><span class="class">.9</span><span class="class">.0</span><span class="class">.0</span><span class="class">.tgz</span></span><br><span class="line">&gt; <span class="tag">cd</span> <span class="tag">kafka_2</span><span class="class">.11-0</span><span class="class">.9</span><span class="class">.0</span><span class="class">.0</span></span><br></pre></td></tr></table></figure>
<h4 id="Step_2_3A_Start_the_server"><a href="#Step_2_3A_Start_the_server" class="headerlink" title="Step 2: Start the server"></a>Step 2: Start the server</h4><p>Kafka uses ZooKeeper so you need to first start a ZooKeeper server if you don’t already have one. You can use the convenience script packaged with kafka to get a quick-and-dirty single-node ZooKeeper instance.</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/zookeeper-server-start<span class="class">.sh</span> config/zookeeper<span class="class">.properties</span></span><br><span class="line">[<span class="number">2013</span>-<span class="number">04</span>-<span class="number">22</span> <span class="number">15</span>:<span class="number">01</span>:<span class="number">37</span>,<span class="number">495</span>] INFO Reading configuration from: config/zookeeper<span class="class">.properties</span> (org<span class="class">.apache</span><span class="class">.zookeeper</span><span class="class">.server</span><span class="class">.quorum</span><span class="class">.QuorumPeerConfig</span>)</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<h4 id="Step_3_3A_Create_a_topic"><a href="#Step_3_3A_Create_a_topic" class="headerlink" title="Step 3: Create a topic"></a>Step 3: Create a topic</h4><p>Let’s create a topic named “test” with a single partition and only one replica:</p>
<figure class="highlight brainfuck"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="comment">bin/kafka</span><span class="literal">-</span><span class="comment">topics</span><span class="string">.</span><span class="comment">sh</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">create</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">zookeeper</span> <span class="comment">localhost:2181</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">replication</span><span class="literal">-</span><span class="comment">factor</span> <span class="comment">1</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">partitions</span> <span class="comment">1</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">topic</span> <span class="comment">test</span></span><br></pre></td></tr></table></figure>
<p>We can now see that topic if we run the list topic command:</p>
<figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/kafka-topics.sh <span class="comment">--list --zookeeper localhost:2181</span></span><br><span class="line">test</span><br><span class="line">Alternatively, instead <span class="operator">of</span> manually creating topics you can also configure your brokers <span class="built_in">to</span> auto-<span class="built_in">create</span> topics when <span class="operator">a</span> non-existent topic is published <span class="built_in">to</span>.</span><br></pre></td></tr></table></figure>
<h4 id="Step_4_3A_Send_some_messages"><a href="#Step_4_3A_Send_some_messages" class="headerlink" title="Step 4: Send some messages"></a>Step 4: Send some messages</h4><p>Kafka comes with a command line client that will take input from a file or from standard input and send it out as messages to the Kafka cluster. By default each line will be sent as a separate message.<br>Run the producer and then type a few messages into the console to send to the server.</p>
<figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/kafka-console-producer.<span class="keyword">sh</span> --broker-<span class="keyword">list</span> localhos<span class="variable">t:9092</span> --topic test</span><br><span class="line">This <span class="keyword">is</span> <span class="keyword">a</span> message</span><br><span class="line">This <span class="keyword">is</span> another message</span><br></pre></td></tr></table></figure>
<h4 id="Step_5_3A_Start_a_consumer"><a href="#Step_5_3A_Start_a_consumer" class="headerlink" title="Step 5: Start a consumer"></a>Step 5: Start a consumer</h4><p>Kafka also has a command line consumer that will dump out messages to standard output.</p>
<figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/kafka-console-consumer.sh <span class="comment">--zookeeper localhost:2181 --topic test --from-beginning</span></span><br><span class="line">This is <span class="operator">a</span> message</span><br><span class="line">This is another message</span><br><span class="line">If you have <span class="keyword">each</span> <span class="operator">of</span> <span class="operator">the</span> above commands running <span class="operator">in</span> <span class="operator">a</span> different terminal <span class="keyword">then</span> you should now be able <span class="built_in">to</span> type messages <span class="keyword">into</span> <span class="operator">the</span> producer terminal <span class="operator">and</span> see them appear <span class="operator">in</span> <span class="operator">the</span> consumer terminal.</span><br></pre></td></tr></table></figure>
<p>All of the command line tools have additional options; running the command with no arguments will display usage information documenting them in more detail.</p>
<h4 id="Step_6_3A_Setting_up_a_multi-broker_cluster"><a href="#Step_6_3A_Setting_up_a_multi-broker_cluster" class="headerlink" title="Step 6: Setting up a multi-broker cluster"></a>Step 6: Setting up a multi-broker cluster</h4><p>So far we have been running against a single broker, but that’s no fun. For Kafka, a single broker is just a cluster of size one, so nothing much changes other than starting a few more broker instances. But just to get feel for it, let’s expand our cluster to three nodes (still all on our local machine).<br>First we make a config file for each of the brokers:</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; cp config/server<span class="class">.properties</span> config/server-<span class="number">1</span><span class="class">.properties</span></span><br><span class="line">&gt; cp config/server<span class="class">.properties</span> config/server-<span class="number">2</span>.properties</span><br></pre></td></tr></table></figure>
<p>Now edit these new files and set the following properties:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">config/server-<span class="number">1.</span>properties:</span><br><span class="line">    broker.id=<span class="number">1</span></span><br><span class="line">    port=<span class="number">9093</span></span><br><span class="line">    <span class="built_in">log</span>.dir=/tmp/kafka-logs-<span class="number">1</span></span><br><span class="line"></span><br><span class="line">config/server-<span class="number">2.</span>properties:</span><br><span class="line">    broker.id=<span class="number">2</span></span><br><span class="line">    port=<span class="number">9094</span></span><br><span class="line">    <span class="built_in">log</span>.dir=/tmp/kafka-logs-<span class="number">2</span></span><br></pre></td></tr></table></figure>
<p>The broker.id property is the unique and permanent name of each node in the cluster. We have to override the port and log directory only because we are running these all on the same machine and we want to keep the brokers from all trying to register on the same port or overwrite each others data.</p>
<p>We already have Zookeeper and our single node started, so we just need to start the two new nodes:</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/kafka-server-start<span class="class">.sh</span> config/server-<span class="number">1</span><span class="class">.properties</span> &amp;</span><br><span class="line">...</span><br><span class="line">&gt; bin/kafka-server-start<span class="class">.sh</span> config/server-<span class="number">2</span><span class="class">.properties</span> &amp;</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>Now create a new topic with a replication factor of three:</p>
<figure class="highlight brainfuck"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="comment">bin/kafka</span><span class="literal">-</span><span class="comment">topics</span><span class="string">.</span><span class="comment">sh</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">create</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">zookeeper</span> <span class="comment">localhost:2181</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">replication</span><span class="literal">-</span><span class="comment">factor</span> <span class="comment">3</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">partitions</span> <span class="comment">1</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">topic</span> <span class="comment">my</span><span class="literal">-</span><span class="comment">replicated</span><span class="literal">-</span><span class="comment">topic</span></span><br></pre></td></tr></table></figure>
<p>Okay but now that we have a cluster how can we know which broker is doing what? To see that run the “describe topics” command:</p>
<figure class="highlight brainfuck"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="comment">bin/kafka</span><span class="literal">-</span><span class="comment">topics</span><span class="string">.</span><span class="comment">sh</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">describe</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">zookeeper</span> <span class="comment">localhost:2181</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">topic</span> <span class="comment">my</span><span class="literal">-</span><span class="comment">replicated</span><span class="literal">-</span><span class="comment">topic</span></span><br></pre></td></tr></table></figure>
<p>Topic:my-replicated-topic    PartitionCount:1    ReplicationFactor:3    Configs:</p>
<pre><code>Topic: my-replicated-topic    Partition: 0    Leader: 1    Replicas: 1,2,0    Isr: 1,2,0
</code></pre><p>Here is an explanation of output. The first line gives a summary of all the partitions, each additional line gives information about one partition. Since we have only one partition for this topic there is only one line.</p>
<ul>
<li>“leader” is the node responsible for all reads and writes for the given partition. Each node will be the leader for a randomly selected portion of the partitions.</li>
<li>“replicas” is the list of nodes that replicate the log for this partition regardless of whether they are the leader or even if they are currently alive.</li>
<li>“isr” is the set of “in-sync” replicas. This is the subset of the replicas list that is currently alive and caught-up to the leader.</li>
</ul>
<p>Note that in my example node 1 is the leader for the only partition of the topic.</p>
<p>We can run the same command on the original topic we created to see where it is:</p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/kafka-topics.sh --describe --zookeeper <span class="string">localhost:</span><span class="number">2181</span> --topic test</span><br><span class="line"><span class="label"></span><br><span class="line">Topic:</span>test	<span class="string">PartitionCount:</span><span class="number">1</span>	<span class="string">ReplicationFactor:</span><span class="number">1</span>	<span class="string">Configs:</span></span><br><span class="line"><span class="label">	Topic:</span> test	<span class="string">Partition:</span> <span class="number">0</span>	<span class="string">Leader:</span> <span class="number">0</span>	<span class="string">Replicas:</span> <span class="number">0</span>	<span class="string">Isr:</span> <span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>So there is no surprise there—the original topic has no replicas and is on server 0, the only server in our cluster when we created it.</p>
<p>Let’s publish a few messages to our new topic:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/kafka-console-producer.sh --broker-<span class="built_in">list</span> localhost:<span class="number">9092</span> --topic my-replicated-topic</span><br><span class="line">...</span><br><span class="line">my test message <span class="number">1</span></span><br><span class="line">my test message <span class="number">2</span></span><br><span class="line">^C</span><br></pre></td></tr></table></figure>
<p>Now let’s consume these messages:</p>
<figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/kafka-console-consumer.sh <span class="comment">--zookeeper localhost:2181 --from-beginning --topic my-replicated-topic</span></span><br><span class="line">...</span><br><span class="line"><span class="keyword">my</span> test message <span class="number">1</span></span><br><span class="line"><span class="keyword">my</span> test message <span class="number">2</span></span><br><span class="line">^C</span><br></pre></td></tr></table></figure>
<p>Now let’s test out fault-tolerance. Broker 1 was acting as the leader so let’s kill it:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; ps | grep server-<span class="number">1.</span>properties</span><br><span class="line"><span class="number">7564</span> ttys002    <span class="number">0</span>:<span class="number">15.91</span> /System/Library/Frameworks/JavaVM.framework/Versions/<span class="number">1.6</span>/Home/bin/java...</span><br><span class="line">&gt; kill -<span class="number">9</span> <span class="number">7564</span></span><br></pre></td></tr></table></figure>
<p>Leadership has switched to one of the slaves and node 1 is no longer in the in-sync replica set:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/kafka-topics.sh --describe --zookeeper localhost:<span class="number">2181</span> --topic my-replicated-topic</span><br><span class="line">Topic:my-replicated-topic	PartitionCount:<span class="number">1</span>	ReplicationFactor:<span class="number">3</span>	Configs:</span><br><span class="line">	Topic: my-replicated-topic	Partition: <span class="number">0</span>	Leader: <span class="number">2</span>	Replicas: <span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>	Isr: <span class="number">2</span>,<span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>But the messages are still be available for consumption even though the leader that took the writes originally is down:</p>
<figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/kafka-console-consumer.sh <span class="comment">--zookeeper localhost:2181 --from-beginning --topic my-replicated-topic</span></span><br><span class="line">...</span><br><span class="line"><span class="keyword">my</span> test message <span class="number">1</span></span><br><span class="line"><span class="keyword">my</span> test message <span class="number">2</span></span><br><span class="line">^C</span><br></pre></td></tr></table></figure>
<h4 id="Step_7_3A_Use_Kafka_Connect_to_import/export_data"><a href="#Step_7_3A_Use_Kafka_Connect_to_import/export_data" class="headerlink" title="Step 7: Use Kafka Connect to import/export data"></a>Step 7: Use Kafka Connect to import/export data</h4><p>Writing data from the console and writing it back to the console is a convenient place to start, but you’ll probably want to use data from other sources or export data from Kafka to other systems. For many systems, instead of writing custom integration code you can use Kafka Connect to import or export data. Kafka Connect is a tool included with Kafka that imports and exports data to Kafka. It is an extensible tool that runs connectors, which implement the custom logic for interacting with an external system. In this quickstart we’ll see how to run Kafka Connect with simple connectors that import data from a file to a Kafka topic and export data from a Kafka topic to a file. First, we’ll start by creating some seed data to test with:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="built_in">echo</span> <span class="operator">-e</span> <span class="string">"foo\nbar"</span> &gt; test.txt</span><br></pre></td></tr></table></figure>
<p>Next, we’ll start two connectors running in standalone mode, which means they run in a single, local, dedicated process. We provide three configuration files as parameters. The first is always the configuration for the Kafka Connect process, containing common configuration such as the Kafka brokers to connect to and the serialization format for data. The remaining configuration files each specify a connector to create. These files include a unique connector name, the connector class to instantiate, and any other configuration required by the connector.</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/connect-standalone<span class="class">.sh</span> config/connect-standalone<span class="class">.properties</span> config/connect-file-source<span class="class">.properties</span> config/connect-file-sink.properties</span><br></pre></td></tr></table></figure>
<p>These sample configuration files, included with Kafka, use the default local cluster configuration you started earlier and create two connectors: the first is a source connector that reads lines from an input file and produces each to a Kafka topic and the second is a sink connector that reads messages from a Kafka topic and produces each as a line in an output file. During startup you’ll see a number of log messages, including some indicating that the connectors are being instantiated. Once the Kafka Connect process has started, the source connector should start reading lines from</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">test</span><span class="class">.txt</span></span><br></pre></td></tr></table></figure>
<p>and producing them to the topic</p>
<figure class="highlight perl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">connect</span>-test</span><br></pre></td></tr></table></figure>
<p>, and the sink connector should start reading messages from the topic</p>
<figure class="highlight perl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">connect</span>-test</span><br></pre></td></tr></table></figure>
<p>and write them to the file</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">test<span class="class">.sink</span><span class="class">.txt</span></span><br></pre></td></tr></table></figure>
<p>. We can verify the data has been delivered through the entire pipeline by examining the contents of the output file:</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; cat test<span class="class">.sink</span><span class="class">.txt</span></span><br><span class="line">foo</span><br><span class="line">bar</span><br></pre></td></tr></table></figure>
<p>Note that the data is being stored in the Kafka topic</p>
<figure class="highlight perl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">connect</span>-test</span><br></pre></td></tr></table></figure>
<p>, so we can also run a console consumer to see the data in the topic (or use custom consumer code to process it):</p>
<figure class="highlight livescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/kafka-<span class="built_in">console</span>-consumer.sh --zookeeper <span class="attribute">localhost</span>:<span class="number">2181</span> --topic connect-test --<span class="keyword">from</span>-beginning</span><br><span class="line">&#123;<span class="string">"schema"</span>:&#123;<span class="string">"type"</span>:<span class="string">"string"</span>,<span class="string">"optional"</span>:<span class="literal">false</span>&#125;,<span class="string">"payload"</span>:<span class="string">"foo"</span>&#125;</span><br><span class="line">&#123;<span class="string">"schema"</span>:&#123;<span class="string">"type"</span>:<span class="string">"string"</span>,<span class="string">"optional"</span>:<span class="literal">false</span>&#125;,<span class="string">"payload"</span>:<span class="string">"bar"</span>&#125;</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>The connectors continue to process data, so we can add data to the file and see it move through the pipeline:</p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; echo <span class="string">"Another line"</span> <span class="prompt">&gt;&gt; </span>test.txt</span><br></pre></td></tr></table></figure>
<p>You should see the line appear in the console consumer output and in the sink file.</p>
<h3 id="1-4_Ecosystem"><a href="#1-4_Ecosystem" class="headerlink" title="1.4 Ecosystem"></a>1.4 Ecosystem</h3><p>There are a plethora of tools that integrate with Kafka outside the main distribution. The ecosystem page lists many of these, including stream processing systems, Hadoop integration, monitoring, and deployment tools.</p>
<h2 id="2-_API"><a href="#2-_API" class="headerlink" title="2. API"></a>2. API</h2><p>Apache Kafka includes new java clients (in the org.apache.kafka.clients package). These are meant to supplant the older Scala clients, but for compatability they will co-exist for some time. These clients are available in a seperate jar with minimal dependencies, while the old Scala clients remain packaged with the server.</p>
<h3 id="2-1_Producer_API"><a href="#2-1_Producer_API" class="headerlink" title="2.1 Producer API"></a>2.1 Producer API</h3><p>We encourage all new development to use the new Java producer. This client is production tested and generally both faster and more fully featured than the previous Scala client. You can use this client by adding a dependency on the client jar using the following example maven co-ordinates (you can change the version numbers with new releases):</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"> <span class="tag">&lt;<span class="title">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">groupId</span>&gt;</span>org.apache.kafka<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>kafka-clients<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">version</span>&gt;</span>0.9.0.0<span class="tag">&lt;/<span class="title">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>Examples showing how to use the producer are given in the <a href="http://kafka.apache.org/090/javadoc/index.html?org/apache/kafka/clients/producer/KafkaProducer.html" target="_blank" rel="external">javadocs</a>.<br>For those interested in the legacy Scala producer api, information can be found <a href="http://kafka.apache.org/081/documentation.html#producerapi" target="_blank" rel="external">here</a>.</p>
<h3 id="2-2_Consumer_API"><a href="#2-2_Consumer_API" class="headerlink" title="2.2 Consumer API"></a>2.2 Consumer API</h3><p>As of the 0.9.0 release we have added a new Java consumer to replace our existing high-level ZooKeeper-based consumer and low-level consumer APIs. This client is considered beta quality. To ensure a smooth upgrade paths for users, we still maintain the old 0.8 consumer clients that continue to work on an 0.9 Kafka cluster. In the following sections we introduce both the old 0.8 consumer APIs (both high-level ConsumerConnector and low-level SimpleConsumer) and the new Java consumer API respectively.</p>
<h4 id="2-2-1_Old_High_Level_Consumer_API"><a href="#2-2-1_Old_High_Level_Consumer_API" class="headerlink" title="2.2.1 Old High Level Consumer API"></a>2.2.1 Old High Level Consumer API</h4><figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line">class Consumer &#123;</span><br><span class="line">  /<span class="keyword">*</span><span class="keyword">*</span></span><br><span class="line">   <span class="keyword">*</span>  Create a ConsumerConnector</span><br><span class="line">   <span class="keyword">*</span></span><br><span class="line">   <span class="keyword">*</span>  <span class="comment">@param config  at the minimum, need to specify the groupid of the consumer and the zookeeper</span></span><br><span class="line">   <span class="keyword">*</span>                 connection string zookeeper.connect.</span><br><span class="line">   <span class="keyword">*</span>/</span><br><span class="line">  public static kafka.javaapi.consumer.ConsumerConnector createJavaConsumerConnector(ConsumerConfig config);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/<span class="keyword">*</span><span class="keyword">*</span></span><br><span class="line"> <span class="keyword">*</span>  V: type of the message</span><br><span class="line"> <span class="keyword">*</span>  K: type of the optional key assciated with the message</span><br><span class="line"> <span class="keyword">*</span>/</span><br><span class="line">public interface kafka.javaapi.consumer.ConsumerConnector &#123;</span><br><span class="line">  /<span class="keyword">*</span><span class="keyword">*</span></span><br><span class="line">   <span class="keyword">*</span>  Create a list of message streams of type T for each topic.</span><br><span class="line">   <span class="keyword">*</span></span><br><span class="line">   <span class="keyword">*</span>  <span class="comment">@param topicCountMap  a map of (topic, #streams) pair</span></span><br><span class="line">   <span class="keyword">*</span>  <span class="comment">@param decoder a decoder that converts from Message to T</span></span><br><span class="line">   <span class="keyword">*</span>  <span class="comment">@return a map of (topic, list of  KafkaStream) pairs.</span></span><br><span class="line">   <span class="keyword">*</span>          The number of items in the list is <span class="comment">#streams. Each stream supports</span></span><br><span class="line">   <span class="keyword">*</span>          an iterator over message/metadata pairs.</span><br><span class="line">   <span class="keyword">*</span>/</span><br><span class="line">  public <span class="variable">&lt;K,V&gt;</span> Map<span class="variable">&lt;String, List&lt;KafkaStream&lt;K,V&gt;</span>&gt;&gt;</span><br><span class="line">    createMessageStreams(Map<span class="variable">&lt;String, Integer&gt;</span> topicCountMap, Decoder<span class="variable">&lt;K&gt;</span> keyDecoder, Decoder<span class="variable">&lt;V&gt;</span> valueDecoder);</span><br><span class="line"></span><br><span class="line">  /<span class="keyword">*</span><span class="keyword">*</span></span><br><span class="line">   <span class="keyword">*</span>  Create a list of message streams of type T for each topic, using the default decoder.</span><br><span class="line">   <span class="keyword">*</span>/</span><br><span class="line">  public Map<span class="variable">&lt;String, List&lt;KafkaStream&lt;byte[], byte[]&gt;</span>&gt;&gt; createMessageStreams(Map<span class="variable">&lt;String, Integer&gt;</span> topicCountMap);</span><br><span class="line"></span><br><span class="line">  /<span class="keyword">*</span><span class="keyword">*</span></span><br><span class="line">   <span class="keyword">*</span>  Create a list of message streams for topics matching a wildcard.</span><br><span class="line">   <span class="keyword">*</span></span><br><span class="line">   <span class="keyword">*</span>  <span class="comment">@param topicFilter a TopicFilter that specifies which topics to</span></span><br><span class="line">   <span class="keyword">*</span>                    subscribe to (encapsulates a whitelist or a blacklist).</span><br><span class="line">   <span class="keyword">*</span>  <span class="comment">@param numStreams the number of message streams to return.</span></span><br><span class="line">   <span class="keyword">*</span>  <span class="comment">@param keyDecoder a decoder that decodes the message key</span></span><br><span class="line">   <span class="keyword">*</span>  <span class="comment">@param valueDecoder a decoder that decodes the message itself</span></span><br><span class="line">   <span class="keyword">*</span>  <span class="comment">@return a list of KafkaStream. Each stream supports an</span></span><br><span class="line">   <span class="keyword">*</span>          iterator over its MessageAndMetadata elements.</span><br><span class="line">   <span class="keyword">*</span>/</span><br><span class="line">  public <span class="variable">&lt;K,V&gt;</span> List<span class="variable">&lt;KafkaStream&lt;K,V&gt;</span>&gt;</span><br><span class="line">    createMessageStreamsByFilter(TopicFilter topicFilter, int numStreams, Decoder<span class="variable">&lt;K&gt;</span> keyDecoder, Decoder<span class="variable">&lt;V&gt;</span> valueDecoder);</span><br><span class="line"></span><br><span class="line">  /<span class="keyword">*</span><span class="keyword">*</span></span><br><span class="line">   <span class="keyword">*</span>  Create a list of message streams for topics matching a wildcard, using the default decoder.</span><br><span class="line">   <span class="keyword">*</span>/</span><br><span class="line">  public List<span class="variable">&lt;KafkaStream&lt;byte[], byte[]&gt;</span>&gt; createMessageStreamsByFilter(TopicFilter topicFilter, int numStreams);</span><br><span class="line"></span><br><span class="line">  /<span class="keyword">*</span><span class="keyword">*</span></span><br><span class="line">   <span class="keyword">*</span>  Create a list of message streams for topics matching a wildcard, using the default decoder, with one stream.</span><br><span class="line">   <span class="keyword">*</span>/</span><br><span class="line">  public List<span class="variable">&lt;KafkaStream&lt;byte[], byte[]&gt;</span>&gt; createMessageStreamsByFilter(TopicFilter topicFilter);</span><br><span class="line"></span><br><span class="line">  /<span class="keyword">*</span><span class="keyword">*</span></span><br><span class="line">   <span class="keyword">*</span>  Commit the offsets of all topic/partitions connected by this connector.</span><br><span class="line">   <span class="keyword">*</span>/</span><br><span class="line">  public void commitOffsets();</span><br><span class="line"></span><br><span class="line">  /<span class="keyword">*</span><span class="keyword">*</span></span><br><span class="line">   <span class="keyword">*</span>  Shut down the connector</span><br><span class="line">   <span class="keyword">*</span>/</span><br><span class="line">  public void shutdown();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>You can follow this <a href="https://cwiki.apache.org/confluence/display/KAFKA/Consumer+Group+Example" target="_blank" rel="external">example</a> to learn how to use the high level consumer api.</p>
<h4 id="2-2-2_Old_Simple_Consumer_API"><a href="#2-2-2_Old_Simple_Consumer_API" class="headerlink" title="2.2.2 Old Simple Consumer API"></a>2.2.2 Old Simple Consumer API</h4><figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">class kafka.javaapi.consumer.SimpleConsumer &#123;</span><br><span class="line">  /<span class="keyword">*</span><span class="keyword">*</span></span><br><span class="line">   <span class="keyword">*</span>  Fetch a set of messages from a topic.</span><br><span class="line">   <span class="keyword">*</span></span><br><span class="line">   <span class="keyword">*</span>  <span class="comment">@param request specifies the topic name, topic partition, starting byte offset, maximum bytes to be fetched.</span></span><br><span class="line">   <span class="keyword">*</span>  <span class="comment">@return a set of fetched messages</span></span><br><span class="line">   <span class="keyword">*</span>/</span><br><span class="line">  public FetchResponse fetch(kafka.javaapi.FetchRequest request);</span><br><span class="line"></span><br><span class="line">  /<span class="keyword">*</span><span class="keyword">*</span></span><br><span class="line">   <span class="keyword">*</span>  Fetch metadata for a sequence of topics.</span><br><span class="line">   <span class="keyword">*</span></span><br><span class="line">   <span class="keyword">*</span>  <span class="comment">@param request specifies the versionId, clientId, sequence of topics.</span></span><br><span class="line">   <span class="keyword">*</span>  <span class="comment">@return metadata for each topic in the request.</span></span><br><span class="line">   <span class="keyword">*</span>/</span><br><span class="line">  public kafka.javaapi.TopicMetadataResponse send(kafka.javaapi.TopicMetadataRequest request);</span><br><span class="line"></span><br><span class="line">  /<span class="keyword">*</span><span class="keyword">*</span></span><br><span class="line">   <span class="keyword">*</span>  Get a list of valid offsets (up to maxSize) before the given time.</span><br><span class="line">   <span class="keyword">*</span></span><br><span class="line">   <span class="keyword">*</span>  <span class="comment">@param request a [[kafka.javaapi.OffsetRequest]] object.</span></span><br><span class="line">   <span class="keyword">*</span>  <span class="comment">@return a [[kafka.javaapi.OffsetResponse]] object.</span></span><br><span class="line">   <span class="keyword">*</span>/</span><br><span class="line">  public kafka.javaapi.OffsetResponse getOffsetsBefore(OffsetRequest request);</span><br><span class="line"></span><br><span class="line">  /<span class="keyword">*</span><span class="keyword">*</span></span><br><span class="line">   <span class="keyword">*</span> Close the SimpleConsumer.</span><br><span class="line">   <span class="keyword">*</span>/</span><br><span class="line">  public void close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>For most applications, the high level consumer Api is good enough. Some applications want features not exposed to the high level consumer yet (e.g., set initial offset when restarting the consumer). They can instead use our low level SimpleConsumer Api. The logic will be a bit more complicated and you can follow the example in <a href="https://cwiki.apache.org/confluence/display/KAFKA/0.8.0+SimpleConsumer+Example" target="_blank" rel="external">here</a>.</p>
<h4 id="2-2-3_New_Consumer_API"><a href="#2-2-3_New_Consumer_API" class="headerlink" title="2.2.3 New Consumer API"></a>2.2.3 New Consumer API</h4><p>This new unified consumer API removes the distinction between the 0.8 high-level and low-level consumer APIs. You can use this client by adding a dependency on the client jar using the following example maven co-ordinates (you can change the version numbers with new releases):</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"> <span class="tag">&lt;<span class="title">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">groupId</span>&gt;</span>org.apache.kafka<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>kafka-clients<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">version</span>&gt;</span>0.9.0.0<span class="tag">&lt;/<span class="title">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>Examples showing how to use the consumer are given in the <a href="http://kafka.apache.org/090/javadoc/index.html?org/apache/kafka/clients/consumer/KafkaConsumer.html" target="_blank" rel="external">javadocs</a>.</p>
<h2 id="3-_Configuration"><a href="#3-_Configuration" class="headerlink" title="3. Configuration"></a>3. Configuration</h2><p>Kafka uses key-value pairs in the <a href="http://en.wikipedia.org/wiki/.properties" target="_blank" rel="external">property file format</a> for configuration. These values can be supplied either from a file or programmatically.</p>
<h3 id="3-1_Broker_Configs"><a href="#3-1_Broker_Configs" class="headerlink" title="3.1 Broker Configs"></a>3.1 Broker Configs</h3><p>The essential configurations are the following:</p>
<ul>
<li>broker.id</li>
<li>log.dirs</li>
<li>zookeeper.connect</li>
</ul>
<p>Topic-level configurations and defaults are discussed in more detail</p>
<p>Configurations pertinent to topics have both a global default as well an optional per-topic override. If no per-topic configuration is given the global default is used. The override can be set at topic creation time by giving one or more –config options. This example creates a topic named my-topic with a custom max message size and flush rate:</p>
<figure class="highlight brainfuck"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="comment">bin/kafka</span><span class="literal">-</span><span class="comment">topics</span><span class="string">.</span><span class="comment">sh</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">zookeeper</span> <span class="comment">localhost:2181</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">create</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">topic</span> <span class="comment">my</span><span class="literal">-</span><span class="comment">topic</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">partitions</span> <span class="comment">1</span></span><br><span class="line">        <span class="literal">-</span><span class="literal">-</span><span class="comment">replication</span><span class="literal">-</span><span class="comment">factor</span> <span class="comment">1</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">config</span> <span class="comment">max</span><span class="string">.</span><span class="comment">message</span><span class="string">.</span><span class="comment">bytes=64000</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">config</span> <span class="comment">flush</span><span class="string">.</span><span class="comment">messages=1</span></span><br></pre></td></tr></table></figure>
<p>Overrides can also be changed or set later using the alter topic command. This example updates the max message size for my-topic:</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/kafka-topics<span class="class">.sh</span> --zookeeper localhost:<span class="number">2181</span> --alter --topic my-topic</span><br><span class="line">    --config max<span class="class">.message</span><span class="class">.bytes</span>=<span class="number">128000</span></span><br></pre></td></tr></table></figure>
<p>To remove an override you can do</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/kafka-topics<span class="class">.sh</span> --zookeeper localhost:<span class="number">2181</span> --alter --topic my-topic</span><br><span class="line">    --deleteConfig max<span class="class">.message</span><span class="class">.bytes</span></span><br></pre></td></tr></table></figure>
<p>The following are the topic-level configurations. The server’s default configuration for this property is given under the Server Default Property heading, setting this default in the server config allows you to change the default given to topics that have no override specified.</p>
<h2 id="4-_Design"><a href="#4-_Design" class="headerlink" title="4. Design"></a>4. Design</h2><h3 id="4-1_Motivation"><a href="#4-1_Motivation" class="headerlink" title="4.1 Motivation"></a>4.1 Motivation</h3><p>We designed Kafka to be able to act as a unified platform for handling all the real-time data feeds a large company might have. To do this we had to think through a fairly broad set of use cases.</p>
<p>It would have to have high-throughput to support high volume event streams such as real-time log aggregation.</p>
<p>It would need to deal gracefully with large data backlogs to be able to support periodic data loads from offline systems.</p>
<p>It also meant the system would have to handle low-latency delivery to handle more traditional messaging use-cases.</p>
<p>We wanted to support partitioned, distributed, real-time processing of these feeds to create new, derived feeds. This motivated our partitioning and consumer model.</p>
<p>Finally in cases where the stream is fed into other data systems for serving we knew the system would have to be able to guarantee fault-tolerance in the presence of machine failures.</p>
<p>Supporting these uses led use to a design with a number of unique elements, more akin to a database log then a traditional messaging system. We will outline some elements of the design in the following sections.</p>
<h3 id="4-2_Persistence"><a href="#4-2_Persistence" class="headerlink" title="4.2 Persistence"></a>4.2 Persistence</h3><p><em>Don’t fear the filesystem!</em></p>
<p>Kafka relies heavily on the filesystem for storing and caching messages. There is a general perception that “disks are slow” which makes people skeptical that a persistent structure can offer competitive performance. In fact disks are both much slower and much faster than people expect depending on how they are used; and a properly designed disk structure can often be as fast as the network.</p>
<p>The key fact about disk performance is that the throughput of hard drives has been diverging from the latency of a disk seek for the last decade. As a result the performance of linear writes on a <a href="http://en.wikipedia.org/wiki/Non-RAID_drive_architectures" target="_blank" rel="external">JBOD</a> configuration with six 7200rpm SATA RAID-5 array is about 600MB/sec but the performance of random writes is only about 100k/sec—a difference of over 6000X. These linear reads and writes are the most predictable of all usage patterns, and are heavily optimized by the operating system. A modern operating system provides read-ahead and write-behind techniques that prefetch data in large block multiples and group smaller logical writes into large physical writes. A further discussion of this issue can be found in this <a href="http://queue.acm.org/detail.cfm?id=1563874" target="_blank" rel="external">ACM Queue article</a>; they actually find that <a href="http://deliveryimages.acm.org/10.1145/1570000/1563874/jacobs3.jpg" target="_blank" rel="external">sequential disk access can in some cases be faster than random memory access</a>!</p>
<p>To compensate for this performance divergence modern operating systems have become increasingly aggressive in their use of main memory for disk caching. A modern OS will happily divert all free memory to disk caching with little performance penalty when the memory is reclaimed. All disk reads and writes will go through this unified cache. This feature cannot easily be turned off without using direct I/O, so even if a process maintains an in-process cache of the data, this data will likely be duplicated in OS pagecache, effectively storing everything twice.</p>
<p>Furthermore we are building on top of the JVM, and anyone who has spent any time with Java memory usage knows two things:</p>
<ol>
<li>The memory overhead of objects is very high, often doubling the size of the data stored (or worse).</li>
<li>Java garbage collection becomes increasingly fiddly and slow as the in-heap data increases.</li>
</ol>
<p>As a result of these factors using the filesystem and relying on pagecache is superior to maintaining an in-memory cache or other structure—we at least double the available cache by having automatic access to all free memory, and likely double again by storing a compact byte structure rather than individual objects. Doing so will result in a cache of up to 28-30GB on a 32GB machine without GC penalties. Furthermore this cache will stay warm even if the service is restarted, whereas the in-process cache will need to be rebuilt in memory (which for a 10GB cache may take 10 minutes) or else it will need to start with a completely cold cache (which likely means terrible initial performance). This also greatly simplifies the code as all logic for maintaining coherency between the cache and filesystem is now in the OS, which tends to do so more efficiently and more correctly than one-off in-process attempts. If your disk usage favors linear reads then read-ahead is effectively pre-populating this cache with useful data on each disk read.</p>
<p>This suggests a design which is very simple: rather than maintain as much as possible in-memory and flush it all out to the filesystem in a panic when we run out of space, we invert that. All data is immediately written to a persistent log on the filesystem without necessarily flushing to disk. In effect this just means that it is transferred into the kernel’s pagecache.</p>
<p>This style of pagecache-centric design is described in an article on the design of Varnish here (along with a healthy dose of arrogance).</p>
<p>Intuitively a persistent queue could be built on simple reads and appends to files as is commonly the case with logging solutions. This structure has the advantage that all operations are O(1) and reads do not block writes or each other. This has obvious performance advantages since the performance is completely decoupled from the data size—one server can now take full advantage of a number of cheap, low-rotational speed 1+TB SATA drives. Though they have poor seek performance, these drives have acceptable performance for large reads and writes and come at 1/3 the price and 3x the capacity.</p>
<p>Having access to virtually unlimited disk space without any performance penalty means that we can provide some features not usually found in a messaging system. For example, in Kafka, instead of attempting to deleting messages as soon as they are consumed, we can retain messages for a relative long period (say a week). This leads to a great deal of flexibility for consumers, as we will describe.</p>
<h2 id="u53C2_u8003_u6587_u732E"><a href="#u53C2_u8003_u6587_u732E" class="headerlink" title="参考文献"></a>参考文献</h2><ul>
<li><a href="https://github.com/docs2cn/apache-kafka-docs/blob/master/index.md" target="_blank" rel="external">https://github.com/docs2cn/apache-kafka-docs/blob/master/index.md</a></li>
<li><a href="http://kafka.apache.org/documentation.html" target="_blank" rel="external">http://kafka.apache.org/documentation.html</a></li>
<li><a href="http://www.cnblogs.com/lzqhss/p/4434901.html" target="_blank" rel="external">http://www.cnblogs.com/lzqhss/p/4434901.html</a></li>
<li><a href="http://gfstart.blog.51cto.com/5081306/1414597" target="_blank" rel="external">http://gfstart.blog.51cto.com/5081306/1414597</a></li>
<li><a href="http://yclod.com/kafka-jie-shao/" target="_blank" rel="external">http://yclod.com/kafka-jie-shao/</a></li>
<li><a href="http://www.oschina.net/translate/kafka-design" target="_blank" rel="external">http://www.oschina.net/translate/kafka-design</a></li>
<li><a href="http://blog.csdn.net/beitiandijun/article/details/40582541" target="_blank" rel="external">http://blog.csdn.net/beitiandijun/article/details/40582541</a></li>
<li><a href="http://www.infoq.com/cn/articles/apache-kafka" target="_blank" rel="external">http://www.infoq.com/cn/articles/apache-kafka</a></li>
<li><a href="http://www.doczj.com/doc/20781790b52acfc788ebc955.html" target="_blank" rel="external">http://www.doczj.com/doc/20781790b52acfc788ebc955.html</a></li>
<li><a href="https://baniuyao.gitbooks.io/kafka-0-8-docs-chinese/content/1.1-introduction.html" target="_blank" rel="external">https://baniuyao.gitbooks.io/kafka-0-8-docs-chinese/content/1.1-introduction.html</a></li>
<li><a href="http://www.infoq.com/cn/articles/kafka-analysis-part-1" target="_blank" rel="external">http://www.infoq.com/cn/articles/kafka-analysis-part-1</a></li>
</ul>
]]></content>
    <summary type="html">
    <![CDATA[<!--  -->
<!--  -->
<!--  _ __    __     __      ____    ___     ___   _____   __  __    ___     -->
<!-- /\`'__\/'__`\ /'__`\   /',__\  / _]]>
    </summary>
    
      <category term="Documentation" scheme="http://reasonpun.com/tags/Documentation/"/>
    
      <category term="HDFS" scheme="http://reasonpun.com/tags/HDFS/"/>
    
      <category term="Kafka" scheme="http://reasonpun.com/tags/Kafka/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[linux shell 获得以前日期]]></title>
    <link href="http://reasonpun.com/2015/12/05/linux-shell-%E8%8E%B7%E5%BE%97%E4%BB%A5%E5%89%8D%E6%97%A5%E6%9C%9F/"/>
    <id>http://reasonpun.com/2015/12/05/linux-shell-获得以前日期/</id>
    <published>2015-12-05T07:26:55.000Z</published>
    <updated>2015-12-25T03:29:02.000Z</updated>
    <content type="html"><![CDATA[<!--  -->
<!--  -->
<!--  _ __    __     __      ____    ___     ___   _____   __  __    ___     -->
<!-- /\`'__\/'__`\ /'__`\   /',__\  / __`\ /' _ `\/\ '__`\/\ \/\ \ /' _ `\   -->
<!-- \ \ \//\  __//\ \L\.\_/\__, `\/\ \L\ \/\ \/\ \ \ \L\ \ \ \_\ \/\ \/\ \  -->
<!--  \ \_\\ \____\ \__/.\_\/\____/\ \____/\ \_\ \_\ \ ,__/\ \____/\ \_\ \_\ -->
<!--   \/_/ \/____/\/__/\/_/\/___/  \/___/  \/_/\/_/\ \ \/  \/___/  \/_/\/_/ -->
<!--                                                 \ \_\                   -->
<!--                                                  \/_/                   -->
<!--  -->
<p>在linux shell里，我想获得以前的日期，<br>1、比如，去年的上个月的昨天的日期：（今天是2009年2月2日，也就是2008年1月1日）</p>
<figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">reasonpun<span class="variable">@reasonpun</span><span class="symbol">:~</span><span class="variable">$ </span>logRecordDate=<span class="string">"`date -d "</span>-<span class="number">1</span> year -<span class="number">1</span> month -<span class="number">1</span> day<span class="string">" "</span>+%<span class="constant">Y_%</span>m<span class="constant">_</span>%d<span class="string">"`"</span></span><br><span class="line">reasonpun<span class="variable">@reasonpun</span><span class="symbol">:~</span><span class="variable">$ </span>echo <span class="variable">$logRecordDate</span></span><br><span class="line"><span class="number">2008_01_01</span></span><br><span class="line">reasonpun<span class="variable">@reasonpun</span><span class="symbol">:~</span>$</span><br></pre></td></tr></table></figure>
<p>2、上个月的今天：</p>
<figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">reasonpun@reasonpun:~$ logRecordDate=<span class="string">"`date -d "</span>-<span class="number">1</span> month<span class="string">" "</span>+<span class="decorator">%Y_</span><span class="decorator">%m_</span><span class="decorator">%d</span><span class="string">"`"</span></span><br><span class="line">reasonpun@reasonpun:~$ echo <span class="variable">$logRecordDate</span></span><br><span class="line"><span class="number">2009_01_02</span></span><br></pre></td></tr></table></figure>
<p>3、去年的今天：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">reasonpun@reasonpun:~$ logRecordDate=<span class="string">"`date -d "</span>-<span class="number">1</span> yea<span class="string">r" "</span>+%Y_%m_%d<span class="string">"`"</span></span><br><span class="line">reasonpun@reasonpun:~$ echo $logRecordDate</span><br><span class="line"><span class="number">2008</span>_02_02</span><br></pre></td></tr></table></figure>
<p>4、上个月的昨天：</p>
<figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">reasonpun@reasonpun:~$ logRecordDate=<span class="string">"`date -d "</span>-<span class="number">1</span> month -<span class="number">1</span> day<span class="string">" "</span>+<span class="decorator">%Y_</span><span class="decorator">%m_</span><span class="decorator">%d</span><span class="string">"`"</span></span><br><span class="line">reasonpun@reasonpun:~$ echo <span class="variable">$logRecordDate</span></span><br><span class="line"><span class="number">2009_01_01</span></span><br></pre></td></tr></table></figure>
<p>其他的类推～～呵呵，还是希望大家给测测其他日期会不会出错呵呵。多谢～～～<br>鼓捣之环境：ubuntu8.04</p>
<p>另附上windows下获得前一天的日期：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line">@echo off</span><br><span class="line"><span class="built_in">set</span> td=%date:~<span class="number">2</span>,<span class="number">2</span>%%date:~<span class="number">5</span>,<span class="number">2</span>%%date:~<span class="number">8</span>,<span class="number">2</span>%</span><br><span class="line"><span class="built_in">set</span> dy=%date:~<span class="number">0</span>,<span class="number">4</span>%</span><br><span class="line"><span class="built_in">set</span> dm=%date:~<span class="number">5</span>,<span class="number">2</span>%</span><br><span class="line"><span class="built_in">set</span> dd=%date:~<span class="number">8</span>,<span class="number">2</span>%</span><br><span class="line"><span class="built_in">set</span> da=%date:~<span class="number">8</span>,<span class="number">2</span>%</span><br><span class="line"><span class="keyword">if</span> %dm%%dd%==<span class="number">0101</span> <span class="keyword">goto</span> L01</span><br><span class="line"><span class="keyword">if</span> %dm%%dd%==<span class="number">0201</span> <span class="keyword">goto</span> L02</span><br><span class="line"><span class="keyword">if</span> %dm%%dd%==<span class="number">0301</span> <span class="keyword">goto</span> L07</span><br><span class="line"><span class="keyword">if</span> %dm%%dd%==<span class="number">0401</span> <span class="keyword">goto</span> L02</span><br><span class="line"><span class="keyword">if</span> %dm%%dd%==<span class="number">0501</span> <span class="keyword">goto</span> L04</span><br><span class="line"><span class="keyword">if</span> %dm%%dd%==<span class="number">0601</span> <span class="keyword">goto</span> L02</span><br><span class="line"><span class="keyword">if</span> %dm%%dd%==<span class="number">0701</span> <span class="keyword">goto</span> L04</span><br><span class="line"><span class="keyword">if</span> %dm%%dd%==<span class="number">0801</span> <span class="keyword">goto</span> L02</span><br><span class="line"><span class="keyword">if</span> %dm%%dd%==<span class="number">0901</span> <span class="keyword">goto</span> L02</span><br><span class="line"><span class="keyword">if</span> %dm%%dd%==<span class="number">1001</span> <span class="keyword">goto</span> L05</span><br><span class="line"><span class="keyword">if</span> %dm%%dd%==<span class="number">1101</span> <span class="keyword">goto</span> L03</span><br><span class="line"><span class="keyword">if</span> %dm%%dd%==<span class="number">1201</span> <span class="keyword">goto</span> L06</span><br><span class="line"><span class="keyword">if</span> %dd%==<span class="number">02</span> <span class="keyword">goto</span> L10</span><br><span class="line"><span class="keyword">if</span> %dd%==<span class="number">03</span> <span class="keyword">goto</span> L10</span><br><span class="line"><span class="keyword">if</span> %dd%==<span class="number">04</span> <span class="keyword">goto</span> L10</span><br><span class="line"><span class="keyword">if</span> %dd%==<span class="number">05</span> <span class="keyword">goto</span> L10</span><br><span class="line"><span class="keyword">if</span> %dd%==<span class="number">06</span> <span class="keyword">goto</span> L10</span><br><span class="line"><span class="keyword">if</span> %dd%==<span class="number">07</span> <span class="keyword">goto</span> L10</span><br><span class="line"><span class="keyword">if</span> %dd%==<span class="number">08</span> <span class="keyword">goto</span> L10</span><br><span class="line"><span class="keyword">if</span> %dd%==<span class="number">09</span> <span class="keyword">goto</span> L10</span><br><span class="line"><span class="keyword">if</span> %dd%==<span class="number">10</span> <span class="keyword">goto</span> L11</span><br><span class="line"><span class="built_in">set</span> /A dd=dd-<span class="number">1</span></span><br><span class="line"><span class="built_in">set</span> dt=%dy%-%dm%-%dd%</span><br><span class="line"><span class="keyword">goto</span> END</span><br><span class="line">:L10</span><br><span class="line"><span class="built_in">set</span> /A dd=%dd:~<span class="number">1</span>,<span class="number">1</span>%-<span class="number">1</span></span><br><span class="line"><span class="built_in">set</span> dt=%dy%-%dm%-<span class="number">0</span>%dd%</span><br><span class="line"><span class="keyword">goto</span> END</span><br><span class="line">:L11</span><br><span class="line"><span class="built_in">set</span> dt=%dy%-%dm%-<span class="number">09</span></span><br><span class="line"><span class="keyword">goto</span> END</span><br><span class="line">:L02</span><br><span class="line"><span class="built_in">set</span> /A dm=%dm:~<span class="number">1</span>,<span class="number">1</span>%-<span class="number">1</span></span><br><span class="line"><span class="built_in">set</span> dt=%dy%-<span class="number">0</span>%dm%-<span class="number">31</span></span><br><span class="line"><span class="keyword">goto</span> END</span><br><span class="line">:L04</span><br><span class="line"><span class="built_in">set</span> /A dm=dm-<span class="number">1</span></span><br><span class="line"><span class="built_in">set</span> dt=%dy%-<span class="number">0</span>%dm%-<span class="number">30</span></span><br><span class="line"><span class="keyword">goto</span> END</span><br><span class="line">:L05</span><br><span class="line"><span class="built_in">set</span> dt=%dy%-<span class="number">09</span>-<span class="number">30</span></span><br><span class="line"><span class="keyword">goto</span> END</span><br><span class="line">:L03</span><br><span class="line"><span class="built_in">set</span> dt=%dy%-<span class="number">10</span>-<span class="number">31</span></span><br><span class="line"><span class="keyword">goto</span> END</span><br><span class="line">:L06</span><br><span class="line"><span class="built_in">set</span> dt=%dy%-<span class="number">11</span>-<span class="number">30</span></span><br><span class="line"><span class="keyword">goto</span> END</span><br><span class="line">:L01</span><br><span class="line"><span class="built_in">set</span> /A dy=dy-<span class="number">1</span></span><br><span class="line"><span class="built_in">set</span> dt=%dy%-<span class="number">12</span>-<span class="number">31</span></span><br><span class="line"><span class="keyword">goto</span> END</span><br><span class="line">:L07</span><br><span class="line"><span class="built_in">set</span> /A <span class="string">"dd=dy%%4"</span></span><br><span class="line"><span class="keyword">if</span> not %dd%==<span class="number">0</span> <span class="keyword">goto</span> L08</span><br><span class="line"><span class="built_in">set</span> /A <span class="string">"dd=dy%%100"</span></span><br><span class="line"><span class="keyword">if</span> not %dd%==<span class="number">0</span> <span class="keyword">goto</span> L09</span><br><span class="line"><span class="built_in">set</span> /A <span class="string">"dd=dy%%400"</span></span><br><span class="line"><span class="keyword">if</span> %dd%==<span class="number">0</span> <span class="keyword">goto</span> L09</span><br><span class="line">:L08</span><br><span class="line"><span class="built_in">set</span> dt=%dy%-<span class="number">02</span>-<span class="number">28</span></span><br><span class="line"><span class="keyword">goto</span> END</span><br><span class="line">:L09</span><br><span class="line"><span class="built_in">set</span> dt=%dy%-<span class="number">02</span>-<span class="number">29</span></span><br><span class="line"><span class="keyword">goto</span> END</span><br><span class="line">:END</span><br><span class="line"><span class="built_in">set</span> dateTime=<span class="number">20</span>%dt:~<span class="number">2</span>,<span class="number">2</span>%%dt:~<span class="number">5</span>,<span class="number">2</span>%%dt:~<span class="number">8</span>,<span class="number">2</span>%</span><br></pre></td></tr></table></figure>
]]></content>
    <summary type="html">
    <![CDATA[<!--  -->
<!--  -->
<!--  _ __    __     __      ____    ___     ___   _____   __  __    ___     -->
<!-- /\`'__\/'__`\ /'__`\   /',__\  / _]]>
    </summary>
    
      <category term="Linux" scheme="http://reasonpun.com/tags/Linux/"/>
    
      <category term="Shell" scheme="http://reasonpun.com/tags/Shell/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[spark-in-docker]]></title>
    <link href="http://reasonpun.com/2015/12/04/spark-in-docker/"/>
    <id>http://reasonpun.com/2015/12/04/spark-in-docker/</id>
    <published>2015-12-04T09:48:17.000Z</published>
    <updated>2015-12-25T03:29:21.000Z</updated>
    <content type="html"><![CDATA[<!--  -->
<!--  -->
<!--  _ __    __     __      ____    ___     ___   _____   __  __    ___     -->
<!-- /\`'__\/'__`\ /'__`\   /',__\  / __`\ /' _ `\/\ '__`\/\ \/\ \ /' _ `\   -->
<!-- \ \ \//\  __//\ \L\.\_/\__, `\/\ \L\ \/\ \/\ \ \ \L\ \ \ \_\ \/\ \/\ \  -->
<!--  \ \_\\ \____\ \__/.\_\/\____/\ \____/\ \_\ \_\ \ ,__/\ \____/\ \_\ \_\ -->
<!--   \/_/ \/____/\/__/\/_/\/___/  \/___/  \/_/\/_/\ \ \/  \/___/  \/_/\/_/ -->
<!--                                                 \ \_\                   -->
<!--                                                  \/_/                   -->
<!--  -->
<h4 id="u5B89_u88C5_u597DDocker_u4E4B_u540E"><a href="#u5B89_u88C5_u597DDocker_u4E4B_u540E" class="headerlink" title="安装好Docker之后"></a>安装好Docker之后</h4><h4 id="u5148_u62C9_u53D6_u4E00_u4E2A_u5B98_u65B9_u7684_u57FA_u672C_u955C_u50CFubuntu"><a href="#u5148_u62C9_u53D6_u4E00_u4E2A_u5B98_u65B9_u7684_u57FA_u672C_u955C_u50CFubuntu" class="headerlink" title="先拉取一个官方的基本镜像ubuntu"></a>先拉取一个官方的基本镜像ubuntu</h4><p>docker pull ubuntu</p>
<p>我们将在这个基础镜像上运行容器，将这个容器当成一个普通的ubuntu虚拟机来操作部署spark，最后将配置好的容器commit为一个镜像，之后就可以通过这个镜像运行n个节点来完成集群的搭建</p>
<h4 id="u4E0B_u8F7D_u5B8Cubuntu_u955C_u50CF_u4E4B_u540E_u8FD0_u884C"><a href="#u4E0B_u8F7D_u5B8Cubuntu_u955C_u50CF_u4E4B_u540E_u8FD0_u884C" class="headerlink" title="下载完ubuntu镜像之后运行"></a>下载完ubuntu镜像之后运行</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[reason@localhost ~]$ docker images</span><br><span class="line">REPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZE</span><br><span class="line">mofun/spark         v2<span class="number">.0</span>                b2dacac3e132        About an hour ago   <span class="number">1.508</span> GB</span><br><span class="line">mofun/spark         v1<span class="number">.3</span>                <span class="number">4</span>d2f33ca61ee        <span class="number">18</span> hours ago        <span class="number">1.506</span> GB</span><br><span class="line">ubuntu              latest              <span class="number">0</span>a17decee413        <span class="number">6</span> days ago          <span class="number">188.3</span> MB</span><br><span class="line">docker/whalesay     latest              ded5e192a685        <span class="number">4</span> months ago        <span class="number">247</span> MB</span><br><span class="line">[reason@localhost ~]$</span><br></pre></td></tr></table></figure>
<h4 id="u8FD0_u884Cubuntu_u5BB9_u5668"><a href="#u8FD0_u884Cubuntu_u5BB9_u5668" class="headerlink" title="运行ubuntu容器"></a>运行ubuntu容器</h4><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[reason<span class="variable">@localhost</span> ~]<span class="variable">$ </span>docker run -v /home/docker/software/<span class="symbol">:/software</span> -it ubuntu</span><br><span class="line">root<span class="variable">@f4c0a9d42852</span><span class="symbol">:/</span><span class="comment">#</span></span><br></pre></td></tr></table></figure>
<h4 id="u5728_u5BB9_u5668_u4E2D_u5B89_u88C5ssh"><a href="#u5728_u5BB9_u5668_u4E2D_u5B89_u88C5ssh" class="headerlink" title="在容器中安装ssh"></a>在容器中安装ssh</h4><figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">[reason<span class="comment">@localhost ~]$ docker run -v /home/docker/software/:/software -it ubuntu</span></span><br><span class="line">root<span class="comment">@3970c1e5466e:/# apt-get install ssh</span></span><br><span class="line">Reading package lists... Done</span><br><span class="line">Building dependency tree</span><br><span class="line">Reading state information... Done</span><br><span class="line"></span><br><span class="line"><span class="comment"># ssh默认配置root无法登陆</span></span><br><span class="line">root<span class="comment">@3970c1e5466e:~/.ssh# vim /etc/ssh/sshd_config</span></span><br><span class="line">root<span class="comment">@3970c1e5466e:~/.ssh#</span></span><br><span class="line"><span class="comment"># 将 /etc/ssh/sshd_config中PermitRootLogin no/without_passwd 改为yes</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成访问密钥</span></span><br><span class="line">root<span class="comment">@3970c1e5466e:~# ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa</span></span><br><span class="line">Generating public/private rsa key pair.</span><br><span class="line">Created directory '/root/.ssh'.</span><br><span class="line">Your identification has been saved in /root/.ssh/id_rsa.</span><br><span class="line">Your public key has been saved in /root/.ssh/id_rsa.pub.</span><br><span class="line">The key fingerprint is:</span><br><span class="line">b5:d1:e1:dd:98:7d:be:cc:55:69:c6:e7:67:80:d8:d3 root<span class="comment">@3970c1e5466e</span></span><br><span class="line">The key's randomart image is:</span><br><span class="line">+--[ RSA 2048]----+</span><br><span class="line">|<span class="string">            .    </span>|</span><br><span class="line">|<span class="string">           = =.=.</span>|</span><br><span class="line">|<span class="string">          + * E=*</span>|</span><br><span class="line">|<span class="string">         . o .o++</span>|</span><br><span class="line">|<span class="string">        S .     *</span>|</span><br><span class="line">|<span class="string">              o.+</span>|</span><br><span class="line">|<span class="string">               + </span>|</span><br><span class="line">|<span class="string">                 </span>|</span><br><span class="line">|<span class="string">                 </span>|</span><br><span class="line">+-----------------+</span><br><span class="line">root<span class="comment">@3970c1e5466e:~#</span></span><br><span class="line">root<span class="comment">@3970c1e5466e:~# cd ~/.ssh/</span></span><br><span class="line">root<span class="comment">@3970c1e5466e:~/.ssh# cat id_rsa.pub &gt;&gt; authorized_keys</span></span><br><span class="line">root<span class="comment">@3970c1e5466e:~/.ssh#</span></span><br><span class="line">root<span class="comment">@3970c1e5466e:~/.ssh# vim ~/.bashrc</span></span><br><span class="line"><span class="comment">#加入/usr/sbin/sshd</span></span><br><span class="line"><span class="comment">#如果在启动容器的时候还是无法启动ssh的话，</span></span><br><span class="line"><span class="comment"># 在/etc/rc.local文件中也加入</span></span><br><span class="line">root<span class="comment">@3970c1e5466e:~/.ssh# vim /etc/rc.local</span></span><br><span class="line"><span class="comment">#加入</span></span><br><span class="line">root<span class="comment">@3970c1e5466e:~/.ssh# /usr/sbin/sshd</span></span><br><span class="line">Missing privilege separation directory: /var/run/sshd</span><br><span class="line">root<span class="comment">@3970c1e5466e:~/.ssh# mkdir /var/run/sshd</span></span><br><span class="line">root<span class="comment">@3970c1e5466e:~/.ssh# /usr/sbin/sshd</span></span><br><span class="line">root<span class="comment">@3970c1e5466e:~/.ssh#</span></span><br><span class="line"><span class="comment"># 开启ssh服务后验证是否可以使用，打印出当前时间</span></span><br><span class="line">root<span class="comment">@3970c1e5466e:~/.ssh# ssh localhost date</span></span><br><span class="line">The authenticity of host 'localhost (::1)' can't be established.</span><br><span class="line">ECDSA key fingerprint is ab:43:27:e6:1c:44:be:2c:f1:17:27:90:6d:2c:68:86.</span><br><span class="line">Are you sure you want to continue connecting (yes/no)? yes</span><br><span class="line">Warning: Permanently added 'localhost' (ECDSA) to the list of known hosts.</span><br><span class="line">Mon Oct 19 05:57:44 UTC 2015</span><br><span class="line">root<span class="comment">@3970c1e5466e:~/.ssh#</span></span><br><span class="line"><span class="comment"># ssh安装完毕</span></span><br></pre></td></tr></table></figure>
<h4 id="u5B89_u88C5JDK"><a href="#u5B89_u88C5JDK" class="headerlink" title="安装JDK"></a>安装JDK</h4><p>可以使用apt-get方式直接下载安装jdk（不推荐，下载速度慢，有可能还会失败）<br>这里选择从网上下载完jdk-8u60-linux-xx.bin之后<br>将其传到Ubuntu宿主机中，在运行容器的时候使用-v参数将宿主机上的目录映射到容器中，这样在容器中就可以访问到宿主机中的文件了</p>
<figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">如果提示不能安装.bin文件，使用以下命令即可解决</span><br><span class="line">root<span class="variable">@3970c1e5466e</span><span class="symbol">:~/</span>.ssh<span class="comment"># apt-get update</span></span><br><span class="line">root<span class="variable">@3970c1e5466e</span><span class="symbol">:~/</span>.ssh<span class="comment"># apt-getinstall g++-multilib</span></span><br></pre></td></tr></table></figure>
<h4 id="u5B89_u88C5Zookeeper"><a href="#u5B89_u88C5Zookeeper" class="headerlink" title="安装Zookeeper"></a>安装Zookeeper</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">将下载好的zookeeper-<span class="number">3.4</span><span class="number">.6</span>.tar.gz上传</span><br><span class="line">root@<span class="number">3970</span>c1e5466e:~/.ssh<span class="preprocessor"># mv /software/zookeeper-<span class="number">3.4</span><span class="number">.6</span>.tar.gz /usr/local/zookeeper-<span class="number">3.4</span><span class="number">.6</span></span></span><br><span class="line">root@<span class="number">3970</span>c1e5466e:~/.ssh<span class="preprocessor"># tar -zxvf zookeeper-<span class="number">3.4</span><span class="number">.6</span>.tar.gz</span></span><br><span class="line">root@<span class="number">3970</span>c1e5466e:~/.ssh<span class="preprocessor"># cd /usr/local/zookeeper-<span class="number">3.4</span><span class="number">.6</span>/conf/</span></span><br><span class="line">root@<span class="number">3970</span>c1e5466e:~/.ssh<span class="preprocessor"># cp zoo_sample.cfgzoo.cfgvim zoo.cfg</span></span><br><span class="line">root@<span class="number">3970</span>c1e5466e:~/.ssh<span class="preprocessor"># vim zoo.cfg</span></span><br><span class="line"><span class="preprocessor">#修改：dataDir=/root/zookeeper/tmp</span></span><br><span class="line"><span class="preprocessor">#在最后添加：</span></span><br><span class="line">server<span class="number">.1</span>=cloud4:<span class="number">2888</span>:<span class="number">3888</span></span><br><span class="line">server<span class="number">.2</span>=cloud5:<span class="number">2888</span>:<span class="number">3888</span></span><br><span class="line">server<span class="number">.3</span>=cloud6:<span class="number">2888</span>:<span class="number">3888</span></span><br><span class="line"></span><br><span class="line"><span class="preprocessor">#保存退出，然后创建一个tmp文件夹</span></span><br><span class="line">root@<span class="number">3970</span>c1e5466e:~/.ssh<span class="preprocessor"># mkdir /data/zookeeper/tmp</span></span><br><span class="line"><span class="preprocessor">#再创建一个空文件</span></span><br><span class="line">root@<span class="number">3970</span>c1e5466e:~/.ssh<span class="preprocessor"># touch /data/zookeeper/tmp/myid</span></span><br><span class="line"><span class="preprocessor">#最后向该文件写入ID</span></span><br><span class="line">root@<span class="number">3970</span>c1e5466e:~/.ssh<span class="preprocessor"># echo <span class="number">1</span>&gt; /data/zookeeper/tmp/myid</span></span><br></pre></td></tr></table></figure>
<h4 id="u5B89_u88C5Hadoop"><a href="#u5B89_u88C5Hadoop" class="headerlink" title="安装Hadoop"></a>安装Hadoop</h4><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">root<span class="variable">@3970c1e5466e</span><span class="symbol">:~/</span>.ssh<span class="comment"># mv /software/hadoop-2.6.1.tar.gz /usr/local/</span></span><br><span class="line">root<span class="variable">@3970c1e5466e</span><span class="symbol">:~/</span>.ssh<span class="comment"># tar -zxvf hadoop-2.2.0-64bit.tar.gz</span></span><br><span class="line">root<span class="variable">@3970c1e5466e</span><span class="symbol">:~/</span>.ssh<span class="comment"># cd /usr/local/hadoop/etc/hadoop</span></span><br></pre></td></tr></table></figure>
<p>更改hadoop-env.sh</p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">root<span class="variable">@3970c1e5466e</span><span class="symbol">:/data/test</span><span class="comment"># cd /usr/local/hadoop-2.6.1/</span></span><br><span class="line">root<span class="variable">@3970c1e5466e</span><span class="symbol">:/usr/local/hadoop-</span><span class="number">2.6</span>.<span class="number">1</span><span class="comment"># ls</span></span><br><span class="line"><span class="constant">LICENSE</span>.txt  <span class="constant">NOTICE</span>.txt  <span class="constant">README</span>.txt  bin  etc  <span class="keyword">include</span>  lib  libexec  logs  sbin  share</span><br><span class="line">root<span class="variable">@3970c1e5466e</span><span class="symbol">:/usr/local/hadoop-</span><span class="number">2.6</span>.<span class="number">1</span><span class="comment"># cd etc/hadoop/</span></span><br><span class="line">root<span class="variable">@3970c1e5466e</span><span class="symbol">:/usr/local/hadoop-</span><span class="number">2.6</span>.<span class="number">1</span>/etc/hadoop<span class="comment"># vim hadoop-env.sh</span></span><br><span class="line"><span class="comment">#加入java环境变量</span></span><br><span class="line">export <span class="constant">JAVA_HOME</span>=<span class="regexp">/usr/local</span><span class="regexp">/jdk1.8.0_60</span></span><br></pre></td></tr></table></figure>
<p>修改core-site.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">root@3970c1e5466e:/usr/local/hadoop-2.6.1/etc/hadoop# vim core-site.xml</span><br><span class="line"></span><br><span class="line"><span class="pi">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span></span><br><span class="line"><span class="pi">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">value</span>&gt;</span>hdfs://ns1<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">value</span>&gt;</span>/data/hadoop/tmp<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">name</span>&gt;</span>ha.zookeeper.quorum<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">value</span>&gt;</span>cloud4:2181,cloud5:2182,cloud6:2183<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>修改hdfs-site.xml, mapred-site.xml, yarn-site.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line">root@3970c1e5466e:/usr/local/hadoop-2.6.1/etc/hadoop# vim hdfs-site.xml</span><br><span class="line"><span class="pi">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span></span><br><span class="line"><span class="pi">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="title">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.nameservices<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">value</span>&gt;</span>ns1<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.ha.namenodes.ns1<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">value</span>&gt;</span>nn1,nn2<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.namenode.rpc-address.ns1.nn1<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">value</span>&gt;</span>cloud1:9000<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.namenode.http-address.ns1.nn1<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">value</span>&gt;</span>cloud1:50070<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.namenode.rpc-address.ns1.nn2<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">value</span>&gt;</span>cloud2:9000<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.namenode.http-address.ns1.nn2<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">value</span>&gt;</span>cloud2:50070<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.namenode.shared.edits.dir<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">value</span>&gt;</span>qjournal://cloud4:8485;cloud5:8485;cloud6:8485/ns1<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.journalnode.edits.dir<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">value</span>&gt;</span>/data/hadoop/journal<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.ha.automatic-failover.enabled<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">value</span>&gt;</span>true<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.client.failover.proxy.provider.ns1<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">value</span>&gt;</span></span><br><span class="line">org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</span><br><span class="line"><span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.ha.fencing.methods<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">value</span>&gt;</span></span><br><span class="line">sshfence</span><br><span class="line"><span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.ha.fencing.ssh.private-key-files<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">value</span>&gt;</span>/root/.ssh/id_rsa<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.ha.fencing.ssh.connect-timeout<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">value</span>&gt;</span>30000<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">value</span>&gt;</span>file:///data/hadoop/workspace/hdfs/name<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">value</span>&gt;</span>file:///data/hadoop/workspace/hdfs/data<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="title">value</span>&gt;</span>2<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">root@3970c1e5466e:/usr/local/hadoop-2.6.1/etc/hadoop# mv mapred-site.xml.template mapred-site.xml</span><br><span class="line">root@3970c1e5466e:/usr/local/hadoop-2.6.1/etc/hadoop# vim mapred-site.xml</span><br><span class="line"><span class="pi">&lt;?xml version="1.0"?&gt;</span></span><br><span class="line"><span class="pi">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">root@3970c1e5466e:/usr/local/hadoop-2.6.1/etc/hadoop# vim yarn-site.xml</span><br><span class="line"><span class="pi">&lt;?xml version="1.0"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">value</span>&gt;</span>cloud3<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="title">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">root<span class="variable">@3970c1e5466e</span><span class="symbol">:/usr/local/hadoop-</span><span class="number">2.6</span>.<span class="number">1</span>/etc/hadoop<span class="comment"># vim slaves</span></span><br><span class="line">cloud1</span><br><span class="line">cloud2</span><br><span class="line">cloud3</span><br><span class="line">cloud4</span><br><span class="line">cloud5</span><br><span class="line">cloud6</span><br></pre></td></tr></table></figure>
<h4 id="u5B89_u88C5Spark"><a href="#u5B89_u88C5Spark" class="headerlink" title="安装Spark"></a>安装Spark</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">root@<span class="number">3970</span>c1e5466e:~/.ssh<span class="preprocessor"># mv /software/scala-<span class="number">2.11</span><span class="number">.7</span> /usr/local/</span></span><br><span class="line">root@<span class="number">3970</span>c1e5466e:~/.ssh<span class="preprocessor"># tar -zxvf scala-<span class="number">2.11</span><span class="number">.7</span>.tgz</span></span><br><span class="line">root@<span class="number">3970</span>c1e5466e:~/.ssh<span class="preprocessor"># vim ~/.bashrc</span></span><br><span class="line"><span class="keyword">export</span> JAVA_HOME=/usr/local/jdk1<span class="number">.8</span><span class="number">.0</span>_60</span><br><span class="line"><span class="keyword">export</span> HADOOP_HOME=/usr/local/hadoop-<span class="number">2.6</span><span class="number">.1</span></span><br><span class="line"><span class="keyword">export</span> SCALA_HOME=/usr/local/scala-<span class="number">2.11</span><span class="number">.7</span></span><br><span class="line"><span class="keyword">export</span> SPARK_HOME=/usr/local/spark-<span class="number">1.5</span><span class="number">.1</span>-bin-hadoop2<span class="number">.6</span></span><br><span class="line"><span class="keyword">export</span> PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$SCALA_HOME/bin:$SPARK_HOME/bin</span><br><span class="line"></span><br><span class="line">root@<span class="number">3970</span>c1e5466e:~/.ssh<span class="preprocessor"># mv /software/spark-<span class="number">1.5</span><span class="number">.1</span>-bin-hadoop2<span class="number">.6</span>.tgz /usr/local/</span></span><br><span class="line">root@<span class="number">3970</span>c1e5466e:~/.ssh<span class="preprocessor"># tar -zxvf spark-<span class="number">1.5</span><span class="number">.1</span>-bin-hadoop2<span class="number">.6</span></span></span><br></pre></td></tr></table></figure>
<p>编辑配置文件</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">root@<span class="number">3970</span>c1e5466e:~/.ssh<span class="preprocessor"># cd /usr/local/spark-<span class="number">1.5</span><span class="number">.1</span>-bin-hadoop2<span class="number">.6</span>/</span></span><br><span class="line">root@<span class="number">3970</span>c1e5466e:/usr/local/spark-<span class="number">1.5</span><span class="number">.1</span>-bin-hadoop2<span class="number">.6</span><span class="preprocessor"># cd conf/</span></span><br><span class="line">root@<span class="number">3970</span>c1e5466e:/usr/local/spark-<span class="number">1.5</span><span class="number">.1</span>-bin-hadoop2<span class="number">.6</span><span class="preprocessor"># vim slaves</span></span><br><span class="line">cloud1</span><br><span class="line">cloud2</span><br><span class="line">cloud3</span><br><span class="line">cloud4</span><br><span class="line">cloud5</span><br><span class="line">cloud6</span><br><span class="line">root@<span class="number">3970</span>c1e5466e:/usr/local/spark-<span class="number">1.5</span><span class="number">.1</span>-bin-hadoop2<span class="number">.6</span><span class="preprocessor"># mv spark-env.sh.template spark-env.sh</span></span><br><span class="line">root@<span class="number">3970</span>c1e5466e:/usr/local/spark-<span class="number">1.5</span><span class="number">.1</span>-bin-hadoop2<span class="number">.6</span><span class="preprocessor"># vim ~/spark/conf/spark-env.sh</span></span><br><span class="line"><span class="keyword">export</span> SPARK_MASTER_IP=cloud1</span><br><span class="line"><span class="keyword">export</span> SPARK_WORKER_MEMORY=<span class="number">128</span>m</span><br><span class="line"><span class="keyword">export</span> JAVA_HOME=/usr/local/jdk1<span class="number">.8</span><span class="number">.0</span>_60</span><br><span class="line"><span class="keyword">export</span> SCALA_HOME=/usr/local/scala-<span class="number">2.11</span><span class="number">.7</span></span><br><span class="line"><span class="keyword">export</span> SPARK_HOME=/usr/local/spark-<span class="number">1.5</span><span class="number">.1</span>-bin-hadoop2<span class="number">.6</span></span><br><span class="line"><span class="keyword">export</span> HADOOP_CONF_DIR=/usr/local/hadoop-<span class="number">2.6</span><span class="number">.1</span>/etc/hadoop</span><br><span class="line"><span class="keyword">export</span> SPARK_LIBRARY_PATH=$$SPARK_HOME/lib</span><br><span class="line"><span class="keyword">export</span> SCALA_LIBRARY_PATH=$SPARK_LIBRARY_PATH</span><br><span class="line"><span class="keyword">export</span> SPARK_WORKER_CORES=<span class="number">1</span></span><br><span class="line"><span class="keyword">export</span> SPARK_WORKER_INSTANCES=<span class="number">1</span></span><br><span class="line"><span class="keyword">export</span> SPARK_MASTER_PORT=<span class="number">7077</span></span><br></pre></td></tr></table></figure>
<p>此时配置已经基本完成，所以需要<strong>保存镜像</strong></p>
<figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[reason<span class="variable">@localhost</span> ~]<span class="variable">$ </span>sudo docker commit -m <span class="string">"mofun spark first commit"</span> -a <span class="string">"reason"</span> cloud1 mofun/<span class="symbol">spark:</span>v1.<span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>查看执行过的镜像</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[reason@localhost ~]$ sudo docker ps -a</span><br><span class="line">CONTAINER ID        IMAGE                 COMMAND                  CREATED             STATUS                         PORTS               NAMES</span><br><span class="line">d4e581ba6af8        mofun/spark:v1<span class="number">.0</span>   <span class="string">"/bin/bash"</span>              <span class="number">11</span> <span class="function">seconds ago      <span class="title">Exited</span> <span class="params">(<span class="number">0</span>)</span> 7 seconds ago                           hungry_pasteur</span></span><br></pre></td></tr></table></figure>
<p>启动容器</p>
<figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 后台模式运行</span></span><br><span class="line">[reason@localhost ~]$ docker <span class="command">run</span> -d <span class="comment">--name cloud2 -h cloud2 -it mofun/spark:v2.0 /bin/bash</span></span><br><span class="line"></span><br><span class="line">需要使用刚才制作的镜像启动<span class="number">6</span>个容器</span><br><span class="line">[reason@localhost ~]$ docker <span class="command">run</span> <span class="comment">--name cloud1 -h cloud1 -it mofun/spark:v2.0 /bin/bash</span></span><br><span class="line">[reason@localhost ~]$ docker <span class="command">run</span> <span class="comment">--name cloud2 -h cloud2 -it mofun/spark:v2.0 /bin/bash</span></span><br><span class="line">[reason@localhost ~]$ docker <span class="command">run</span> <span class="comment">--name cloud3 -h cloud3 -it mofun/spark:v2.0 /bin/bash</span></span><br><span class="line">[reason@localhost ~]$ docker <span class="command">run</span> <span class="comment">--name cloud4 -h cloud4 -it mofun/spark:v2.0 /bin/bash</span></span><br><span class="line">[reason@localhost ~]$ docker <span class="command">run</span> <span class="comment">--name cloud5 -h cloud5 -it mofun/spark:v2.0 /bin/bash</span></span><br><span class="line">[reason@localhost ~]$ docker <span class="command">run</span> <span class="comment">--name cloud6 -h cloud6 -it mofun/spark:v2.0 /bin/bash</span></span><br></pre></td></tr></table></figure>
<p>做最后的修改</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor">#在cloud5~cloud6中分别手动修改myid</span></span><br><span class="line">root@cloud5:~<span class="preprocessor"># echo <span class="number">2</span> &gt;  /data/zookeeper/myid</span></span><br><span class="line">root@cloud5:~<span class="preprocessor"># echo <span class="number">2</span> &gt; /usr/local/zookeeper-<span class="number">3.4</span><span class="number">.6</span>/tmp/myid</span></span><br><span class="line">root@cloud6:~<span class="preprocessor"># echo <span class="number">3</span> &gt;  /data/zookeeper/myid</span></span><br><span class="line">root@cloud6:~<span class="preprocessor"># echo <span class="number">3</span> &gt; /usr/local/zookeeper-<span class="number">3.4</span><span class="number">.6</span>/tmp/myid</span></span><br></pre></td></tr></table></figure>
<p>启动zookeeper集群</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor"># 启动zookeeper集群（分别在cloud4、cloud5、cloud6上启动zk）</span></span><br><span class="line"><span class="preprocessor"># 全部节点启动后，再执行zkServer.sh status</span></span><br><span class="line"><span class="preprocessor"># 返回正常</span></span><br><span class="line"><span class="preprocessor"># root@cloud6:/usr/local/zookeeper-<span class="number">3.4</span><span class="number">.6</span>/bin# ./zkServer.sh status</span></span><br><span class="line"><span class="preprocessor"># JMX enabled by default</span></span><br><span class="line"><span class="preprocessor"># Using config: /usr/local/zookeeper-<span class="number">3.4</span><span class="number">.6</span>/bin/../conf/zoo.cfg</span></span><br><span class="line"><span class="preprocessor"># Mode: follower</span></span><br><span class="line">root@cloud5:~<span class="preprocessor"># /usr/local/zookeeper-<span class="number">3.4</span><span class="number">.6</span>/bin/zkServer.sh start</span></span><br><span class="line"><span class="preprocessor"># 当<span class="number">3</span>个节点服务正常启动后</span></span><br><span class="line"><span class="preprocessor"># 使用status查看是否启动</span></span><br><span class="line">root@cloud5:~<span class="preprocessor"># /usr/local/zookeeper-<span class="number">3.4</span><span class="number">.6</span>/bin/zkServer.sh status</span></span><br><span class="line">JMX enabled by <span class="keyword">default</span></span><br><span class="line">Using config: /usr/local/zookeeper-<span class="number">3.4</span><span class="number">.6</span>/bin/../conf/zoo.cfg</span><br><span class="line">Mode: follower</span><br><span class="line">root@cloud5:~<span class="preprocessor">#</span></span><br></pre></td></tr></table></figure>
<p>进入cloud1，开启hadoop和spark服务</p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动journalnode（在cloud1上启动所有journalnode，注意：是调用的hadoop-daemons.sh这个脚本，注意是复数s的那个脚本）</span></span><br><span class="line"><span class="comment"># 运行jps命令检验，cloud4、cloud5、cloud6上多了JournalNode进程</span></span><br><span class="line">root<span class="variable">@cloud1</span><span class="symbol">:/usr/local/hadoop-</span><span class="number">2.6</span>.<span class="number">1</span>/sbin<span class="comment"># pwd</span></span><br><span class="line">/usr/local/hadoop-<span class="number">2.6</span>.<span class="number">1</span>/sbin</span><br><span class="line">root<span class="variable">@cloud1</span><span class="symbol">:/usr/local/hadoop-</span><span class="number">2.6</span>.<span class="number">1</span>/sbin<span class="comment">#</span></span><br><span class="line">root<span class="variable">@cloud1</span><span class="symbol">:/usr/local/hadoop-</span><span class="number">2.6</span>.<span class="number">1</span>/sbin<span class="comment"># hadoop-daemons.sh start journalnode</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 格式化ZK(在cloud1上执行即可，在bin目录下)</span></span><br><span class="line">root<span class="variable">@cloud1</span><span class="symbol">:/usr/local/hadoop-</span><span class="number">2.6</span>.<span class="number">1</span>/bin<span class="comment"># hdfs zkfc -formatZK</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 进入节点cloud4，查看zookeeper信息</span></span><br><span class="line">root<span class="variable">@cloud4</span><span class="symbol">:/usr/local/zookeeper-</span><span class="number">3.4</span>.<span class="number">6</span>/bin<span class="comment"># pwd</span></span><br><span class="line">/usr/local/zookeeper-<span class="number">3.4</span>.<span class="number">6</span>/bin</span><br><span class="line">root<span class="variable">@cloud4</span><span class="symbol">:/usr/local/zookeeper-</span><span class="number">3.4</span>.<span class="number">6</span>/bin<span class="comment"># ls</span></span><br><span class="line"><span class="constant">README</span>.txt  zkCleanup.sh  zkCli.cmd  zkCli.sh  zkEnv.cmd  zkEnv.sh  zkServer.cmd  zkServer.sh  zookeeper.out</span><br><span class="line">root<span class="variable">@cloud4</span><span class="symbol">:/usr/local/zookeeper-</span><span class="number">3.4</span>.<span class="number">6</span>/bin<span class="comment">#</span></span><br><span class="line">root<span class="variable">@cloud4</span><span class="symbol">:/usr/local/zookeeper-</span><span class="number">3.4</span>.<span class="number">6</span>/bin<span class="comment"># ./zkCli.sh</span></span><br><span class="line"><span class="constant">Connecting</span> to <span class="symbol">localhost:</span><span class="number">2181</span></span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span> 09<span class="symbol">:</span><span class="number">34</span><span class="symbol">:</span><span class="number">34</span>,<span class="number">485</span> [<span class="symbol">myid:</span>] - <span class="constant">INFO</span>  [<span class="symbol">main:</span><span class="constant">Environment</span><span class="variable">@100</span>] - <span class="constant">Client</span> <span class="symbol">environment:</span>zookeeper.version=<span class="number">3.4</span>.<span class="number">6</span>-<span class="number">1569965</span>, built on <span class="number">02</span>/<span class="number">20</span>/<span class="number">2014</span> 09<span class="symbol">:</span>09 <span class="constant">GMT</span></span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span> 09<span class="symbol">:</span><span class="number">34</span><span class="symbol">:</span><span class="number">34</span>,<span class="number">487</span> [<span class="symbol">myid:</span>] - <span class="constant">INFO</span>  [<span class="symbol">main:</span><span class="constant">Environment</span><span class="variable">@100</span>] - <span class="constant">Client</span> <span class="symbol">environment:</span>host.name=cloud4</span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span> 09<span class="symbol">:</span><span class="number">34</span><span class="symbol">:</span><span class="number">34</span>,<span class="number">487</span> [<span class="symbol">myid:</span>] - <span class="constant">INFO</span>  [<span class="symbol">main:</span><span class="constant">Environment</span><span class="variable">@100</span>] - <span class="constant">Client</span> <span class="symbol">environment:</span>java.version=<span class="number">1.8</span>.<span class="number">0_</span>...</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">[<span class="symbol">zk:</span> <span class="symbol">localhost:</span><span class="number">2181</span>(<span class="constant">CONNECTED</span>) <span class="number">2</span>] ls /hadoop-ha</span><br><span class="line">[ns1]</span><br><span class="line">[<span class="symbol">zk:</span> <span class="symbol">localhost:</span><span class="number">2181</span>(<span class="constant">CONNECTED</span>) <span class="number">3</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 格式化HDFS(在bin目录下),在cloud1上执行命令:</span></span><br><span class="line">root<span class="variable">@cloud1</span><span class="symbol">:/usr/local/hadoop-</span><span class="number">2.6</span>.<span class="number">1</span>/bin<span class="comment"># hdfs namenode -format</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 首先启动active节点，执行如下命令(在cloud1上执行)</span></span><br><span class="line">root<span class="variable">@cloud1</span><span class="symbol">:/usr/local/hadoop-</span><span class="number">2.6</span>.<span class="number">1</span>/sbin<span class="comment"># hadoop-daemon.sh start namenode  </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 进入cloud2，需要启动standy模式</span></span><br><span class="line">root<span class="variable">@cloud2</span><span class="symbol">:/usr/local/hadoop-</span><span class="number">2.6</span>.<span class="number">1</span>/bin<span class="comment"># pwd</span></span><br><span class="line">/usr/local/hadoop-<span class="number">2.6</span>.<span class="number">1</span>/bin</span><br><span class="line">root<span class="variable">@cloud2</span><span class="symbol">:/usr/local/hadoop-</span><span class="number">2.6</span>.<span class="number">1</span>/bin<span class="comment"># ./hdfs namenode -bootstrapStandby</span></span><br><span class="line">root<span class="variable">@cloud2</span><span class="symbol">:/usr/local/hadoop-</span><span class="number">2.6</span>.<span class="number">1</span>/sbin<span class="comment"># pwd</span></span><br><span class="line">/usr/local/hadoop-<span class="number">2.6</span>.<span class="number">1</span>/sbin</span><br><span class="line"><span class="comment"># 这条命令可以等到hadoop-daemons.sh start zkfc 成功以后执行</span></span><br><span class="line"><span class="comment"># 貌似是zkfc的启动慢导致standby模式启动出错？</span></span><br><span class="line"></span><br><span class="line">root<span class="variable">@cloud1</span><span class="symbol">:/usr/local/spark-</span><span class="number">1.5</span>.<span class="number">1</span>-bin-hadoop2.<span class="number">6</span>/bin<span class="comment"># jps</span></span><br><span class="line"><span class="number">1522</span> <span class="constant">NameNode</span></span><br><span class="line"><span class="number">2546</span> <span class="constant">Jps</span></span><br><span class="line"><span class="number">1859</span> <span class="constant">DFSZKFailoverController</span></span><br><span class="line"><span class="number">1109</span> <span class="constant">Worker</span></span><br><span class="line"><span class="number">104</span> <span class="constant">JournalNode</span></span><br><span class="line"><span class="number">426</span> <span class="constant">DataNode</span></span><br><span class="line"><span class="number">558</span> <span class="constant">NodeManager</span></span><br><span class="line"><span class="number">943</span> <span class="constant">Master</span></span><br><span class="line"><span class="comment"># 仔细观察DFSZKFailoverController 这个进程的存在情况</span></span><br><span class="line"></span><br><span class="line">root<span class="variable">@cloud2</span><span class="symbol">:/usr/local/hadoop-</span><span class="number">2.6</span>.<span class="number">1</span>/sbin<span class="comment"># ./hadoop-daemon.sh start namenode</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 重新进入cloud1 ，启动datanode</span></span><br><span class="line">root<span class="variable">@cloud4</span><span class="symbol">:/usr/local/hadoop-</span><span class="number">2.6</span>.<span class="number">1</span>/sbin<span class="comment"># pwd</span></span><br><span class="line">/usr/local/hadoop-<span class="number">2.6</span>.<span class="number">1</span>/sbin</span><br><span class="line">root<span class="variable">@cloud4</span><span class="symbol">:/usr/local/hadoop-</span><span class="number">2.6</span>.<span class="number">1</span>/sbin<span class="comment"># ./hadoop-daemons.sh start datanode</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在cloud3上执行start-yarn.sh</span></span><br><span class="line">root<span class="variable">@cloud3</span><span class="symbol">:/usr/local/hadoop-</span><span class="number">2.6</span>.<span class="number">1</span>/sbin<span class="comment"># start-yarn.sh</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动ZKFC</span></span><br><span class="line">root<span class="variable">@cloud1</span><span class="symbol">:/usr/local/hadoop-</span><span class="number">2.6</span>.<span class="number">1</span>/sbin<span class="comment"># ./hadoop-daemons.sh start zkfc</span></span><br><span class="line"><span class="comment"># 启动spark集群</span></span><br><span class="line">root<span class="variable">@cloud1</span><span class="symbol">:/usr/local/hadoop-</span><span class="number">2.6</span>.<span class="number">1</span>/sbin<span class="comment"># cd /usr/local/spark-1.5.1-bin-hadoop2.6/sbin/</span></span><br><span class="line">root<span class="variable">@cloud1</span><span class="symbol">:/usr/local/spark-</span><span class="number">1.5</span>.<span class="number">1</span>-bin-hadoop2.<span class="number">6</span>/sbin<span class="comment"># start-all.sh</span></span><br></pre></td></tr></table></figure>
<p>此时可以通过CURL访问服务了，如果宿主机中的hosts文件没有配置docker容器的主机名和IP地址映射关系的话要换成用IP访问</p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[reason<span class="annotation">@localhost</span> ~]$ curl <span class="string">http:</span><span class="comment">//172.17.0.56:50070</span></span><br><span class="line">[reason<span class="annotation">@localhost</span> ~]$</span><br></pre></td></tr></table></figure>
<h4 id="u5176_u4ED6"><a href="#u5176_u4ED6" class="headerlink" title="其他"></a>其他</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor"># 删除镜像</span></span><br><span class="line">[reason@localhost ~]$ docker images</span><br><span class="line">REPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZE</span><br><span class="line">mofun/spark         v2<span class="number">.0</span>                b2dacac3e132        <span class="number">3</span> hours ago         <span class="number">1.508</span> GB</span><br><span class="line">mofun/spark         v1<span class="number">.3</span>                <span class="number">4</span>d2f33ca61ee        <span class="number">21</span> hours ago        <span class="number">1.506</span> GB</span><br><span class="line">ubuntu              latest              <span class="number">0</span>a17decee413        <span class="number">6</span> days ago          <span class="number">188.3</span> MB</span><br><span class="line">docker/whalesay     latest              ded5e192a685        <span class="number">4</span> months ago        <span class="number">247</span> MB</span><br><span class="line">[reason@localhost ~]$ sudo docker rmi <span class="number">4</span>d2f</span><br></pre></td></tr></table></figure>
<figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 删除容器</span></span><br><span class="line">[reason<span class="variable">@localhost</span> ~]<span class="variable">$ </span>docker ps -a</span><br><span class="line"><span class="constant">CONTAINER ID </span>       <span class="constant">IMAGE </span>              <span class="constant">COMMAND </span>            <span class="constant">CREATED </span>            <span class="constant">STATUS </span>                       <span class="constant">PORTS </span>              <span class="constant">NAMES</span></span><br><span class="line"><span class="number">8258875263</span>be        ubuntu              <span class="string">"/bin/bash"</span>         <span class="number">3</span> minutes ago       <span class="constant">Exited </span>(<span class="number">127</span>) <span class="number">3</span> minutes ago                        mad_mestorf</span><br><span class="line">[reason<span class="variable">@localhost</span> ~]<span class="variable">$ </span>sudo docker rm <span class="number">8258875263</span>be</span><br><span class="line"><span class="number">8258875263</span>be</span><br><span class="line">[reason<span class="variable">@localhost</span> ~]$</span><br></pre></td></tr></table></figure>
<figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 删除(NULL) 容器</span></span><br><span class="line">root<span class="variable">@iZ28ikebrg6Z</span><span class="symbol">:/var/run</span><span class="comment"># docker images  </span></span><br><span class="line"><span class="constant">REPOSITORY </span>            <span class="constant">TAG </span>                <span class="constant">IMAGE ID </span>           <span class="constant">CREATED </span>            <span class="constant">VIRTUAL SIZE </span> </span><br><span class="line">&lt;none&gt;                 &lt;none&gt;              def2e0b08cbc        <span class="constant">About </span>an hour ago   <span class="number">1.37</span> <span class="constant">GB </span> </span><br><span class="line">sameersbn/redmine      latest              f0bec095f291        <span class="number">2</span> hours ago         <span class="number">614.6</span> <span class="constant">MB </span> </span><br><span class="line">root<span class="variable">@iZ28ikebrg6Z</span><span class="symbol">:/var/run</span><span class="comment"># docker ps -a  </span></span><br><span class="line"><span class="constant">CONTAINER ID </span>       <span class="constant">IMAGE </span>                   <span class="constant">COMMAND </span>               <span class="constant">CREATED </span>            <span class="constant">STATUS </span>                    <span class="constant">PORTS </span>              <span class="constant">NAMES </span> </span><br><span class="line"><span class="number">5</span>d6373cb79e6        <span class="number">224</span>b40d4b89f             <span class="string">"/bin/sh -c 'apt-get   25 hours ago        Exited (0) 25 hours ago                        distracted_blackwell    </span><br><span class="line">root@iZ28ikebrg6Z:/var/run# docker rm 5d63  </span><br><span class="line">5d63</span></span><br></pre></td></tr></table></figure>
<figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 镜像导出</span></span><br><span class="line">[reason<span class="variable">@localhost</span> ~]<span class="variable">$ </span>sudo docker ps -a</span><br><span class="line"><span class="constant">CONTAINER ID </span>       <span class="constant">IMAGE </span>                <span class="constant">COMMAND </span>                 <span class="constant">CREATED </span>            <span class="constant">STATUS </span>                        <span class="constant">PORTS </span>              <span class="constant">NAMES</span></span><br><span class="line">d4e581ba6af8        mofun/<span class="symbol">spark_rc:</span>v1.<span class="number">0</span>   <span class="string">"/bin/bash"</span>              <span class="number">11</span> seconds ago      <span class="constant">Exited </span>(<span class="number">0</span>) <span class="number">7</span> seconds ago                           hungry_pasteur</span><br><span class="line">[reason<span class="variable">@localhost</span> ~]<span class="variable">$ </span>sudo docker export d4e581ba6af8 &gt; mofunspark_v1.<span class="number">0</span>.tar</span><br><span class="line"></span><br><span class="line"><span class="comment"># 和导入恢复</span></span><br><span class="line">[reason<span class="variable">@localhost</span> ~]<span class="variable">$ </span>cat mofunspark_v1.<span class="number">0</span>.tar | sudo docker import - mofun/<span class="symbol">spark:</span>v1.<span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>执行Scala交互模式时出错时，需要检查</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">root@cloud1:/usr/local/jdk1<span class="number">.8</span><span class="number">.0</span>_60/jre/lib/ext<span class="preprocessor"># ls -la</span></span><br><span class="line">total <span class="number">25632</span></span><br><span class="line">drwxr-xr-x.  <span class="number">3</span>  <span class="number">501</span> staff     <span class="number">4096</span> Oct <span class="number">19</span> <span class="number">02</span>:<span class="number">35</span> .</span><br><span class="line">drwxr-xr-x. <span class="number">15</span>  <span class="number">501</span> staff     <span class="number">4096</span> Oct <span class="number">18</span> <span class="number">03</span>:<span class="number">34</span> ..</span><br><span class="line">-rw-r--r--.  <span class="number">1</span>  <span class="number">501</span> staff  <span class="number">3860522</span> Aug  <span class="number">4</span> <span class="number">19</span>:<span class="number">29</span> cldrdata.jar</span><br><span class="line">-rw-r--r--.  <span class="number">1</span>  <span class="number">501</span> staff     <span class="number">8286</span> Aug  <span class="number">4</span> <span class="number">19</span>:<span class="number">29</span> dnsns.jar</span><br><span class="line">-rw-r--r--.  <span class="number">1</span>  <span class="number">501</span> staff    <span class="number">44516</span> Aug  <span class="number">4</span> <span class="number">19</span>:<span class="number">29</span> jaccess.jar</span><br><span class="line">-rwxr-xr-x.  <span class="number">1</span>  <span class="number">501</span> staff <span class="number">18464934</span> Aug  <span class="number">3</span> <span class="number">17</span>:<span class="number">58</span> jfxrt.jar</span><br><span class="line">-rw-r--r--.  <span class="number">1</span>  <span class="number">501</span> staff  <span class="number">1178935</span> Aug  <span class="number">4</span> <span class="number">19</span>:<span class="number">29</span> localedata.jar</span><br><span class="line">-rw-r--r--.  <span class="number">1</span>  <span class="number">501</span> staff     <span class="number">1269</span> Aug  <span class="number">4</span> <span class="number">19</span>:<span class="number">29</span> meta-index</span><br><span class="line">-rw-r--r--.  <span class="number">1</span>  <span class="number">501</span> staff  <span class="number">2014239</span> Aug  <span class="number">4</span> <span class="number">19</span>:<span class="number">29</span> nashorn.jar</span><br><span class="line">-rw-r--r--.  <span class="number">1</span>  <span class="number">501</span> staff    <span class="number">39771</span> Aug  <span class="number">4</span> <span class="number">19</span>:<span class="number">29</span> sunec.jar</span><br><span class="line">-rw-r--r--.  <span class="number">1</span>  <span class="number">501</span> staff   <span class="number">278680</span> Aug  <span class="number">4</span> <span class="number">19</span>:<span class="number">29</span> sunjce_provider.jar</span><br><span class="line">-rw-r--r--.  <span class="number">1</span>  <span class="number">501</span> staff   <span class="number">250826</span> Aug  <span class="number">4</span> <span class="number">19</span>:<span class="number">29</span> sunpkcs11.jar</span><br><span class="line">drwxr-xr-x.  <span class="number">2</span> root root      <span class="number">4096</span> Oct <span class="number">19</span> <span class="number">02</span>:<span class="number">35</span> tmp</span><br><span class="line">-rw-r--r--.  <span class="number">1</span>  <span class="number">501</span> staff    <span class="number">68848</span> Aug  <span class="number">4</span> <span class="number">19</span>:<span class="number">29</span> zipfs.jar</span><br><span class="line">root@cloud1:/usr/local/jdk1<span class="number">.8</span><span class="number">.0</span>_60/jre/lib/ext<span class="preprocessor">#</span></span><br><span class="line"><span class="preprocessor"># 该目录下出现了很多类似._jfxrt.jar 的包，直接予以删除即可。</span></span><br></pre></td></tr></table></figure>
<h4 id="u53C2_u8003_u6587_u732E"><a href="#u53C2_u8003_u6587_u732E" class="headerlink" title="参考文献"></a>参考文献</h4><ul>
<li><a href="http://eksliang.iteye.com/blog/2226986" target="_blank" rel="external">http://eksliang.iteye.com/blog/2226986</a></li>
<li><a href="http://dockerpool.com/static/books/docker_practice/container/daemon.html" target="_blank" rel="external">http://dockerpool.com/static/books/docker_practice/container/daemon.html</a></li>
<li><a href="http://dockerpool.com/static/books/docker_practice/install/ubuntu.html" target="_blank" rel="external">http://dockerpool.com/static/books/docker_practice/install/ubuntu.html</a></li>
<li><a href="http://dockerpool.com/static/books/docker_practice/image/create.html" target="_blank" rel="external">http://dockerpool.com/static/books/docker_practice/image/create.html</a></li>
<li><a href="http://dockerpool.com/static/books/docker_practice/image/save_load.html" target="_blank" rel="external">http://dockerpool.com/static/books/docker_practice/image/save_load.html</a></li>
<li><a href="http://dockerpool.com/static/books/docker_practice/container/rm.html" target="_blank" rel="external">http://dockerpool.com/static/books/docker_practice/container/rm.html</a></li>
<li><a href="http://blog.csdn.net/qq1010885678/article/details/46353101" target="_blank" rel="external">http://blog.csdn.net/qq1010885678/article/details/46353101</a></li>
<li><a href="http://cn.soulmachine.me/blog/20131027/" target="_blank" rel="external">http://cn.soulmachine.me/blog/20131027/</a></li>
<li><a href="http://my.oschina.net/zjzhai/blog/225112" target="_blank" rel="external">http://my.oschina.net/zjzhai/blog/225112</a></li>
<li><a href="http://blog.csdn.net/minimicall/article/details/40188251" target="_blank" rel="external">http://blog.csdn.net/minimicall/article/details/40188251</a></li>
<li><a href="http://www.scala-lang.org/documentation/" target="_blank" rel="external">http://www.scala-lang.org/documentation/</a>  </li>
<li><a href="https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/streaming/NetworkWordCount.scala" target="_blank" rel="external">https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/streaming/NetworkWordCount.scala</a></li>
<li><a href="http://spark.apache.org/docs/latest/" target="_blank" rel="external">http://spark.apache.org/docs/latest/</a></li>
<li><a href="https://docs.sigmoidanalytics.com/index.php/Error:_Failed_to_initialize_compiler:_object_scala_not_found." target="_blank" rel="external">https://docs.sigmoidanalytics.com/index.php/Error:_Failed_to_initialize_compiler:_object_scala_not_found.</a></li>
<li><a href="http://docs.docker.com/linux/step_one/" target="_blank" rel="external">http://docs.docker.com/linux/step_one/</a></li>
<li><a href="http://blog.sequenceiq.com/blog/2015/01/09/spark-1-2-0-docker/" target="_blank" rel="external">http://blog.sequenceiq.com/blog/2015/01/09/spark-1-2-0-docker/</a></li>
</ul>
]]></content>
    <summary type="html">
    <![CDATA[<!--  -->
<!--  -->
<!--  _ __    __     __      ____    ___     ___   _____   __  __    ___     -->
<!-- /\`'__\/'__`\ /'__`\   /',__\  / _]]>
    </summary>
    
      <category term="Docker" scheme="http://reasonpun.com/tags/Docker/"/>
    
      <category term="HDFS" scheme="http://reasonpun.com/tags/HDFS/"/>
    
      <category term="Spark" scheme="http://reasonpun.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[nginx-logrotate]]></title>
    <link href="http://reasonpun.com/2015/12/04/nginx-logrotate/"/>
    <id>http://reasonpun.com/2015/12/04/nginx-logrotate/</id>
    <published>2015-12-04T08:11:51.000Z</published>
    <updated>2015-12-25T03:29:38.000Z</updated>
    <content type="html"><![CDATA[<!--  -->
<!--  -->
<!--  _ __    __     __      ____    ___     ___   _____   __  __    ___     -->
<!-- /\`'__\/'__`\ /'__`\   /',__\  / __`\ /' _ `\/\ '__`\/\ \/\ \ /' _ `\   -->
<!-- \ \ \//\  __//\ \L\.\_/\__, `\/\ \L\ \/\ \/\ \ \ \L\ \ \ \_\ \/\ \/\ \  -->
<!--  \ \_\\ \____\ \__/.\_\/\____/\ \____/\ \_\ \_\ \ ,__/\ \____/\ \_\ \_\ -->
<!--   \/_/ \/____/\/__/\/_/\/___/  \/___/  \/_/\/_/\ \ \/  \/___/  \/_/\/_/ -->
<!--                                                 \ \_\                   -->
<!--                                                  \/_/                   -->
<!--  -->
<h3 id="Centos_Logrotate"><a href="#Centos_Logrotate" class="headerlink" title="Centos Logrotate"></a>Centos Logrotate</h3><h4 id="u5173_u4E8E"><a href="#u5173_u4E8E" class="headerlink" title="关于"></a>关于</h4><p><em>logrotate</em> is  designed to ease administration of systems that generatelarge numbers of log files.  It allows automatic rotation, compression,removal, and mailing of log files.  Each log file may be handled daily,weekly, monthly, or when it grows too large. (<a href="http://linuxcommand.org/man_pages/logrotate8.html" target="_blank" rel="external">Logrotate man page</a>)</p>
<h4 id="u914D_u7F6E_u6587_u4EF6"><a href="#u914D_u7F6E_u6587_u4EF6" class="headerlink" title="配置文件"></a>配置文件</h4><h5 id="u5185_u5BB9_u793A_u4F8B"><a href="#u5185_u5BB9_u793A_u4F8B" class="headerlink" title="内容示例"></a>内容示例</h5><figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">[reason@online_http:/etc/logrotate.d]$ cat nginx.logrotate.d</span><br><span class="line"><span class="preprocessor">#</span></span><br><span class="line"><span class="preprocessor"># author: reason@mofunsky.com</span></span><br><span class="line"><span class="preprocessor"># date:   2015-03-31 10:15</span></span><br><span class="line"><span class="preprocessor"># cron:</span></span><br><span class="line"><span class="preprocessor">#       59 23 * * * /usr/sbin/logrotate /etc/logrotate.conf &gt; /dev/null</span></span><br><span class="line"><span class="preprocessor">#</span></span><br><span class="line"><span class="preprocessor"># location:</span></span><br><span class="line"><span class="preprocessor">#       /etc/logrotate.d/nginx.logrotate.d</span></span><br><span class="line"><span class="preprocessor">#</span></span><br><span class="line">/local/nginx*.log &#123;</span><br><span class="line">    daily</span><br><span class="line">    rotate <span class="number">10</span></span><br><span class="line">    missingok</span><br><span class="line">    compress</span><br><span class="line">    notifempty</span><br><span class="line">    sharedscripts</span><br><span class="line">    postrotate</span><br><span class="line">        /bin/kill -USR1 $(cat /<span class="keyword">var</span>/nginx.pid <span class="number">2</span>&gt;/dev/<span class="literal">null</span>) <span class="number">2</span>&gt;/dev/<span class="literal">null</span> || :</span><br><span class="line">    endscript</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="u5B58_u653E_u4F4D_u7F6E"><a href="#u5B58_u653E_u4F4D_u7F6E" class="headerlink" title="存放位置"></a>存放位置</h5><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[reason<span class="variable">@online_http</span><span class="symbol">:/etc/logrotate</span>.d]<span class="variable">$ </span>pwd</span><br><span class="line">/etc/logrotate.d</span><br><span class="line">[reason<span class="variable">@online_http</span><span class="symbol">:/etc/logrotate</span>.d]$</span><br></pre></td></tr></table></figure>
<h5 id="u6267_u884C_u65B9_u5F0F_u548C_u65F6_u95F4"><a href="#u6267_u884C_u65B9_u5F0F_u548C_u65F6_u95F4" class="headerlink" title="执行方式和时间"></a>执行方式和时间</h5><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor"># add by reason @ <span class="number">2015</span>-<span class="number">04</span>-<span class="number">05</span> <span class="number">20</span>:<span class="number">41</span></span></span><br><span class="line"><span class="preprocessor"># logrotate nginx log daily</span></span><br><span class="line"><span class="number">59</span>      <span class="number">23</span>      *       *       *       root /usr/sbin/logrotate /etc/logrotate.conf &gt;/dev/null <span class="number">2</span>&gt;&amp;<span class="number">1</span></span><br></pre></td></tr></table></figure>
<h5 id="u6267_u884C_u6548_u679C"><a href="#u6267_u884C_u6548_u679C" class="headerlink" title="执行效果"></a>执行效果</h5><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor"># 会按照日期，在每天的<span class="number">23</span>：<span class="number">59</span>分将改天日志转储压缩为*.gz文件</span></span><br><span class="line">[reason@online_http:/local/nginx_access_log]$ ls -lsh</span><br><span class="line">total <span class="number">5.1</span>G</span><br><span class="line"><span class="number">427</span>M -rwxrwxrwx <span class="number">1</span> nginx nginx <span class="number">427</span>M Oct <span class="number">11</span> <span class="number">12</span>:<span class="number">01</span> nginx_me_access.<span class="built_in">log</span></span><br><span class="line"><span class="number">259</span>M -rwxrwxrwx <span class="number">1</span> nginx nginx <span class="number">259</span>M Oct  <span class="number">1</span> <span class="number">23</span>:<span class="number">59</span> nginx_me_access.<span class="built_in">log</span>-<span class="number">20151001.</span>gz</span><br><span class="line"><span class="number">233</span>M -rwxrwxrwx <span class="number">1</span> nginx nginx <span class="number">233</span>M Oct  <span class="number">2</span> <span class="number">23</span>:<span class="number">59</span> nginx_me_access.<span class="built_in">log</span>-<span class="number">20151002.</span>gz</span><br><span class="line"><span class="number">228</span>M -rwxrwxrwx <span class="number">1</span> nginx nginx <span class="number">228</span>M Oct  <span class="number">3</span> <span class="number">23</span>:<span class="number">59</span> nginx_me_access.<span class="built_in">log</span>-<span class="number">20151003.</span>gz</span><br><span class="line"><span class="number">225</span>M -rwxrwxrwx <span class="number">1</span> nginx nginx <span class="number">225</span>M Oct  <span class="number">4</span> <span class="number">23</span>:<span class="number">59</span> nginx_me_access.<span class="built_in">log</span>-<span class="number">20151004.</span>gz</span><br><span class="line"><span class="number">232</span>M -rwxrwxrwx <span class="number">1</span> nginx nginx <span class="number">232</span>M Oct  <span class="number">5</span> <span class="number">23</span>:<span class="number">59</span> nginx_me_access.<span class="built_in">log</span>-<span class="number">20151005.</span>gz</span><br></pre></td></tr></table></figure>
<h5 id="u53E6_u5916_u4E00_u79CD_u6267_u884C_u65B9_u5F0F"><a href="#u53E6_u5916_u4E00_u79CD_u6267_u884C_u65B9_u5F0F" class="headerlink" title="另外一种执行方式"></a>另外一种执行方式</h5><p>logrotate 是linux系统的缺省安装命令，初始化状态下即存在缺省的配置信息，其中包括执行文件和时间</p>
<figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[reason<span class="variable">@online_http</span><span class="symbol">:/local/nginx_access_log</span>]<span class="variable">$ </span>cd /etc/logrotate.d/</span><br><span class="line">[reason<span class="variable">@online_http</span><span class="symbol">:/etc/logrotate</span>.d]<span class="variable">$ </span>ls</span><br><span class="line">nginx.logrotate.d</span><br><span class="line">[reason<span class="variable">@online_http</span><span class="symbol">:/etc/logrotate</span>.d]<span class="variable">$ </span>pwd</span><br><span class="line">/etc/logrotate.d</span><br><span class="line">[reason<span class="variable">@online_http</span><span class="symbol">:/etc/logrotate</span>.d]$</span><br></pre></td></tr></table></figure>
<p>比如在/etc/logrotate.d目录下会缺省放置一批需要转储的日志服务对应的配置文件，比如httpd，exim等，logrotate会按照配置文件信息按照既定时间对其产生的日志数据进行转储操作。</p>
<figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[reason<span class="variable">@online_http</span><span class="symbol">:/etc/logrotate</span>.d]<span class="variable">$ </span>pwd</span><br><span class="line">/etc/logrotate.d</span><br><span class="line">[reason<span class="variable">@online_http</span><span class="symbol">:/etc/logrotate</span>.d]<span class="variable">$ </span>cat yum</span><br><span class="line">/var/log/yum.log &#123;</span><br><span class="line">    missingok</span><br><span class="line">    notifempty</span><br><span class="line">    size <span class="number">30</span>k</span><br><span class="line">    yearly</span><br><span class="line">    create <span class="number">0600</span> root root</span><br><span class="line">&#125;</span><br><span class="line">[reason<span class="variable">@online_http</span><span class="symbol">:/etc/logrotate</span>.d]$</span><br></pre></td></tr></table></figure>
<p>具体参数涵义可参考<a href="http://linuxcommand.org/man_pages/logrotate8.html" target="_blank" rel="external">Logrotate man page</a></p>
<p>缺省状态下，logrotate的自动执行分别被放入了如下几个文件中</p>
<ul>
<li>cron.daily</li>
<li>cron.weekly</li>
<li>cron.monthly</li>
</ul>
<p>而对于不同的linux发行版本以上脚本可能被放置在/etc/crontab中<br>或者</p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[root<span class="variable">@i</span>-bntub2bp logrotate.d]<span class="comment"># cat /etc/anacrontab</span></span><br><span class="line"><span class="comment"># /etc/anacrontab: configuration file for anacron</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># See anacron(8) and anacrontab(5) for details.</span></span><br><span class="line"></span><br><span class="line"><span class="constant">SHELL</span>=<span class="regexp">/bin/sh</span></span><br><span class="line"><span class="constant">PATH</span>=<span class="regexp">/sbin:/bin</span><span class="symbol">:/usr/sbin</span><span class="symbol">:/usr/bin</span></span><br><span class="line"><span class="constant">MAILTO</span>=root</span><br><span class="line"><span class="comment"># the maximal random delay added to the base delay of the jobs</span></span><br><span class="line"><span class="constant">RANDOM_DELAY</span>=<span class="number">45</span></span><br><span class="line"><span class="comment"># the jobs will be started during the following hours only</span></span><br><span class="line"><span class="constant">START_HOURS_RANGE</span>=<span class="number">3</span>-<span class="number">22</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#period in days   delay in minutes   job-identifier   command</span></span><br><span class="line"><span class="number">1</span>       <span class="number">5</span>       cron.daily              nice run-parts /etc/cron.daily</span><br><span class="line"><span class="number">7</span>       <span class="number">25</span>      cron.weekly             nice run-parts /etc/cron.weekly</span><br><span class="line"><span class="variable">@monthly</span> <span class="number">45</span>     cron.monthly            nice run-parts /etc/cron.monthly</span><br><span class="line">[root<span class="variable">@i</span>-bntub2bp logrotate.d]<span class="comment">#</span></span><br></pre></td></tr></table></figure>
<p>针对</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@i-bntub2bp logrotate.d]# cat /etc/redhat-<span class="operator"><span class="keyword">release</span></span><br><span class="line">CentOS <span class="keyword">release</span> <span class="number">6.6</span> (<span class="keyword">Final</span>)</span><br><span class="line">[root@<span class="keyword">i</span>-bntub2bp logrotate.<span class="keyword">d</span>]#</span></span><br></pre></td></tr></table></figure>
<p>而言，可以直接在/etc/<a href="https://www.centos.org/docs/2/rhl-cg-en-7.2/anacron.html" target="_blank" rel="external">anacrontab</a>中找到如上信息。</p>
<p>大体解释下anacrontab的部分参数涵义</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">START_HOURS_RANGE=<span class="number">3</span>-<span class="number">22</span>  <span class="preprocessor"># 执行时间为<span class="number">3</span>点到<span class="number">22</span>点之间</span></span><br><span class="line">RANDOM_DELAY=<span class="number">45</span> <span class="preprocessor"># 时间处于可执行区间内随机延迟<span class="number">45</span>分钟之内任意时间</span></span><br><span class="line"><span class="number">1</span>       <span class="number">5</span>       cron.daily              nice run-parts /etc/cron.daily</span><br><span class="line"><span class="preprocessor"># 执行时间为<span class="number">3</span>点到<span class="number">22</span>点之间执行/etc/cron.daily脚本 (after reboot and after the machine has been up for <span class="number">5</span> minutes^^), 如果没有重启服务，则会在<span class="number">3</span>：<span class="number">05</span>之后执行。</span></span><br></pre></td></tr></table></figure>
<h4 id="u53C2_u8003_u6587_u732E"><a href="#u53C2_u8003_u6587_u732E" class="headerlink" title="参考文献"></a>参考文献</h4><ul>
<li><a href="http://huoding.com/2013/04/21/246" target="_blank" rel="external">http://huoding.com/2013/04/21/246</a></li>
<li><a href="http://serverfault.com/questions/135906/when-does-cron-daily-run" target="_blank" rel="external">http://serverfault.com/questions/135906/when-does-cron-daily-run</a></li>
<li><a href="http://www.cyberciti.biz/faq/linux-when-does-cron-daily-weekly-monthly-run/" target="_blank" rel="external">http://www.cyberciti.biz/faq/linux-when-does-cron-daily-weekly-monthly-run/</a></li>
<li><a href="http://linuxcommand.org/man_pages/logrotate8.html" target="_blank" rel="external">http://linuxcommand.org/man_pages/logrotate8.html</a></li>
<li><a href="https://www.centos.org/docs/2/rhl-cg-en-7.2/anacron.html" target="_blank" rel="external">https://www.centos.org/docs/2/rhl-cg-en-7.2/anacron.html</a></li>
</ul>
]]></content>
    <summary type="html">
    <![CDATA[<!--  -->
<!--  -->
<!--  _ __    __     __      ____    ___     ___   _____   __  __    ___     -->
<!-- /\`'__\/'__`\ /'__`\   /',__\  / _]]>
    </summary>
    
      <category term="Logrorate" scheme="http://reasonpun.com/tags/Logrorate/"/>
    
      <category term="Nginx" scheme="http://reasonpun.com/tags/Nginx/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[spark source (1)]]></title>
    <link href="http://reasonpun.com/2015/12/04/spark-source-1/"/>
    <id>http://reasonpun.com/2015/12/04/spark-source-1/</id>
    <published>2015-12-04T02:59:14.000Z</published>
    <updated>2015-12-25T03:30:47.000Z</updated>
    <content type="html"><![CDATA[<!--  -->
<!--  -->
<!--  _ __    __     __      ____    ___     ___   _____   __  __    ___     -->
<!-- /\`'__\/'__`\ /'__`\   /',__\  / __`\ /' _ `\/\ '__`\/\ \/\ \ /' _ `\   -->
<!-- \ \ \//\  __//\ \L\.\_/\__, `\/\ \L\ \/\ \/\ \ \ \L\ \ \ \_\ \/\ \/\ \  -->
<!--  \ \_\\ \____\ \__/.\_\/\____/\ \____/\ \_\ \_\ \ ,__/\ \____/\ \_\ \_\ -->
<!--   \/_/ \/____/\/__/\/_/\/___/  \/___/  \/_/\/_/\ \ \/  \/___/  \/_/\/_/ -->
<!--                                                 \ \_\                   -->
<!--                                                  \/_/                   -->
<!--  -->
<h3 id="fork_git"><a href="#fork_git" class="headerlink" title="fork git"></a>fork git</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git <span class="built_in">clone</span> https://github.com/apache/spark.git</span><br></pre></td></tr></table></figure>
]]></content>
    <summary type="html">
    <![CDATA[<!--  -->
<!--  -->
<!--  _ __    __     __      ____    ___     ___   _____   __  __    ___     -->
<!-- /\`'__\/'__`\ /'__`\   /',__\  / _]]>
    </summary>
    
      <category term="Kernel" scheme="http://reasonpun.com/tags/Kernel/"/>
    
      <category term="Source" scheme="http://reasonpun.com/tags/Source/"/>
    
      <category term="Spark" scheme="http://reasonpun.com/tags/Spark/"/>
    
  </entry>
  
</feed>
