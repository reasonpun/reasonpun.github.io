<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[Tensor Cell]]></title>
  <subtitle><![CDATA[还要再走500里]]></subtitle>
  <link href="/atom.xml" rel="self"/>
  <link href="http://reasonpun.com/"/>
  <updated>2016-04-06T11:10:33.000Z</updated>
  <id>http://reasonpun.com/</id>
  
  <author>
    <name><![CDATA[reasono]]></name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title><![CDATA[Oryx2 开发者文档]]></title>
    <link href="http://reasonpun.com/2016/04/06/Oryx2-developer-doc/"/>
    <id>http://reasonpun.com/2016/04/06/Oryx2-developer-doc/</id>
    <published>2016-04-06T09:58:20.000Z</published>
    <updated>2016-04-06T11:10:33.000Z</updated>
    <content type="html"><![CDATA[<!--  -->
<!--  -->
<!--  _ __    __     __      ____    ___     ___   _____   __  __    ___     -->
<!-- /\`'__\/'__`\ /'__`\   /',__\  / __`\ /' _ `\/\ '__`\/\ \/\ \ /' _ `\   -->
<!-- \ \ \//\  __//\ \L\.\_/\__, `\/\ \L\ \/\ \/\ \ \ \L\ \ \ \_\ \/\ \/\ \  -->
<!--  \ \_\\ \____\ \__/.\_\/\____/\ \____/\ \_\ \_\ \ ,__/\ \____/\ \_\ \_\ -->
<!--   \/_/ \/____/\/__/\/_/\/___/  \/___/  \/_/\/_/\ \ \/  \/___/  \/_/\/_/ -->
<!--                                                 \ \_\                   -->
<!--                                                  \/_/                   -->
<!--  -->
<h3 id="u73AF_u5883_u9700_u6C42"><a href="#u73AF_u5883_u9700_u6C42" class="headerlink" title="环境需求"></a>环境需求</h3><ul>
<li><a href="http://git-scm.com/" target="_blank" rel="external">git</a>, 或者一个支持git的IDE</li>
<li><a href="http://maven.apache.org/" target="_blank" rel="external">Apache Maven</a> 3.2.1 或者更新版本</li>
<li><a href="http://www.oracle.com/technetwork/java/javase/downloads/index.html" target="_blank" rel="external">Java JDK</a> (不能只有JRE) 7 或者更新版本</li>
</ul>
<p>以上需要已经安装到了你的开发环境中。</p>
<h2 id="Building"><a href="#Building" class="headerlink" title="Building"></a>Building</h2><p>克隆代码到你本地，并且编译：</p>
<figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git <span class="keyword">clone</span> <span class="title">https</span>://github.com/OryxProject/oryx.git oryx</span><br><span class="line">cd oryx</span><br><span class="line">mvn -DskipTests package</span><br></pre></td></tr></table></figure>
<p>会编译出如下的二进制的jar文件：</p>
<ul>
<li>批处理层: deploy/oryx-batch/target/oryx-batch-2.1.2.jar</li>
<li>实时处理层: deploy/oryx-speed/target/oryx-speed-2.1.2.jar</li>
<li>服务层: deploy/oryx-serving/target/oryx-serving-2.1.2.jar</li>
</ul>
<p>友情提醒，如果你对开发Oryx感兴趣，可以根据这个<a href="https://help.github.com/articles/fork-a-repo" target="_blank" rel="external">分支</a>克隆自己的分支，然后就可以提交修改了。</p>
<h3 id="Java_8"><a href="#Java_8" class="headerlink" title="Java 8"></a>Java 8</h3><p>如果使用Java8编译，需要添加参数-Pjava8 并且测试这里的指令。</p>
<h3 id="Platform_Only"><a href="#Platform_Only" class="headerlink" title="Platform Only"></a>Platform Only</h3><p>默认的编译包括基于Spark MLlib和其他库的端到端的ML程序。如果只是编译lambda和ML层，需要通过参数-P!app-tier关闭其他的选项。注意，在bash中，！需要转义： -P!app-tier。</p>
<h3 id="u6D4B_u8BD5"><a href="#u6D4B_u8BD5" class="headerlink" title="测试"></a>测试</h3><p>mvn 测试会执行所有的但愿测试用例。也同时会执行所有的集成测试，这个可能会需要稍微长点的时间。</p>
<h2 id="u6A21_u578B_u5BF9_u5E94_u8868"><a href="#u6A21_u578B_u5BF9_u5E94_u8868" class="headerlink" title="模型对应表"></a>模型对应表</h2><p>主要的模型和他们对应的层：</p>
<table>
<thead>
<tr>
<th></th>
<th>Serving</th>
<th>Speed</th>
<th>Batch</th>
</tr>
</thead>
<tbody>
<tr>
<td>Binary</td>
<td>oryx-serving</td>
<td>oryx-speed</td>
<td>oryx-batch</td>
</tr>
<tr>
<td>App</td>
<td>oryx-app-serving</td>
<td>oryx-app-mllib oryx-app</td>
<td>oryx-app-mllib oryx-app</td>
</tr>
<tr>
<td>ML</td>
<td></td>
<td>oryx-ml</td>
<td>oryx-ml</td>
</tr>
<tr>
<td>Lambda</td>
<td>oryx-lambda-serving</td>
<td>oryx-lambda</td>
<td>oryx-lambda</td>
</tr>
</tbody>
</table>
<p>支持的模型，比如：<a href="https://github.com/OryxProject/oryx/tree/master/framework/oryx-common" target="_blank" rel="external">oryx-common</a>, <a href="https://github.com/OryxProject/oryx/tree/master/app/oryx-app-common" target="_blank" rel="external">oryx-app-common</a>, <a href="https://github.com/OryxProject/oryx/tree/master/framework/oryx-api" target="_blank" rel="external">oryx-api</a>, and <a href="https://github.com/OryxProject/oryx/tree/master/app/oryx-app-api" target="_blank" rel="external">oryx-app-api</a> 没有在这里列出了。</p>
<h2 id="u5B9E_u73B0_u4E00_u4E2AOryx__u7A0B_u5E8F"><a href="#u5B9E_u73B0_u4E00_u4E2AOryx__u7A0B_u5E8F" class="headerlink" title="实现一个Oryx 程序"></a>实现一个Oryx 程序</h2><p>Oryx 中的“app 层”，是实现了推荐的真实的批处理，实时，服务层逻辑，集群和分类。然而，任何实现都需要使用到Oryx。他们也可以混合和匹配。举个例子，你可以重新实现ALS-related推荐的批处理层，但是仍然使用原来的ALS的服务层和实时计算层。</p>
<h3 id="u521B_u5EFA_u4E00_u4E2A_u7A0B_u5E8F"><a href="#u521B_u5EFA_u4E00_u4E2A_u7A0B_u5E8F" class="headerlink" title="创建一个程序"></a>创建一个程序</h3><p>在每个例子中，创建一个自定义的批处理层，实时计算层或者服务层的程序都需要实现com.cloudera.oryx.api中的几个关键的Java接口或者Scala的特性。这些接口/特性可以在项目的oryx-api模型中找到。</p>
<table>
<thead>
<tr>
<th></th>
<th>Java</th>
<th>Scala</th>
</tr>
</thead>
<tbody>
<tr>
<td>Batch</td>
<td>batch.BatchLayerUpdate</td>
<td>batch.ScalaBatchLayerUpdate</td>
</tr>
<tr>
<td>Speed</td>
<td>speed.SpeedModelManager</td>
<td>speed.ScalaSpeedModelManager</td>
</tr>
<tr>
<td>Serving</td>
<td>serving.ServingModelManager</td>
<td>serving.ScalaServingModelManager</td>
</tr>
</tbody>
</table>
<p>com.cloudera.oryx.api也包含大量的关键的类和接口，举个例子，<a href="https://github.com/OryxProject/oryx/blob/master/framework/oryx-api/src/main/java/com/cloudera/oryx/api/serving/OryxResource.java" target="_blank" rel="external">serving.OryxResource</a>  是一个入口，用来编译自定义的JAX-RS 端点，但是不需要使用。</p>
<h3 id="u7F16_u8BD1_u7A0B_u5E8F"><a href="#u7F16_u8BD1_u7A0B_u5E8F" class="headerlink" title="编译程序"></a>编译程序</h3><p>进入你的程序的这些接口/特性，添加一个com.cloudera.oryx:oryx-api的依赖，scope字段需要填写“provided”，在Maven中，需要添加如下依赖：<br>In Maven, this would mean adding a dependency like:</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="title">dependencies</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">groupId</span>&gt;</span>com.cloudera.oryx<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>oryx-api<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="title">scope</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">version</span>&gt;</span>2.1.2<span class="tag">&lt;/<span class="title">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="title">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>这些artifacts被放在了<a href="https://repository.cloudera.com/artifactory/cloudera-repos/" target="_blank" rel="external">Cloudera</a>这个分支下，因此在编译的时候需要引用这个分支：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="title">repositories</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">repository</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">id</span>&gt;</span>cloudera<span class="tag">&lt;/<span class="title">id</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">url</span>&gt;</span>https://repository.cloudera.com/artifactory/cloudera-repos/<span class="tag">&lt;/<span class="title">url</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="title">repository</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">repositories</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>一个最小的实例可以访问<a href="https://github.com/OryxProject/oryx/tree/master/app/example" target="_blank" rel="external">example/</a> 这里。</p>
<p>”Word Count” 这个程序是按照空格将行分割成独立的单词，然后统计出排重后的单词出现的次数。</p>
<p>编译代码后生成一个JAR文件，包含了程序实现和所有第三方的diam，如果使用Maven，可以通过mvn package命令。</p>
<h3 id="u7F16_u8BD1_Word_Count__u4F8B_u5B50"><a href="#u7F16_u8BD1_Word_Count__u4F8B_u5B50" class="headerlink" title="编译 Word Count 例子"></a>编译 Word Count 例子</h3><p>举例：编译样例代码如下：<br><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">cd</span> <span class="keyword">app</span>/example</span><br><span class="line">mvn package</span><br></pre></td></tr></table></figure></p>
<p>生成的JAR包在target/example-2.1.2.jar。</p>
<h3 id="u81EA_u5B9A_u4E49Oryx_u7A0B_u5E8F"><a href="#u81EA_u5B9A_u4E49Oryx_u7A0B_u5E8F" class="headerlink" title="自定义Oryx程序"></a>自定义Oryx程序</h3><p>当发布一个源自Oryx的预打包程序，在某些情况下，有可能会提供一个扩展的实现，从而可以自定义他们的行为。举例，ALS推荐程序暴露了com.cloudera.oryx.app.als.RescorerProvider接口。这些特定程序API类可以在模块oryx-app-api中找到。这些接口的实现也可以在独立模式下被编译，打包，部署。</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="title">dependencies</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">groupId</span>&gt;</span>com.cloudera.oryx<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>oryx-app-api<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="title">scope</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">version</span>&gt;</span>2.1.2<span class="tag">&lt;/<span class="title">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="title">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h2 id="u53D1_u5E03_u7A0B_u5E8F"><a href="#u53D1_u5E03_u7A0B_u5E8F" class="headerlink" title="发布程序"></a>发布程序</h2><p>拷贝生成的JAR文件–myapp.jar，放到需要执行的Oryx 二进制JAR文件相同的目录下。</p>
<p>修改Oryx的配置文件，以便于引用自定义的批处理，实时计算和服务层的实现。<br>当执行批处理，实时计算和服务层时，需要添加–app-jar myapp.jar到oryx-run.sh命令行中。</p>
<h3 id="u53D1_u5E03_Word_Count__u4F8B_u5B50"><a href="#u53D1_u5E03_Word_Count__u4F8B_u5B50" class="headerlink" title="发布 Word Count 例子"></a>发布 Word Count 例子</h3><p>举例，如果已经编译好了上述的“word count”的程序，你可以执行这个程序，直接引用wordcount-example.conf这个配置文件：</p>
<figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./oryx-<span class="keyword">run</span>.<span class="keyword">sh</span> batch --<span class="keyword">conf</span> wordcount-example.<span class="keyword">conf</span> --<span class="keyword">app</span>-jar example-2.1.2.jar</span><br></pre></td></tr></table></figure>
<p>… 对于实时计算和服务层也是同样的。</p>
<figure class="highlight nimrod"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">curl -X <span class="type">POST</span> http://.../add/foo%<span class="number">20</span>bar%<span class="number">20</span>baz</span><br><span class="line">...</span><br><span class="line">curl http://.../<span class="keyword">distinct</span></span><br><span class="line">&#123;<span class="string">"foo"</span>:<span class="number">2</span>,<span class="string">"bar"</span>:<span class="number">2</span>,<span class="string">"baz"</span>:<span class="number">2</span>&#125;</span><br></pre></td></tr></table></figure>
<p>配置文件本身已经配置好了主机名称和<a href="http://www.cloudera.com/content/www/en-us/downloads/quickstart_vms.html" target="_blank" rel="external">Cloudera Quickstart VM</a>的参数。事实上，这个例子可以作为一个集群配置的例子：<a href="http://oryx.io/docs/admin.html#cloudera_quickstart_vm_setup" target="_blank" rel="external">Cloudera Quickstart VM Setup</a>。</p>
]]></content>
    <summary type="html">
    <![CDATA[<!--  -->
<!--  -->
<!--  _ __    __     __      ____    ___     ___   _____   __  __    ___     -->
<!-- /\`'__\/'__`\ /'__`\   /',__\  / _]]>
    </summary>
    
      <category term="Admin" scheme="http://reasonpun.com/tags/Admin/"/>
    
      <category term="Hadoop" scheme="http://reasonpun.com/tags/Hadoop/"/>
    
      <category term="Oryx2" scheme="http://reasonpun.com/tags/Oryx2/"/>
    
      <category term="Spark" scheme="http://reasonpun.com/tags/Spark/"/>
    
      <category term="Spark-Streaming" scheme="http://reasonpun.com/tags/Spark-Streaming/"/>
    
      <category term="documentation" scheme="http://reasonpun.com/tags/documentation/"/>
    
      <category term="数据挖掘" scheme="http://reasonpun.com/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
      <category term="实时计算" scheme="http://reasonpun.com/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Oryx2 终端用户文档]]></title>
    <link href="http://reasonpun.com/2016/04/06/Oryx2-%E7%BB%88%E7%AB%AF%E7%94%A8%E6%88%B7%E6%96%87%E6%A1%A3/"/>
    <id>http://reasonpun.com/2016/04/06/Oryx2-终端用户文档/</id>
    <published>2016-04-06T07:18:22.000Z</published>
    <updated>2016-04-06T09:54:44.000Z</updated>
    <content type="html"><![CDATA[<!--  -->
<!--  -->
<!--  _ __    __     __      ____    ___     ___   _____   __  __    ___     -->
<!-- /\`'__\/'__`\ /'__`\   /',__\  / __`\ /' _ `\/\ '__`\/\ \/\ \ /' _ `\   -->
<!-- \ \ \//\  __//\ \L\.\_/\__, `\/\ \L\ \/\ \/\ \ \ \L\ \ \ \_\ \/\ \/\ \  -->
<!--  \ \_\\ \____\ \__/.\_\/\____/\ \____/\ \_\ \_\ \ ,__/\ \____/\ \_\ \_\ -->
<!--   \/_/ \/____/\/__/\/_/\/___/  \/___/  \/_/\/_/\ \ \/  \/___/  \/_/\/_/ -->
<!--                                                 \ \_\                   -->
<!--                                                  \/_/                   -->
<!--  -->
<h3 id="u8FD0_u884C"><a href="#u8FD0_u884C" class="headerlink" title="运行"></a>运行</h3><p>注意：你必须已经按照<a href="http://reasonpun.com/2015/12/21/Oryx2-Admin-Docs/">管理员文档</a>中提到的配置好了你的集群。</p>
<p>下载最新的Oryx版本，包括批处理层，实时计算层和服务层的jar文件和sh脚本。</p>
<p>或者，源码编译他们并从deploy/bin/获取最新的脚本。</p>
<p>拷贝二进制和脚本到hadoop集群的机器上。他们可以会被部署到不同的机器，或者是被部署到一个测试机器上。实时和批处理层应该运行且只能运行在一台机器上（The Speed and Batch Layers should run on at most one machine, each），服务层则可以运行于多个节点上。</p>
<p>创建一个配置文件，可以简单的拷贝例子中的conf/als-example.conf。并修改host名称，端口和目录。实际上，选择hdfs上已经存在的数据和模型目录便于用户运行Oryx 二进制命令。</p>
<p>拷贝该配置文件，并重命名为oryx.conf，将他放到每个机器的上二进制命令和脚本相同的目录下。</p>
<p>执行如下命令开始这3个层的运行：</p>
<figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">./oryx-<span class="keyword">run</span>.<span class="keyword">sh</span> batch</span><br><span class="line">./oryx-<span class="keyword">run</span>.<span class="keyword">sh</span> speed</span><br><span class="line">./oryx-<span class="keyword">run</span>.<span class="keyword">sh</span> serving</span><br></pre></td></tr></table></figure>
<p>参数–layer-jar your-layer.jar and –conf your-config.conf可以指定某一层特定位置的jar文件和配置文件。也可以使用–jvm-args直接传递更多的参数给Spark驱动程序，比如：–jvm-args=”-Dkey=value”</p>
<p>这都不需要在同一台机器上，但是也不一定（如果配置特殊指定批处理和实时处理层，服务层API不同的端口）。服务层可以运行在多个机器上。</p>
<p>举个例子，批处理层SparkUI运行在启动脚本所在的机器的4040端口（除非通过配置更改）。一个简单的基于web端的控制台的服务层默认是运行在8080端口的。</p>
<p>完美！</p>
<h3 id="u5C1D_u8BD5_u4E0BALS_u7684_u4F8B_u5B50_u5427"><a href="#u5C1D_u8BD5_u4E0BALS_u7684_u4F8B_u5B50_u5427" class="headerlink" title="尝试下ALS的例子吧"></a>尝试下ALS的例子吧</h3><p>如果你已经使用了上述的配置，你就已经可以运行一个基于ALS的推荐程序实例。<br>自获取GroupLens 100K的数据集，并且找到u.data文件，这个文件的内容需要转换成csv格式：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">tr</span> <span class="string">'\t'</span> <span class="string">','</span> &lt; u<span class="class">.data</span> &gt; data.csv</span><br></pre></td></tr></table></figure>
<p>将这些数据放入服务层，使用本地的命令行工具，如下：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">wget --quiet --post-file data<span class="class">.csv</span> --output-document - \</span><br><span class="line">  --<span class="tag">header</span> <span class="string">"Content-Type: text/csv"</span> \</span><br><span class="line">  http:<span class="comment">//your-serving-layer:8080/ingest</span></span><br></pre></td></tr></table></figure>
<p>如果你使用tail命令查看输入的内容，可以看到如下数据：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">196</span>,<span class="number">242</span>,<span class="number">3.0</span>,<span class="number">881250949186</span></span><br><span class="line"><span class="number">196</span>,<span class="number">242</span>,<span class="number">3.0</span>,<span class="number">881250949</span></span><br><span class="line"><span class="number">186</span>,<span class="number">302</span>,<span class="number">3.0</span>,<span class="number">891717742</span></span><br><span class="line"><span class="number">22</span>,<span class="number">377</span>,<span class="number">1.0</span>,<span class="number">878887116</span></span><br><span class="line"><span class="number">244</span>,<span class="number">51</span>,<span class="number">2.0</span>,<span class="number">880606923</span></span><br><span class="line"><span class="number">166</span>,<span class="number">346</span>,<span class="number">1.0</span>,<span class="number">886397596</span></span><br><span class="line"><span class="number">298</span>,<span class="number">474</span>,<span class="number">4.0</span>,<span class="number">884182806</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>很快的，你也可以看到批处理层已经开始触发一个新的计算了。这个例子被配置为5分钟一个周期。</p>
<p>数据首先会被写入HDFS。默认配置被写入hdfs:///user/example/Oryx/data/目录下。且目录以时间戳命名，每一部分都包含Hadoop part-r-* 文件，都是以文本的序列话文件的方式存储。虽然不是纯文本，打印出来的话，还是有一部分是可以识别的，因为这其实真的是文本。</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SEQorg<span class="class">.apache</span><span class="class">.hadoop</span><span class="class">.io</span><span class="class">.Text</span>org<span class="class">.apache</span><span class="class">.hadoop</span><span class="class">.io</span><span class="class">.Text</span>����^�]�XسN�<span class="number">22</span>,<span class="number">377</span>,<span class="number">1.0</span>,<span class="number">878887116</span><span class="number">62</span>...</span><br></pre></td></tr></table></figure>
<p>模型计算开始。这些批处理层会以大量的新的分布式的作业形式展现。在这个例子中，Spark UI可以通过<a href="http://your-batch-layer:4040访问。" target="_blank" rel="external">http://your-batch-layer:4040访问。</a></p>
<p>模型计算是非常快的，执行完毕以后会合并PMML和支持数据文件并存储到目录hdfs:///user/example/Oryx/model/下。举个例子，model.pmml 的内容如下：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="pi">&lt;?xml version="1.0" encoding="UTF-8" standalone="yes"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">PMML</span> <span class="attribute">xmlns</span>=<span class="value">"http://www.dmg.org/PMML-4_2"</span> <span class="attribute">version</span>=<span class="value">"4.2.1"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">Header</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">Application</span> <span class="attribute">name</span>=<span class="value">"Oryx"</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">Timestamp</span>&gt;</span>2014-12-18T04:48:54-0800<span class="tag">&lt;/<span class="title">Timestamp</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="title">Header</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">Extension</span> <span class="attribute">name</span>=<span class="value">"X"</span> <span class="attribute">value</span>=<span class="value">"X/"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">Extension</span> <span class="attribute">name</span>=<span class="value">"Y"</span> <span class="attribute">value</span>=<span class="value">"Y/"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">Extension</span> <span class="attribute">name</span>=<span class="value">"features"</span> <span class="attribute">value</span>=<span class="value">"10"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">Extension</span> <span class="attribute">name</span>=<span class="value">"lambda"</span> <span class="attribute">value</span>=<span class="value">"0.001"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">Extension</span> <span class="attribute">name</span>=<span class="value">"implicit"</span> <span class="attribute">value</span>=<span class="value">"true"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">Extension</span> <span class="attribute">name</span>=<span class="value">"alpha"</span> <span class="attribute">value</span>=<span class="value">"1.0"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">Extension</span> <span class="attribute">name</span>=<span class="value">"XIDs"</span>&gt;</span>56 168 222 343 397 ...</span><br><span class="line">     ...</span><br></pre></td></tr></table></figure>
<p>The X/ and Y/ subdirectories next to it contain feature vectors, like:</p>
<figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[56,[<span class="number">0.57462828</span><span class="number">34154238</span>,-<span class="number">0.088966141</span><span class="number">31333057</span>,-<span class="number">0.029456222</span><span class="number">765775263</span>,</span><br><span class="line">  <span class="number">0.603982121</span><span class="number">9690552,0</span>.<span class="number">149790181477</span>4658,-<span class="number">0.01865431</span><span class="number">2114339863</span>,</span><br><span class="line">  -<span class="number">0.37342063</span><span class="number">488340266</span>,-<span class="number">0.237076884</span><span class="number">3521807,1</span>.<span class="number">148260034028</span>485,</span><br><span class="line">  <span class="number">1.06456436</span><span class="number">56769153</span>]]</span><br><span class="line">[168,[<span class="number">0.87227698</span><span class="number">82777296,0</span>.<span class="number">43704169430</span><span class="number">31704,0.27</span><span class="number">40204446154</span>9885,</span><br><span class="line">  -<span class="number">0.031252701</span><span class="number">117490456</span>,-<span class="number">0.724138575</span><span class="number">3098256,0</span>.<span class="number">02607908100</span><span class="number">2582338</span>,</span><br><span class="line">  <span class="number">0.42050973</span><span class="number">702065714</span>,<span class="number">0.277669233</span><span class="number">96205817,0</span>.<span class="number">6241033215856</span>671,</span><br><span class="line">  -<span class="number">0.48530795</span><span class="number">198811266</span>]]</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>如果使用tail命令查看更新的内容。<br>这些数据会很快放入服务层，此时访问/ready会返回200 OK。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">wget --quiet --output-document - --server-response \</span><br><span class="line">  http:<span class="comment">//your-serving-layer:8080/ready</span></span><br><span class="line">...</span><br><span class="line">  HTTP/<span class="number">1.1</span> <span class="number">200</span> OK</span><br><span class="line">  Content-Length: <span class="number">0</span></span><br><span class="line">  Date: Tue, <span class="number">1</span> Sep <span class="number">2015</span> <span class="number">13</span>:<span class="number">26</span>:<span class="number">53</span> GMT</span><br><span class="line">  Server: Oryx</span><br></pre></td></tr></table></figure>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">wget --quiet --output-document -  http:<span class="comment">//your-serving-layer:8080/recommend/17</span></span><br><span class="line">...</span><br><span class="line"><span class="number">50</span>,<span class="number">0.7749542842056966</span></span><br><span class="line"><span class="number">275</span>,<span class="number">0.7373013861581563</span></span><br><span class="line"><span class="number">258</span>,<span class="number">0.731818692628511</span></span><br><span class="line"><span class="number">181</span>,<span class="number">0.7049967175706345</span></span><br><span class="line"><span class="number">127</span>,<span class="number">0.704518989947498</span></span><br><span class="line"><span class="number">121</span>,<span class="number">0.7014631029793741</span></span><br><span class="line"><span class="number">15</span>,<span class="number">0.6954683387287907</span></span><br><span class="line"><span class="number">288</span>,<span class="number">0.6774889711024022</span></span><br><span class="line"><span class="number">25</span>,<span class="number">0.6663619887033064</span></span><br><span class="line"><span class="number">285</span>,<span class="number">0.6398968471343595</span></span><br></pre></td></tr></table></figure>
<p>恭喜！实时推荐系统搭建完毕！可以通过Ctrl-C关闭。</p>
<h3 id="API_u624B_u518C"><a href="#API_u624B_u518C" class="headerlink" title="API手册"></a>API手册</h3><p>Oryx 支持多种端到端的程序，包括服务层的REST 接口。</p>
<h3 id="u534F_u540C_u8FC7_u6EE4_u548C_u63A8_u8350"><a href="#u534F_u540C_u8FC7_u6EE4_u548C_u63A8_u8350" class="headerlink" title="协同过滤和推荐"></a>协同过滤和推荐</h3><ul>
<li><a href="http://oryx.io/apidocs/com/cloudera/oryx/app/serving/als/Recommend.html" target="_blank" rel="external">/recommend</a></li>
<li><a href="http://oryx.io/apidocs/com/cloudera/oryx/app/serving/als/RecommendToMany.html" target="_blank" rel="external">/recommendToMany</a></li>
<li><a href="http://oryx.io/apidocs/com/cloudera/oryx/app/serving/als/RecommendToAnonymous.html" target="_blank" rel="external">/recommendToAnonymous</a></li>
<li><a href="http://oryx.io/apidocs/com/cloudera/oryx/app/serving/als/RecommendWithContext.html" target="_blank" rel="external">/recommendWithContext</a></li>
<li><a href="http://oryx.io/apidocs/com/cloudera/oryx/app/serving/als/Similarity.html" target="_blank" rel="external">/similarity</a></li>
<li><a href="http://oryx.io/apidocs/com/cloudera/oryx/app/serving/als/SimilarityToItem.html" target="_blank" rel="external">/similarityToItem</a></li>
<li><a href="http://oryx.io/apidocs/com/cloudera/oryx/app/serving/als/KnownItems.html" target="_blank" rel="external">/knownItems</a></li>
<li><a href="http://oryx.io/apidocs/com/cloudera/oryx/app/serving/als/Estimate.html" target="_blank" rel="external">/estimate</a></li>
<li><a href="http://oryx.io/apidocs/com/cloudera/oryx/app/serving/als/EstimateForAnonymous.html" target="_blank" rel="external">/estimateForAnonymous</a></li>
<li><a href="http://oryx.io/apidocs/com/cloudera/oryx/app/serving/als/Because.html" target="_blank" rel="external">/because</a></li>
<li><a href="http://oryx.io/apidocs/com/cloudera/oryx/app/serving/als/MostSurprising.html" target="_blank" rel="external">/mostSurprising</a></li>
<li><a href="http://oryx.io/apidocs/com/cloudera/oryx/app/serving/als/PopularRepresentativeItems.html" target="_blank" rel="external">/popularRepresentativeItems</a></li>
<li><a href="http://oryx.io/apidocs/com/cloudera/oryx/app/serving/als/MostActiveUsers.html" target="_blank" rel="external">/mostActiveUsers</a></li>
<li><a href="http://oryx.io/apidocs/com/cloudera/oryx/app/serving/als/MostPopularItems.html" target="_blank" rel="external">/mostPopularItems</a></li>
<li><a href="http://oryx.io/apidocs/com/cloudera/oryx/app/serving/als/MostActiveUsers.html" target="_blank" rel="external">/mostActiveUsers</a></li>
<li><a href="http://oryx.io/apidocs/com/cloudera/oryx/app/serving/als/AllItemIDs.html" target="_blank" rel="external">/item/allIDs</a></li>
<li><a href="http://oryx.io/apidocs/com/cloudera/oryx/app/serving/als/Ready.html" target="_blank" rel="external">/ready</a></li>
<li><a href="http://oryx.io/apidocs/com/cloudera/oryx/app/serving/als/Preference.html" target="_blank" rel="external">/pref</a></li>
<li><a href="http://oryx.io/apidocs/com/cloudera/oryx/app/serving/als/Ingest.html" target="_blank" rel="external">/ingest</a></li>
</ul>
<h3 id="u5206_u7C7B_/__u56DE_u5F52"><a href="#u5206_u7C7B_/__u56DE_u5F52" class="headerlink" title="分类 / 回归"></a>分类 / 回归</h3><ul>
<li><a href="http://oryx.io/apidocs/com/cloudera/oryx/app/serving/rdf/Predict.html" target="_blank" rel="external">/predict</a></li>
<li><a href="http://oryx.io/apidocs/com/cloudera/oryx/app/serving/rdf/ClassificationDistribution.html" target="_blank" rel="external">/classificationDistribution</a></li>
<li><a href="http://oryx.io/apidocs/com/cloudera/oryx/app/serving/rdf/Ready.html" target="_blank" rel="external">/ready</a></li>
<li><a href="http://oryx.io/apidocs/com/cloudera/oryx/app/serving/rdf/Train.html" target="_blank" rel="external">/train</a></li>
</ul>
<h3 id="u805A_u7C7B"><a href="#u805A_u7C7B" class="headerlink" title="聚类"></a>聚类</h3><ul>
<li><a href="http://oryx.io/apidocs/com/cloudera/oryx/app/serving/kmeans/Assign.html" target="_blank" rel="external">/assign</a></li>
<li><a href="http://oryx.io/apidocs/com/cloudera/oryx/app/serving/kmeans/DistanceToNearest.html" target="_blank" rel="external">/distanceToNearest</a></li>
<li><a href="http://oryx.io/apidocs/com/cloudera/oryx/app/serving/kmeans/Ready.html" target="_blank" rel="external">/ready</a></li>
<li><a href="http://oryx.io/apidocs/com/cloudera/oryx/app/serving/kmeans/Add.html" target="_blank" rel="external">/add</a></li>
</ul>
<h3 id="u914D_u7F6E"><a href="#u914D_u7F6E" class="headerlink" title="配置"></a>配置</h3><ul>
<li><a href="https://github.com/OryxProject/oryx/blob/master/app/conf/als-example.conf" target="_blank" rel="external">app/conf/als-example.conf</a></li>
<li><a href="https://github.com/OryxProject/oryx/blob/master/app/conf/kmeans-example.conf" target="_blank" rel="external">app/conf/kmeans-example.conf</a></li>
<li><a href="https://github.com/OryxProject/oryx/blob/master/app/conf/rdf-example.conf" target="_blank" rel="external">app/conf/rdf-example.conf</a></li>
</ul>
]]></content>
    <summary type="html">
    <![CDATA[<!--  -->
<!--  -->
<!--  _ __    __     __      ____    ___     ___   _____   __  __    ___     -->
<!-- /\`'__\/'__`\ /'__`\   /',__\  / _]]>
    </summary>
    
      <category term="Admin" scheme="http://reasonpun.com/tags/Admin/"/>
    
      <category term="Hadoop" scheme="http://reasonpun.com/tags/Hadoop/"/>
    
      <category term="Oryx2" scheme="http://reasonpun.com/tags/Oryx2/"/>
    
      <category term="Spark" scheme="http://reasonpun.com/tags/Spark/"/>
    
      <category term="Spark-Streaming" scheme="http://reasonpun.com/tags/Spark-Streaming/"/>
    
      <category term="documentation" scheme="http://reasonpun.com/tags/documentation/"/>
    
      <category term="数据挖掘" scheme="http://reasonpun.com/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
      <category term="实时计算" scheme="http://reasonpun.com/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Java关键字volatile]]></title>
    <link href="http://reasonpun.com/2016/03/08/Java%E5%85%B3%E9%94%AE%E5%AD%97volatile/"/>
    <id>http://reasonpun.com/2016/03/08/Java关键字volatile/</id>
    <published>2016-03-08T09:59:30.000Z</published>
    <updated>2016-03-15T07:19:35.000Z</updated>
    <content type="html"><![CDATA[<!--  -->
<!--  -->
<!--  _ __    __     __      ____    ___     ___   _____   __  __    ___     -->
<!-- /\`'__\/'__`\ /'__`\   /',__\  / __`\ /' _ `\/\ '__`\/\ \/\ \ /' _ `\   -->
<!-- \ \ \//\  __//\ \L\.\_/\__, `\/\ \L\ \/\ \/\ \ \ \L\ \ \ \_\ \/\ \/\ \  -->
<!--  \ \_\\ \____\ \__/.\_\/\____/\ \____/\ \_\ \_\ \ ,__/\ \____/\ \_\ \_\ -->
<!--   \/_/ \/____/\/__/\/_/\/___/  \/___/  \/_/\/_/\ \ \/  \/___/  \/_/\/_/ -->
<!--                                                 \ \_\                   -->
<!--                                                  \/_/                   -->
<!--  -->
<p>翻译自：<a href="http://tutorials.jenkov.com/java-concurrency/volatile.html" target="_blank" rel="external">http://tutorials.jenkov.com/java-concurrency/volatile.html</a></p>
<h3 id="Java_Volatile__u5173_u952E_u5B57"><a href="#Java_Volatile__u5173_u952E_u5B57" class="headerlink" title="Java Volatile 关键字"></a>Java Volatile 关键字</h3><p>Java关键字volatile标识一个变量“被存储在主内存中”。更准确的说法是：每次volatile变量会从主内存中读取，而不是从CPU缓存；每次volatile变量的写操作会写入主内存，而不仅仅是CPU缓存。</p>
<p>实际上，自Java5开始，volatile关键字保证的是volatile声明的变量都是写入和读取自主内存的。以下将会详细讲解。</p>
<h3 id="Java_volatile_u4FDD_u8BC1_u53EF_u89C1_u6027"><a href="#Java_volatile_u4FDD_u8BC1_u53EF_u89C1_u6027" class="headerlink" title="Java volatile保证可见性"></a>Java volatile保证可见性</h3><p>Java中volatile关键字保证了线程之间变量修改的可见性。这个可能会有点抽象，因此让我详细讲解下。</p>
<p>在多线程程序中，由于性能的原因，在操作非volatile变量的时候，每个线程会将变量自主内存拷贝🈯️CPU缓存。如果你的计算机是多核的，每个线程就会运行在不同的CPU上。也就是说，每个线程会拷贝变量到不同的CPU缓存中。见插图：</p>
<img src="/images/volatile/java-volatile-1.png" title="volatile">
<p>使用非volatile变量，当JVM自主内存读取数据到CPU缓存，或者自CPU缓存写入主内存时，是没有保证的。这可能会引起很多问题，接下来章节中会降到。</p>
<p>设想一种情况，多个线程可以访问类似如下的同一个共享对象，这个对象包含一个计数器变量：</p>
<figure class="highlight axapta"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SharedObject</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">int</span> counter = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>另一个设想，只有线程1改变计数变量，但是线程1和线程2都可能经常读取这个计数变量。</p>
<p>如果计数变量没有被声明为volatile，那么自CPU缓存回写到主内存的过程中，就无法保证这个计数变量的值了。也就是说，技术变量的值在通过CPU缓存写会主内存的过程中发生了改变。如图：</p>
<img src="/images/volatile/java-volatile-2.png" title="volatile 2">
<p>这个问题是因为线程并没有得到最新的值，另外那个线程并没有将最新的变量写回主内存，这就是所谓的“可见”问题。某一个线程的更新并没有被另外的线程获取。</p>
<p>通过声明计数变量为volatile，那么这个计数变量所有的更新都会立即被写回主内存。同时，所有的读操作都会直接通过主内存。如下既是如何声明计数变量为volatile类型：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> SharedObject &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">volatile</span> <span class="keyword">int</span> counter = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>声明一个变量为volatile由此可以保证了变量的写操作对于其他线程都是可见的。</p>
<h3 id="The_Java_volatile_Happens-Before_Guarantee"><a href="#The_Java_volatile_Happens-Before_Guarantee" class="headerlink" title="The Java volatile Happens-Before Guarantee"></a>The Java volatile Happens-Before Guarantee</h3><p>自Java5开始，volatile关键字保证的不止是变量的主内存读和写，实际上还包含：</p>
<ul>
<li><p>如果线程A写入一个volatile变量，此时线程B读取了相同的volatile变量，那么此时所有的变量在写入volatile变量前对线程A是可见的，同时也对线程B在读取了volatile变量后也是可见的。</p>
</li>
<li><p>volatile变量的读写指令不能促使JVM重新排序（只要JVM发现排序不会改变程序的行为，基于性能的原因，JVM会进行排序）。前后的指令会被排序，但是volatile得读写不会混入这些指令。无论如何，指令都会follow一个volatile变量的读写，并且保证这种操作发生在读或者写之后。</p>
</li>
</ul>
<p>这个需要更深入的解释。</p>
<p>当一个线程写入一个volatile变量时，并不止这个volatile变量自己写入了主内存。同时所有被线程修改的变量在写入volatile变量之前都会被回写到主内存。当一个线程读取一个volatile变量时，他会同时读取主内存中所有其他的变量，这些变量都是和volatile变量一同写入主内存的。</p>
<p>看例子：</p>
<figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Thread <span class="keyword">A</span>:</span><br><span class="line">    sharedObject.nonVolatile = 123<span class="comment">;</span></span><br><span class="line">    sharedObject.counter     = sharedObject.counter + 1<span class="comment">;</span></span><br><span class="line"></span><br><span class="line">Thread B:</span><br><span class="line">    int counter     = sharedObject.counter<span class="comment">;</span></span><br><span class="line">    int nonVolatile = sharedObject.nonVolatile<span class="comment">;</span></span><br></pre></td></tr></table></figure>
<p>因为线程A在写入volatile变量sharedObject.counter之前写入了非volatile变量sharedObject.nonVolatile，那么当线程A写入sharedObject.counter时，sharedObject.nonVolatile 和 sharedObject.counter 都会被写入主内存。</p>
<p>由于线程B开始的时候读取volatile类型的sharedObject.counter，那么sharedObject.counter and sharedObject.nonVolatile都会使用线程B自主内存读取到CPU缓存。等到线程B读取sharedObject.nonVolatile时，就会看到线程A写入的值。</p>
<p>开发者可能会使用这种扩展的可见性，保证了线程之间变量的可见性。只需要声明一个或者非常少的volatile变量替换掉每个变量都声明为volatile。如下是一个例子：</p>
<figure class="highlight cs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title">Exchanger</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Object   <span class="keyword">object</span>       = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">volatile</span> hasNewObject = <span class="keyword">false</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">put</span>(<span class="params">Object newObject</span>) </span>&#123;</span><br><span class="line">        <span class="keyword">while</span>(hasNewObject) &#123;</span><br><span class="line">            <span class="comment">//wait - do not overwrite existing new object</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">object</span> = newObject;</span><br><span class="line">        hasNewObject = <span class="keyword">true</span>; <span class="comment">//volatile write</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Object <span class="title">take</span>(<span class="params"></span>)</span>&#123;</span><br><span class="line">        <span class="keyword">while</span>(!hasNewObject)&#123; <span class="comment">//volatile read</span></span><br><span class="line">            <span class="comment">//wait - don't take old object (or null)</span></span><br><span class="line">        &#125;</span><br><span class="line">        Object obj = <span class="keyword">object</span>;</span><br><span class="line">        hasNewObject = <span class="keyword">false</span>; <span class="comment">//volatile write</span></span><br><span class="line">        <span class="keyword">return</span> obj;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>线程A通过调用put()持续的写入对象。线程B通过take()持续的获取对象。这个类只有在线程A调用put()和线程B调用take()时，通过使用volatile变量才能工作正常。</p>
<p>如果JVM在不改变排序指令的语义的基础上实现，那么JVM则会通过记录JAVA指令优化性能。</p>
<figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span>(hasNewObject) &#123;</span><br><span class="line">    <span class="regexp">//wait</span> - <span class="keyword">do</span> <span class="keyword">not</span> overwrite existing new object</span><br><span class="line">&#125;</span><br><span class="line">hasNewObject = <span class="keyword">true</span>; <span class="regexp">//volatile</span> write</span><br><span class="line">object = newObject;</span><br></pre></td></tr></table></figure>
<p>注意到volatile变量hasNewObject在被实际设置前已经被执行。对于JVM，这看上去完全合法。这两个写入变量的值相对于另外一个是独立的。</p>
<p>然而，排序执行的指令会有损对象变量的可见性。首先，线程B可能在线程A设置新值之前将hasNewObject设置为true。其次，甚至没法保证当新值写入对象后回写到主内存。</p>
<p>为了防止如上述情况的发生，volatile关键字采用“发生前保证”机制。”发生前保证“机制保证读和写volatile变量的指令不能被排序。前后指令可以被排序，但是volatile读或者写指令发生前后不能被排序。</p>
<p>举个例子：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">sharedObject.nonVolatile1 = <span class="number">123</span>;</span><br><span class="line">sharedObject.nonVolatile2 = <span class="number">456</span>;</span><br><span class="line">sharedObject.nonVolatile3 = <span class="number">789</span>;</span><br><span class="line"></span><br><span class="line">sharedObject.<span class="keyword">volatile</span>     = <span class="literal">true</span>; <span class="comment">//a volatile variable</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> someValue1 = sharedObject.nonVolatile4;</span><br><span class="line"><span class="keyword">int</span> someValue2 = sharedObject.nonVolatile5;</span><br><span class="line"><span class="keyword">int</span> someValue3 = sharedObject.nonVolatile6;</span><br></pre></td></tr></table></figure>
<p>JVM会排序前3个指令，只要他们在volatile写指令之前。</p>
<p>同样的，只要volatile写入质量发生在后三个指令之前，那么后三个指令就会被排序。在此之前，这三个指令中任何一个指令都不会被排序。<br>That is basically the meaning of the Java volatile happens before guarantee.</p>
<h3 id="volatile__u5E76_u4E0D_u603B_u662F_u9002_u7528_u7684"><a href="#volatile__u5E76_u4E0D_u603B_u662F_u9002_u7528_u7684" class="headerlink" title="volatile 并不总是适用的"></a>volatile 并不总是适用的</h3><p>即使volatile关键字保证了所有的volatile变量直接从主内存读取，所有的写操作都是直接写入主内存，但是存在声明了volatile而仍然不够的情况。</p>
<p>之前提过的，只有线程1写入共享的counter变量，声明为volatile的时候才能确保线程2总是能获取最新的写入值。</p>
<p>事实上，多线程甚至能写入一个共享的volatile变量，如果这个新写入的值不依赖于旧数据的话就将正确的值存入主内存。换句话说, 如果一个线程写入一个值到共享的volatile变量，并不会马上将读取的值用于下一个值得话。</p>
<p>一旦一个线程首次读取一个volatile变量的值，并且需要基于这个值生成一个新的共享volatile变量值，那么这个volatile变量就不能保证正确的可见性。读取volatile变量的值并且写入新值得很短的间隙，就会创建一个竞争的条件：多个线程会读取同一个volatile变量的值，并且生成一个新的值，当回写进主内存后就会覆盖掉其他的值。</p>
<p>多线程增加相同的计数器值得这种情况，准确的说对于volatile变量是不适用的。一下片段讲话详细讲述这种情况。</p>
<p>设想，如果线程1读取共享计数变量，他的值是0，并放入CPU的缓存，使其加1，但是这样的改变并不会回写如主内存。线程2此时可以自主内存读取相同的计数器变量，但是读到的值仍然是0，并将这个值放入了自己的CPU缓存。线程2也可以对其加1，并且可以回写到主内存。这种情况详见下图：</p>
<img src="/images/volatile/java-volatile-3.png" title="volatile 3">
<p>线程1和线程2此时并不是同步的。这个共享计数器的值应该是2，但是每个线程在自己的CPU缓存中存放的值却是1，在主内存中的值是0.这已经完全乱了！即使最后每个线程回写这个共享计数变量的值到主内存，这个值也是错误的。</p>
<h3 id="volatile_u5728_u4EC0_u4E48_u65F6_u5019_u662F_u9002_u7528_u7684_3F"><a href="#volatile_u5728_u4EC0_u4E48_u65F6_u5019_u662F_u9002_u7528_u7684_3F" class="headerlink" title="volatile在什么时候是适用的?"></a>volatile在什么时候是适用的?</h3><p>就像之前提到过的，如果是两个线程都需要读取和写入一个共享变量，那么此时适用volatile关键字是不适用的。那么就需要使用synchronized来保证读和写的原子性。读写volatile变量不会阻塞线程的读写。为了实现这一点，你必须使用synchronized关键字。</p>
<p>作为synchronized可供选择，需要采用java.util.concurrent包中的某一个。比如AtomicLong，AtomicReference或者其他的任意一个。</p>
<p>在这种情况下，只有一个线程读写volatile变量的值，其他线程只是读取变量，那么读取线程就可以通过volatile变量保证可以读取到最新的写入值。如果没有volatile变量，则这个过程就不能被保证。</p>
<p>volatile关键字可以被用在32和64位变量中。</p>
<h3 id="volatile__u6027_u80FD_u6CE8_u610F_u4E8B_u9879"><a href="#volatile__u6027_u80FD_u6CE8_u610F_u4E8B_u9879" class="headerlink" title="volatile 性能注意事项"></a>volatile 性能注意事项</h3><p>volatile变量的读写会触发变量在主内存中的读写。主内存中的读写会比CPU缓存消耗的代价高。使用volatile变量同样会阻止普通性能增强指令的排序。因此，只能在确实需要增加变量可见性基础上使用volatile变量。</p>
<h3 id="u53C2_u8003_u6587_u732E"><a href="#u53C2_u8003_u6587_u732E" class="headerlink" title="参考文献"></a>参考文献</h3><ul>
<li><a href="http://www.ibm.com/developerworks/cn/java/j-jtp06197.html" target="_blank" rel="external">http://www.ibm.com/developerworks/cn/java/j-jtp06197.html</a></li>
<li><a href="http://www.infoq.com/cn/articles/ftf-java-volatile" target="_blank" rel="external">http://www.infoq.com/cn/articles/ftf-java-volatile</a></li>
<li><a href="http://www.infoq.com/cn/articles/java-memory-model-4" target="_blank" rel="external">http://www.infoq.com/cn/articles/java-memory-model-4</a></li>
<li><a href="http://sakyone.iteye.com/blog/668091" target="_blank" rel="external">http://sakyone.iteye.com/blog/668091</a></li>
<li><a href="https://zh.wikipedia.org/wiki/Volatile%E5%8F%98%E9%87%8F" target="_blank" rel="external">https://zh.wikipedia.org/wiki/Volatile%E5%8F%98%E9%87%8F</a></li>
<li><a href="http://www.cnblogs.com/aigongsi/archive/2012/04/01/2429166.html" target="_blank" rel="external">http://www.cnblogs.com/aigongsi/archive/2012/04/01/2429166.html</a></li>
</ul>
]]></content>
    <summary type="html">
    <![CDATA[<!--  -->
<!--  -->
<!--  _ __    __     __      ____    ___     ___   _____   __  __    ___     -->
<!-- /\`'__\/'__`\ /'__`\   /',__\  / _]]>
    </summary>
    
      <category term="Android" scheme="http://reasonpun.com/tags/Android/"/>
    
      <category term="Android" scheme="http://reasonpun.com/categories/Android/"/>
    
      <category term="关键字" scheme="http://reasonpun.com/categories/Android/%E5%85%B3%E9%94%AE%E5%AD%97/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[OKHttp源码学习过程]]></title>
    <link href="http://reasonpun.com/2016/03/08/OKHttp%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%E8%BF%87%E7%A8%8B/"/>
    <id>http://reasonpun.com/2016/03/08/OKHttp源码学习过程/</id>
    <published>2016-03-08T09:52:23.000Z</published>
    <updated>2016-03-15T07:17:42.000Z</updated>
    <content type="html"><![CDATA[<!--  -->
<!--  -->
<!--  _ __    __     __      ____    ___     ___   _____   __  __    ___     -->
<!-- /\`'__\/'__`\ /'__`\   /',__\  / __`\ /' _ `\/\ '__`\/\ \/\ \ /' _ `\   -->
<!-- \ \ \//\  __//\ \L\.\_/\__, `\/\ \L\ \/\ \/\ \ \ \L\ \ \ \_\ \/\ \/\ \  -->
<!--  \ \_\\ \____\ \__/.\_\/\____/\ \____/\ \_\ \_\ \ ,__/\ \____/\ \_\ \_\ -->
<!--   \/_/ \/____/\/__/\/_/\/___/  \/___/  \/_/\/_/\ \ \/  \/___/  \/_/\/_/ -->
<!--                                                 \ \_\                   -->
<!--                                                  \/_/                   -->
<!--  -->
<h2 id="u5B66_u4E60OKHttp_u6E90_u7801_u7684_u8FC7_u7A0B"><a href="#u5B66_u4E60OKHttp_u6E90_u7801_u7684_u8FC7_u7A0B" class="headerlink" title="学习OKHttp源码的过程"></a>学习OKHttp源码的过程</h2><h3 id="u51C6_u5907"><a href="#u51C6_u5907" class="headerlink" title="准备"></a>准备</h3><p>可以在<a href="https://github.com/square/okhttp" target="_blank" rel="external">这里</a>下载OKHttp源代码</p>
<h3 id="u8D77_u6B65"><a href="#u8D77_u6B65" class="headerlink" title="起步"></a>起步</h3><h4 id="Request-java"><a href="#Request-java" class="headerlink" title="Request.java"></a>Request.java</h4><p>所在目录：/okhttp/okhttp/src/main/java/com/squareup/okhttp/Request.java</p>
<p>文件第38行~40行，遇到关键字 volatile</p>
<figure class="highlight aspectj"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">volatile</span> URL url; <span class="comment">// Lazily initialized.</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">volatile</span> URI uri; <span class="comment">// Lazily initialized.</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">volatile</span> CacheControl cacheControl; <span class="comment">// Lazily initialized.</span></span><br></pre></td></tr></table></figure>
<p>然后稍微看了下这个关键字的解释，可以移步<a href="http://reasonpun.com/2016/03/08/Java%E5%85%B3%E9%94%AE%E5%AD%97volatile/">这里</a></p>
<hr>
<p>未完待续</p>
<h3 id="u53C2_u8003_u6587_u732E"><a href="#u53C2_u8003_u6587_u732E" class="headerlink" title="参考文献"></a>参考文献</h3><ul>
<li><a href="http://frodoking.github.io/2015/03/12/android-okhttp/" target="_blank" rel="external">http://frodoking.github.io/2015/03/12/android-okhttp/</a></li>
</ul>
]]></content>
    <summary type="html">
    <![CDATA[<!--  -->
<!--  -->
<!--  _ __    __     __      ____    ___     ___   _____   __  __    ___     -->
<!-- /\`'__\/'__`\ /'__`\   /',__\  / _]]>
    </summary>
    
      <category term="Android" scheme="http://reasonpun.com/tags/Android/"/>
    
      <category term="Android" scheme="http://reasonpun.com/categories/Android/"/>
    
      <category term="源码" scheme="http://reasonpun.com/categories/Android/%E6%BA%90%E7%A0%81/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[正则表达式处理Nginx]]></title>
    <link href="http://reasonpun.com/2016/01/24/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%A4%84%E7%90%86Nginx/"/>
    <id>http://reasonpun.com/2016/01/24/正则表达式处理Nginx/</id>
    <published>2016-01-24T03:37:16.000Z</published>
    <updated>2016-01-25T03:30:56.000Z</updated>
    <content type="html"><![CDATA[<!--  -->
<!--  -->
<!--  _ __    __     __      ____    ___     ___   _____   __  __    ___     -->
<!-- /\`'__\/'__`\ /'__`\   /',__\  / __`\ /' _ `\/\ '__`\/\ \/\ \ /' _ `\   -->
<!-- \ \ \//\  __//\ \L\.\_/\__, `\/\ \L\ \/\ \/\ \ \ \L\ \ \ \_\ \/\ \/\ \  -->
<!--  \ \_\\ \____\ \__/.\_\/\____/\ \____/\ \_\ \_\ \ ,__/\ \____/\ \_\ \_\ -->
<!--   \/_/ \/____/\/__/\/_/\/___/  \/___/  \/_/\/_/\ \ \/  \/___/  \/_/\/_/ -->
<!--                                                 \ \_\                   -->
<!--                                                  \/_/                   -->
<!--  -->
<h1 id="Nginx__u65E5_u5FD7_u914D_u7F6E_u683C_u5F0F"><a href="#Nginx__u65E5_u5FD7_u914D_u7F6E_u683C_u5F0F" class="headerlink" title="Nginx 日志配置格式"></a>Nginx 日志配置格式</h1><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="title">log_format</span>  main</span><br><span class="line">        <span class="string">'[<span class="variable">$upstream_addr</span>] <span class="variable">$remote_addr</span> [<span class="variable">$time_local</span>] "<span class="variable">$request</span>" <span class="variable">$status</span> '</span></span><br><span class="line">        <span class="string">'"<span class="variable">$request_body</span>" <span class="variable">$body_bytes_sent</span> "<span class="variable">$http_referer</span>" "<span class="variable">$http_user_agent</span>" '</span></span><br><span class="line">        <span class="string">'RESP:<span class="variable">$upstream_response_time</span> '</span></span><br><span class="line">        <span class="string">'REQ:<span class="variable">$request_time</span>'</span>;</span><br></pre></td></tr></table></figure>
<h1 id="u6837_u4F8B"><a href="#u6837_u4F8B" class="headerlink" title="样例"></a>样例</h1><figure class="highlight perl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">192.168</span>.<span class="number">1.5</span>:<span class="number">80</span>] <span class="number">19.78</span>.<span class="number">22.51</span> [<span class="number">31</span>/Dec/<span class="number">2015</span>:<span class="number">13</span>:<span class="number">59</span>:<span class="number">02</span> +080<span class="number">0</span>] <span class="string">"POST /api/mbbb/dup_msg_send?pallow_dubbing=0&amp;partner_msgs_id=279&amp;roles_id=23&amp;score=6700&amp;section_id=512&amp;whole_audio=200.m4a&amp;d</span><br><span class="line">evice_id=112222f0fc3&amp;lang=zh-CN&amp;trigger=user&amp;user_id=516487&amp;v=ios_7.0.3 HTTP/1.1"</span> <span class="number">200</span> <span class="string">"audio_fragment=00280<span class="variable">%22</span><span class="variable">%3A</span><span class="variable">%7B</span><span class="variable">%22pitch</span><span class="variable">%22</span><span class="variable">%3A47</span><span class="variable">%2C</span><span class="variable">%22rhythm</span><span class="variable">%22</span><span class="variable">%3A95</span><span class="variable">%2C</span><span class="variable">%22tone</span><span class="variable">%22</span><span class="variable">%3A75</span><span class="variable">%7D</span><span class="variable">%2C</span><span class="variable">%226800278</span><span class="variable">%22</span><span class="variable">%3A</span><span class="variable">%7B</span><span class="variable">%22pitch</span><span class="variable">%22</span><span class="variable">%3A70</span><span class="variable">%2C</span><span class="variable">%22rhythm</span><span class="variable">%22</span><span class="variable">%3A90</span><span class="variable">%2</span><span class="variable">%7D</span><span class="variable">%2C</span><span class="variable">%226800276</span><span class="variable">%22</span></span><br><span class="line"><span class="variable">%3A</span><span class="variable">%7B</span><span class="variable">%22pitch</span><span class="variable">%22</span><span class="variable">%3A60</span><span class="variable">%2C</span><span class="variable">%22rhythm</span><span class="variable">%22</span><span class="variable">%3A82</span><span class="variable">%2C</span><span class="variable">%22tone</span><span class="variable">%22D</span>&amp;content=hhhhhh<span class="variable">%E5</span><span class="variable">%B0</span><span class="variable">%8F</span><span class="variable">%E5</span><span class="variable">%AB</span><span class="variable">%A9</span><span class="variable">%E8</span><span class="variable">%8D</span><span class="variable">%89</span>"</span> <span class="number">51</span> <span class="string">"-"</span> <span class="string">"paipao/7.0.3 (iPhone; iOS 9.2; Scale/2.00)"</span> RESP:<span class="number">0</span>.<span class="number">166</span> REQ:<span class="number">0</span>.<span class="number">167</span></span><br><span class="line">[<span class="number">192.168</span>.<span class="number">1.5</span>:<span class="number">80</span>, <span class="number">192.169</span>.<span class="number">1.33</span>:<span class="number">88</span>] <span class="number">60.12</span>.<span class="number">246.5</span> [<span class="number">31</span>/Dec/<span class="number">2015</span>:<span class="number">23</span>:<span class="number">59</span>:<span class="number">02</span> +080<span class="number">0</span>] <span class="string">"GET /api/mppb/notice/list?device_id=112233fe6a6d991f&amp;lang=zh-CN&amp;user_id=6120&amp;v=ios_7.0.3 HTTP/1.1"</span> <span class="number">200</span> <span class="string">"-"</span> <span class="number">54</span> <span class="string">"-"</span> <span class="string">"paipao/7.0.3 (iPhone; iOS 9.2; Scale/2.00)"</span> RESP:<span class="number">0</span>.<span class="number">006</span> REQ:<span class="number">0</span>.<span class="number">006</span></span><br></pre></td></tr></table></figure>
<h1 id="u6B63_u5219_u8868_u8FBE_u5F0F"><a href="#u6B63_u5219_u8868_u8FBE_u5F0F" class="headerlink" title="正则表达式"></a>正则表达式</h1><h2 id="Python_u7248_u672C"><a href="#Python_u7248_u672C" class="headerlink" title="Python版本"></a>Python版本</h2><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">p = re.compile(</span><br><span class="line"><span class="code">            r"\[\-?[\d.\:]*[\ \,]*?.*?\]\ [\d.\:]*\ \[(\d+)/(\w+)/(\d+)\:(\S+)\ [\S]+\]\ \"(\S+)\ (\S+)\ .*?\"\ (\d+)\ \"(.*?)\"\ (\d+)\ \"([^\"]*)\"\ \".*?\" .*?")</span></span><br><span class="line">m = re.findall(p, line)</span><br><span class="line">day = m[<span class="link_label">0</span>][<span class="link_reference">0</span>]</span><br><span class="line">month = m[<span class="link_label">0</span>][<span class="link_reference">1</span>]</span><br><span class="line">year = m[<span class="link_label">0</span>][<span class="link_reference">2</span>]</span><br><span class="line">ttime = m[<span class="link_label">0</span>][<span class="link_reference">3</span>]</span><br><span class="line">method = m[<span class="link_label">0</span>][<span class="link_reference">4</span>]</span><br><span class="line">request = m[<span class="link_label">0</span>][<span class="link_reference">5</span>]</span><br><span class="line">status = m[<span class="link_label">0</span>][<span class="link_reference">6</span>]</span><br></pre></td></tr></table></figure>
<h2 id="Scala_u7248_u672C"><a href="#Scala_u7248_u672C" class="headerlink" title="Scala版本"></a>Scala版本</h2><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val regex = new Regex( """<span class="command">\[</span><span class="command">\-</span>?<span class="special">[</span><span class="command">\d</span>.<span class="command">\:</span><span class="special">]</span>*<span class="special">[</span><span class="command">\ </span><span class="command">\,</span><span class="special">]</span>*?.*?<span class="command">\]</span><span class="command">\ </span><span class="special">[</span><span class="command">\d</span>.<span class="command">\:</span><span class="special">]</span>*<span class="command">\ </span><span class="command">\[</span>(<span class="command">\d</span>+)/(<span class="command">\w</span>+)/(<span class="command">\d</span>+)<span class="command">\:</span>(<span class="command">\S</span>+)<span class="command">\ </span><span class="special">[</span><span class="command">\S</span><span class="special">]</span>+<span class="command">\]</span><span class="command">\ </span><span class="command">\"</span>(<span class="command">\S</span>+)<span class="command">\ </span>(<span class="command">\S</span>+)<span class="command">\ </span>.*?<span class="command">\"</span><span class="command">\ </span>(<span class="command">\d</span>+)<span class="command">\ </span><span class="command">\"</span>(.*?)<span class="command">\"</span><span class="command">\ </span>(<span class="command">\d</span>+)<span class="command">\ </span><span class="command">\"</span>(<span class="special">[</span>^<span class="command">\"</span><span class="special">]</span>*)<span class="command">\"</span><span class="command">\ </span><span class="command">\"</span>.*?<span class="command">\"</span> .*?""")</span><br><span class="line">val regex(day, month, year, time, method, request, status, postData, bytes, refer) = line</span><br></pre></td></tr></table></figure>
<h1 id="u53C2_u8003_u6587_u732E"><a href="#u53C2_u8003_u6587_u732E" class="headerlink" title="参考文献"></a>参考文献</h1><ul>
<li><a href="http://www.jianshu.com/p/5d8c802be13d" target="_blank" rel="external">http://www.jianshu.com/p/5d8c802be13d</a></li>
<li><a href="https://segmentfault.com/a/1190000002727070" target="_blank" rel="external">https://segmentfault.com/a/1190000002727070</a></li>
<li><a href="http://desert3.iteye.com/blog/1001568" target="_blank" rel="external">http://desert3.iteye.com/blog/1001568</a></li>
<li><a href="http://stackoverflow.com/questions/996536/regex-in-python" target="_blank" rel="external">http://stackoverflow.com/questions/996536/regex-in-python</a></li>
</ul>
]]></content>
    <summary type="html">
    <![CDATA[<!--  -->
<!--  -->
<!--  _ __    __     __      ____    ___     ___   _____   __  __    ___     -->
<!-- /\`'__\/'__`\ /'__`\   /',__\  / _]]>
    </summary>
    
      <category term="Nginx" scheme="http://reasonpun.com/tags/Nginx/"/>
    
      <category term="日志" scheme="http://reasonpun.com/tags/%E6%97%A5%E5%BF%97/"/>
    
      <category term="正则" scheme="http://reasonpun.com/tags/%E6%AD%A3%E5%88%99/"/>
    
      <category term="Nginx" scheme="http://reasonpun.com/categories/Nginx/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[TAPL]]></title>
    <link href="http://reasonpun.com/2016/01/06/TAPL/"/>
    <id>http://reasonpun.com/2016/01/06/TAPL/</id>
    <published>2016-01-06T04:42:09.000Z</published>
    <updated>2016-01-25T03:17:11.000Z</updated>
    <content type="html"><![CDATA[<!--  -->
<!--  -->
<!--  _ __    __     __      ____    ___     ___   _____   __  __    ___     -->
<!-- /\`'__\/'__`\ /'__`\   /',__\  / __`\ /' _ `\/\ '__`\/\ \/\ \ /' _ `\   -->
<!-- \ \ \//\  __//\ \L\.\_/\__, `\/\ \L\ \/\ \/\ \ \ \L\ \ \ \_\ \/\ \/\ \  -->
<!--  \ \_\\ \____\ \__/.\_\/\____/\ \____/\ \_\ \_\ \ ,__/\ \____/\ \_\ \_\ -->
<!--   \/_/ \/____/\/__/\/_/\/___/  \/___/  \/_/\/_/\ \ \/  \/___/  \/_/\/_/ -->
<!--                                                 \ \_\                   -->
<!--                                                  \/_/                   -->
<!--  -->
<p>TAPL The awk programming language, <a href="https://reasonpun.gitbooks.io/tapl/content/index.html" target="_blank" rel="external">see git book</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<!--  -->
<!--  -->
<!--  _ __    __     __      ____    ___     ___   _____   __  __    ___     -->
<!-- /\`'__\/'__`\ /'__`\   /',__\  / _]]>
    </summary>
    
      <category term="awk" scheme="http://reasonpun.com/tags/awk/"/>
    
      <category term="programming" scheme="http://reasonpun.com/tags/programming/"/>
    
      <category term="AWK" scheme="http://reasonpun.com/categories/AWK/"/>
    
      <category term="Linux" scheme="http://reasonpun.com/categories/AWK/Linux/"/>
    
      <category term="Ebook" scheme="http://reasonpun.com/categories/AWK/Linux/Ebook/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Spark优化]]></title>
    <link href="http://reasonpun.com/2016/01/04/Spark%E4%BC%98%E5%8C%96/"/>
    <id>http://reasonpun.com/2016/01/04/Spark优化/</id>
    <published>2016-01-04T06:13:35.000Z</published>
    <updated>2016-01-18T03:07:41.000Z</updated>
    <content type="html"><![CDATA[<!--  -->
<!--  -->
<!--  _ __    __     __      ____    ___     ___   _____   __  __    ___     -->
<!-- /\`'__\/'__`\ /'__`\   /',__\  / __`\ /' _ `\/\ '__`\/\ \/\ \ /' _ `\   -->
<!-- \ \ \//\  __//\ \L\.\_/\__, `\/\ \L\ \/\ \/\ \ \ \L\ \ \ \_\ \/\ \/\ \  -->
<!--  \ \_\\ \____\ \__/.\_\/\____/\ \____/\ \_\ \_\ \ ,__/\ \____/\ \_\ \_\ -->
<!--   \/_/ \/____/\/__/\/_/\/___/  \/___/  \/_/\/_/\ \ \/  \/___/  \/_/\/_/ -->
<!--                                                 \ \_\                   -->
<!--                                                  \/_/                   -->
<!--  -->
<h2 id="Spark__u4F18_u5316"><a href="#Spark__u4F18_u5316" class="headerlink" title="Spark 优化"></a>Spark 优化</h2><p>由于Spark内存计算特性，Spark程序会由集群上的如下因素决定其性能</p>
<ul>
<li>CPU</li>
<li>网络带宽</li>
<li>内存</li>
</ul>
<p>通常来说，如果配置适当的内存，那么瓶颈就是带宽。但是有些时候，有需要做些优化，比如以序列化的形式存储RDD，从而降低内存的占用。</p>
<p>此会从两方面分析</p>
<ul>
<li>数据序列化</li>
<li>内存优化</li>
</ul>
<h3 id="Data_Serialization"><a href="#Data_Serialization" class="headerlink" title="Data Serialization"></a>Data Serialization</h3><p>序列化在分布式计算程序中占有非常重要的地位。迟缓的序列化格式，或者消费一个超大的字节数据，都会大大的减缓计算速度。所以，第一件事应该先尝试优化Spark程序。Spark试图达到在易用性（允许你定义任何的Java类型）和性能两方面达到一种平衡，并且提供两种序列化库：</p>
<ul>
<li><p>Java序列化：缺省情况下，Spark使用Java的ObjectOutputStream框架序列化对象，<br>从而可以和任何实现了java.io.Serializable接口的类一起玩耍。也可以更紧密的控制扩展了java.io.Externalizable的序列化的性能。Java的序列化是非常丰富的，但是速度奇慢，并且会导致很多类产生大量的序列化格式。</p>
</li>
<li><p>Kryo序列化：Spark也可以使用Kryo库（Version2）非常迅速的序列化对象。Kryo比Java序列化快很多并且更大的压缩率（通常是10x），但是并不支持所有的类型，另外也需要你在程序中注册类以获得最好的性能。</p>
</li>
</ul>
<p>你可以选择通过设置SparkConf或者调用conf.set(“spark.serializer”, “org.apache.spark.serializer.KryoSerializer”)来使用Kryo初始化Job。这样设置的话，不仅可以配置通过worker节点间shuffling数据，还能将RDD序列化到磁盘上。Kryo不是缺省配置的原因是由于自定义注册的要求决定的，同时我们也想在网络密集型应用中使用它。</p>
<p>Spark automatically includes Kryo serializers for the many commonly-used core Scala classes covered in the AllScalaRegistrar from the Twitter chill library.</p>
<p>使用Kryo注册自定义类，需要使用registerKryoClasses方法。</p>
  <figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val conf = new <span class="function"><span class="title">SparkConf</span><span class="params">()</span></span>.<span class="function"><span class="title">setMaster</span><span class="params">(...)</span></span>.<span class="function"><span class="title">setAppName</span><span class="params">(...)</span></span></span><br><span class="line">conf.<span class="function"><span class="title">registerKryoClasses</span><span class="params">(Array(classOf[MyClass1], classOf[MyClass2])</span></span>)</span><br><span class="line">val sc = new <span class="function"><span class="title">SparkContext</span><span class="params">(conf)</span></span></span><br></pre></td></tr></table></figure>
<p>Kryo文档描述了更多高级的注册选项，比如添加自定义序列代码。</p>
<p>如果你的对象非常大，则需要增加spark.kryoserializer.buffer配置参数。这个值缺省为2，但是这个值需要足够大足以保存序列号的对象。</p>
<p>最后，如果你没有注册自定义类，Kryo仍然会起作用，但是它不得不存储每个对象的全部类，这样是非常浪费的。</p>
<h3 id="u5185_u5B58_u4F18_u5316"><a href="#u5185_u5B58_u4F18_u5316" class="headerlink" title="内存优化"></a>内存优化</h3><p>内存优化的方法有3个方面需要考虑的：你的对象使用的总内存量（你可能希望全部数据都放到内存里），对象存取成本，GC的开销（如果你有非常高的对象交换频率）</p>
<p>默认情况下，Java对象可以被快速的访问，但是会轻易的耗尽它们字段中比“raw”数据多2到5倍的空间的因子。这取决于多个因素：</p>
<ul>
<li>每个独立的Java对象包含一个“对象头”，16字节长度并包含诸如只想它的类的一些信息。<br>对于一个非常小的数据来说（比如Int），这个可能比数据本身都要大一些。</li>
<li>Java字符串有大约40字节的对象头，比“raw”数据要多（因为它们按照字符串数组的形式保存，并且还包含扩展数据，比如说数据的长度），<br>由于采用了UTF-16编码格式，所以在字符串内部，存储一个字符需要占用2个字节的空间。因此10个字符就可以轻易的消耗掉60字节空间。</li>
<li>普通的集合对象，比如说HashMap和LinkedList，使用的是链式数据结构，对于每个实体都存在一个“wrapper”对象（比如 Map.Entry）。这个对象不仅包含头，链表中还有指向下一个对象的指针（通常需要8个字节）。</li>
<li>私有类型的集合经常以“装箱”对象的形式存储，比如java.lang.Integer。</li>
</ul>
<p>这一章讨论的是如何确定对象的内存占用情况，和改进的方法－不止保护改变你的数据结构，另外还需要以一种序列号的形式存储数据。然后我们就可以覆盖到优化Spark缓存和Java GC的知识了。</p>
<h4 id="u641E_u6E05_u695A_u5185_u5B58_u6D88_u8017"><a href="#u641E_u6E05_u695A_u5185_u5B58_u6D88_u8017" class="headerlink" title="搞清楚内存消耗"></a>搞清楚内存消耗</h4><p>测试一个数据集消耗内存的总量最好的方式创建一个RDD，并放到缓冲中，通过web页面查看存储情况。这个页面的内容可以显示RDD到底占有了多少内存。</p>
<p>估算一个分区数据消耗的内存，可以使用SizeEstimator’s estimate的方法，这是一种非常有用的方式去试验不同的数据结构去减少内存的使用，即可以确定广播变量占用空间可以消耗每个可执行堆的情况。</p>
<h4 id="u4F18_u5316_u6570_u636E_u7ED3_u6784"><a href="#u4F18_u5316_u6570_u636E_u7ED3_u6784" class="headerlink" title="优化数据结构"></a>优化数据结构</h4><p>首选的降低内存消耗的方法是避免使用具有Java的特性导致的开销，比如基于指针的数据结构核包装类。有多种方式可以做到：</p>
<ul>
<li>设计你的数据结构以提升对象数组和原始类型，用来替换标准的Java或者Scala的集合类（比如：HashMap）。<br>fastutil类库为原始类型提供了适当的集合类型，可以兼容Java的标准库。</li>
<li>尽量避免包含大量小对象核指针的嵌套数据结构。</li>
<li>考虑使用数字类型的ID或者枚举对象来替代字符串行的key。</li>
<li>如果你的RAM不足32GB，可以通过设置JVM的-XX:+UseCompressedOops参数，修改指针默认占用8字节为4个字节。也可以讲这些参数加到spaspark-env.sh中。</li>
</ul>
<h4 id="u5E8F_u5217_u5316_u7684RDD_u5B58_u50A8"><a href="#u5E8F_u5217_u5316_u7684RDD_u5B58_u50A8" class="headerlink" title="序列化的RDD存储"></a>序列化的RDD存储</h4><p>通过以上的优化，你的对象仍然很大从而影响到高效的存储的话，一种更加简单的减少内存使用的方法是通过RDD的持久化API序列化StorageLevels，从而使他们以序列化的方式存储，比如MEMORY_ONLY_SER。Spark这时就可以将每个RDD作为一个大字节数组分区存储。唯一的不足是，序列化存储的数据每次读取的时候会很慢，这个取决于每个对象的反序列化（on the fly）。我们强烈建议使用Kryo作为缓存序列化数据的方法，这样可以比Java序列化占用更少的空间（甚至是原始的Java对象）</p>
<h4 id="GC_u4F18_u5316"><a href="#GC_u4F18_u5316" class="headerlink" title="GC优化"></a>GC优化</h4><p>JVM 的GC可能会是一个问题，当你的程序在存储一个RDD方面存在一个很大的『churn』。（他不会是一个大问题，如果只是每次读取一个RDD，并多次操作）当Java需要逐渐的用新对象替换旧对象时，GC就会追踪你的所有Java对象，找到并替换掉。这里的重点指出的，GC的执行成本是与Java对象的个数成正比的，因此使用更少对象的数据结构可以大大降低GC的成本（比如一个Int类型的数组替换LinkedList的数组）。更好的方法是持久化被序列化之后的对象，如上述，在每个RDD分区里只会存在一个对象（一个字节数组）。在尝试其他技术之前，如果GC是一个问题，那么第一件事就是尝试序列化的缓存。</p>
<p>你的任务的占用的活动内存核缓存在你节点上的RDD之间的干扰也会导致GC出现问题（需要执行任务所需的内容总量）。我们将会讨论怎样控制分配给RDD缓存的空间以减少这种影响。</p>
<h5 id="u8861_u91CFGC_u7684_u5F71_u54CD"><a href="#u8861_u91CFGC_u7684_u5F71_u54CD" class="headerlink" title="衡量GC的影响"></a>衡量GC的影响</h5><p>GC优化的第一步需要先手机统计数据：GC发生的频率和发费的时间。这个可以通过添加设置java参数-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps实现。（详情见传递Java参数到Spark任务的指南）下一次你的Spark任务执行时，你就可以在每个worker的日志中看到关于GC事件的信息。注意，这些日志时存放到worker节点上的，并不是在driver 程序上。</p>
<h5 id="u7F13_u51B2_u5927_u5C0F_u4F18_u5316"><a href="#u7F13_u51B2_u5927_u5C0F_u4F18_u5316" class="headerlink" title="缓冲大小优化"></a>缓冲大小优化</h5><p>GC一个重要的配置参数是分配给缓存RDD使用的内存总量。缺省情况下，Spark会使用配置给executor memory (spark.executor.memory) 60%的内存量缓冲RDD。也就是说40%的内存是用于在任务执行过程中任何其它的对象。</p>
<p>当你的任务速度缓慢，并且发现JVM GC频率特别频繁或者出现内存溢出的问题时，降低这个值会对降低内存消耗提供帮助。修改这个值可以通过设置spark.storage.memoryFraction，比如修改成50%，则可以conf.set(“spark.storage.memoryFraction”, “0.5”)。结合使用序列化缓存，使用更少的缓存就可以充分的减轻大部分的GC问题。</p>
<h5 id="u9AD8_u7EA7GC_u4F18_u5316"><a href="#u9AD8_u7EA7GC_u4F18_u5316" class="headerlink" title="高级GC优化"></a>高级GC优化</h5><p>更进一步的GC优化，我们首先需要理解一些基本的JVM管理内存管理知识：</p>
<ul>
<li>Java堆空间被划分为两个部分Young 和 Old。Young generation用来保存短周期的对象，同时Old generation是用来保持长周期对象。</li>
<li>Young generation被进一步划分为3部分：Eden, Survivor1, Survivor2。</li>
<li>一个简单的关于垃圾回收程序的描述：当Eden满了的时候，a minor GC is run on Eden and objects that are alive from Eden and Survivor1 are copied to Survivor2。Survivor部分是可以交换的。如果一个对象已经旧了或者Survivor2满了，他就会被移动到旧的部分。最后，当旧的部分接近满的时候，一个满的GC就会被唤起。</li>
</ul>
<p>Spark中GC优化的目标是确保只有需要长久存在的RDD才会被存储在Old generation，Young generation有足够的空间处处短周期的对象。这就可以帮助避免full GCs在任务执行过程中收集临时任务。这里的一些步骤可能会比较有用：</p>
<ul>
<li>通过检查GC的状态检查是否存在多个垃圾回收。如果一个full GC在一个任务结束之前被多次唤醒，这就意味着没有足够的可用内存涌来执行任务。</li>
<li>In the GC stats that are printed，如果OldGen接近满的状态的话，就减少缓存的内存总量。这个可以通过设置spark.storage.memoryFraction property来达成。缓存更少的对象总比减慢任务执行速度要好的多！</li>
<li>如果存在很多的次要collections而不是很多的主要的GCs，给Eden分配更多的内存会有所帮助。你可以设置Eden稍微高些的内存给每个任务。如果Eden表示成E，那么可以通过设置Young generation的参数大小为-Xmn=4/3*E。（The scaling up by 4/3 is to account for space used by survivor regions as well.）</li>
<li>举个例子，如果你的任务正在从HDFS中读取数据，任务使用的内存被标记为数据的块大小。<br>注意，解压后的数据块大约是之前数据的2～3倍。因此我们希望有3或者4个任务的工作空间，并且HDFS块的大小是64M，我们就可以估算出Eden大小大约是4<em>3</em>64MB。</li>
<li>通过修改新的设置来监测垃圾回收的频率和时间</li>
</ul>
<p>我的经验得出GC优化的改进取决于你的程序和可用的内存总量。在网上有很多优化选项的描述，但是再进一步，管理full GC的频率可以有助于降低顶层限制。</p>
<h3 id="u5176_u5B83_u9700_u8981_u8003_u8651_u7684_u4E8B_u60C5"><a href="#u5176_u5B83_u9700_u8981_u8003_u8651_u7684_u4E8B_u60C5" class="headerlink" title="其它需要考虑的事情"></a>其它需要考虑的事情</h3><h4 id="u5E76_u884C_u7684_u6C34_u5E73"><a href="#u5E76_u884C_u7684_u6C34_u5E73" class="headerlink" title="并行的水平"></a>并行的水平</h4><p>节点不会被充分利用，除非喂每个操作设置了足够高的并行水平。Spark会自动设置“map”任务的数量执行每个文件以和他的大小保持一致（尽管你可以通过SparkContext.textFile的可选参数控制），并且对于分布式的“reduce”操作，比如groupByKey 和 reduceByKey，会使用最大的父RDD的分区数。你可以传递并行的level作为第二个参数，活着设置spark.default.parallelism配置属性改变默认值。通常情况下，我们建议在节点上每个CPU内核执行2～3个任务。</p>
<h4 id="u964D_u4F4E_u4EFB_u52A1_u7684_u5185_u5B58_u4F7F_u7528"><a href="#u964D_u4F4E_u4EFB_u52A1_u7684_u5185_u5B58_u4F7F_u7528" class="headerlink" title="降低任务的内存使用"></a>降低任务的内存使用</h4><p>有时，你可能得到OutOfMemoryError错误，但是这并不是因为你的Rdd不适合你的内存，而是因为任务中的其中一个的设置，比如其中一个reduce任务执行groupByKey时太大。Spark的shuffle操作（sortByKey, groupByKey, reduceByKey, join等）会创建一个哈希表并且每个任务会执行grouping操作，这个通常情况下也会很大。此时最简单的解决办法是增加并行的水平，以降低任务的输入集大小。Spark可以高效的支持任务，因为它可以重复利用一个executor JVM，并且有很低的任务加载消耗，所以你可以安全的增加并发水平，甚至超过你节点上核心的数目。</p>
<h4 id="u5E7F_u64AD_u5927_u53D8_u91CF"><a href="#u5E7F_u64AD_u5927_u53D8_u91CF" class="headerlink" title="广播大变量"></a>广播大变量</h4><p>使用SparkContext上可用的广播功能可以明显的减少每个序列化任务的大小，和通过节点加载任务的消耗。如果你的任务使用来自driver程序的大的对象（比如：静态查找表），试着转化成一个广播变量。Spark打印出主节点上每个任务的序列化后的大小，从而你可以通过这个决定你的任务是否过大；通常的任务大于20KB就值得优化了。</p>
<h4 id="u6570_u636E_u7684_u4F4D_u7F6E"><a href="#u6570_u636E_u7684_u4F4D_u7F6E" class="headerlink" title="数据的位置"></a>数据的位置</h4><p>数据的位置可能是影响Spark作业的最主要的一个影响因素。如果数据和代码被放置在一起的话，计算速度会明显加快。但是如果代码和数据是分开的，不同的两块数据会被移动到一起。特别是，传输序列化的代码比数据块要快很多，这是因为代码的大小比数据小很多。Spark构建调度的主要原则就是由数据的位置决定的。</p>
<p>数据的位置到底距离操作他的代码多近的距离才合适呢？这里存在多个级别（自近及远）：</p>
<ul>
<li>PROCESS_LOCAL  数据和代码在同一个JVM中，这种方式是最好的一种情况</li>
<li>NODE_LOCAL 数据在同一个节点。比如说在HDFS的同一个节点，或者相同节点的不同executor。<br>这种方式比PROCESS_LOCAL稍微慢些，因为这种情况下，数据需要在不同进程中传输</li>
<li>NO_PREF data is accessed equally quickly from anywhere and has no locality preference</li>
<li>RACK_LOCAL 数据在同一个服务器机架。数据在同一个机架的不同的服务器上的话，数据需要通过网络进行传输，typically through a single switch</li>
<li>ANY 数据分布在网络的各个地方，并且不在同一个机架</li>
</ul>
<p>Spark更倾向于调度最好的locality级别的所有任务，但是这并不是经常发生的。在这种情况下，不在处理数据的任何空闲executor，Spark都会选择更低的locality级别。这有两种选择：</p>
<ul>
<li>等待同一个服务器上的任务，直到对应的CPU闲下来才执行</li>
<li>立即执行一个新任务，并通过移动数据到更远的节点执行</li>
</ul>
<p>Spark通常会稍微等一下，以等待CPU执行完毕。一旦超时，他就会将远端的数据传递给空闲的CPU。不同级别的等待超时回退可以单独配置，或者在一个参数中集中修改；前往配置页查看spark.locality参数详情。如果你的任务are long and see poor locality，你也可以修改设置，但是缺省值是完全可以满足要求的。</p>
<h3 id="u603B_u7ED3"><a href="#u603B_u7ED3" class="headerlink" title="总结"></a>总结</h3><p>这是一个简短的介绍，其中指出了你可能需要了解的优化Spark应用的最主要的要点，数据序列化和内存优化。对于大多数的程序来说，选择Kryo序列化，并且以序列化形式保存数据可以解决大部分一般的性能问题。Feel free to ask on the Spark mailing list about other tuning best practices.</p>
]]></content>
    <summary type="html">
    <![CDATA[<!--  -->
<!--  -->
<!--  _ __    __     __      ____    ___     ___   _____   __  __    ___     -->
<!-- /\`'__\/'__`\ /'__`\   /',__\  / _]]>
    </summary>
    
      <category term="spark" scheme="http://reasonpun.com/tags/spark/"/>
    
      <category term="优化" scheme="http://reasonpun.com/tags/%E4%BC%98%E5%8C%96/"/>
    
      <category term="数据挖掘" scheme="http://reasonpun.com/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
      <category term="实时计算" scheme="http://reasonpun.com/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97/"/>
    
      <category term="优化" scheme="http://reasonpun.com/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97/%E4%BC%98%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Python关键字:yield]]></title>
    <link href="http://reasonpun.com/2016/01/02/Python%E5%85%B3%E9%94%AE%E5%AD%97-yield/"/>
    <id>http://reasonpun.com/2016/01/02/Python关键字-yield/</id>
    <published>2016-01-02T09:15:47.000Z</published>
    <updated>2016-01-04T06:13:14.000Z</updated>
    <content type="html"><![CDATA[<!--  -->
<!--  -->
<!--  _ __    __     __      ____    ___     ___   _____   __  __    ___     -->
<!-- /\`'__\/'__`\ /'__`\   /',__\  / __`\ /' _ `\/\ '__`\/\ \/\ \ /' _ `\   -->
<!-- \ \ \//\  __//\ \L\.\_/\__, `\/\ \L\ \/\ \/\ \ \ \L\ \ \ \_\ \/\ \/\ \  -->
<!--  \ \_\\ \____\ \__/.\_\/\____/\ \____/\ \_\ \_\ \ ,__/\ \____/\ \_\ \_\ -->
<!--   \/_/ \/____/\/__/\/_/\/___/  \/___/  \/_/\/_/\ \ \/  \/___/  \/_/\/_/ -->
<!--                                                 \ \_\                   -->
<!--                                                  \/_/                   -->
<!--  -->
<p>注：以下代码实验环境均为</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">➜  ~  python</span><br><span class="line">Python <span class="number">2.7</span><span class="number">.10</span> (<span class="keyword">default</span>, Oct <span class="number">23</span> <span class="number">2015</span>, <span class="number">18</span>:<span class="number">05</span>:<span class="number">06</span>)</span><br><span class="line">[GCC <span class="number">4.2</span><span class="number">.1</span> Compatible Apple LLVM <span class="number">7.0</span><span class="number">.0</span> (clang-<span class="number">700.0</span><span class="number">.59</span><span class="number">.5</span>)] on darwin</span><br><span class="line">Type <span class="string">"help"</span>, <span class="string">"copyright"</span>, <span class="string">"credits"</span> or <span class="string">"license"</span> <span class="keyword">for</span> more information.</span><br></pre></td></tr></table></figure>
<h3 id="Yield_u5173_u952E_u5B57"><a href="#Yield_u5173_u952E_u5B57" class="headerlink" title="Yield关键字"></a>Yield关键字</h3><p>理解yield关键字之前需要先明白什么是迭代。</p>
<h4 id="u53EF_u8FED_u4EE3_u5BF9_u8C61"><a href="#u53EF_u8FED_u4EE3_u5BF9_u8C61" class="headerlink" title="可迭代对象"></a>可迭代对象</h4><p>建立一个列表之后，可以逐项的读取列表中的元素，这就是一个可迭代的对象：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; <span class="built_in">list</span> = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]</span><br><span class="line">&gt;&gt;&gt; <span class="keyword">for</span> i in <span class="built_in">list</span>:</span><br><span class="line">...     print i</span><br><span class="line">...</span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="number">3</span></span><br><span class="line"><span class="number">4</span></span><br><span class="line">&gt;&gt;&gt;</span><br></pre></td></tr></table></figure>
<p>使用列表生成器建立一个列表，同样也是创建了一个可迭代的对象：</p>
<figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; <span class="keyword">list</span> = [x * 2 <span class="keyword">for</span> x <span class="keyword">in</span> <span class="keyword">range</span>(3)]</span><br><span class="line">&gt;&gt;&gt; <span class="keyword">list</span></span><br><span class="line">[0, 2, 4]</span><br><span class="line">&gt;&gt;&gt; <span class="keyword">for</span> i <span class="keyword">in</span> <span class="keyword">list</span>:</span><br><span class="line">...     <span class="keyword">print</span> <span class="literal">i</span></span><br><span class="line">...</span><br><span class="line">0</span><br><span class="line">2</span><br><span class="line">4</span><br><span class="line">&gt;&gt;&gt;</span><br></pre></td></tr></table></figure>
<p>可以使用for .. in .. 的方式处理：链表，字符串等，这就叫做一个迭代器。但是这样会将数据存放到内存中，如果数据量过大的话，就非常不适合了。</p>
<h4 id="u751F_u6210_u5668"><a href="#u751F_u6210_u5668" class="headerlink" title="生成器"></a>生成器</h4><p><a href="http://pyzh.readthedocs.org/en/latest/the-python-yield-keyword-explained.html" target="_blank" rel="external">生成器是可以迭代的，但是你 只可以读取它一次 ，因为它并不把所有的值放在内存中，它是实时地生成数据</a></p>
<figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; <span class="keyword">g</span> = (x * 2 <span class="keyword">for</span> x <span class="keyword">in</span> <span class="keyword">range</span>(3))</span><br><span class="line">&gt;&gt;&gt; <span class="keyword">g</span></span><br><span class="line">&lt;generator object &lt;genexpr&gt; at 0x10978ca00&gt;</span><br><span class="line">&gt;&gt;&gt; <span class="keyword">for</span> i <span class="keyword">in</span> <span class="keyword">g</span>:</span><br><span class="line">...     <span class="keyword">print</span> <span class="literal">i</span></span><br><span class="line">...</span><br><span class="line">0</span><br><span class="line">2</span><br><span class="line">4</span><br><span class="line">&gt;&gt;&gt; <span class="keyword">for</span> i <span class="keyword">in</span> <span class="keyword">g</span>:</span><br><span class="line">...     <span class="keyword">print</span> <span class="literal">i</span></span><br><span class="line">...</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">&gt;&gt;&gt;</span><br></pre></td></tr></table></figure>
<h4 id="yield_u5173_u952E_u5B57"><a href="#yield_u5173_u952E_u5B57" class="headerlink" title="yield关键字"></a>yield关键字</h4><p>yield 是一个类似return的关键字，但是这个函数返回的是生成器。</p>
<p>同时我们可以利用 isgeneratorfunction 判断一个特殊的 generator 函数</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; mg = <span class="function"><span class="title">createGenerator</span><span class="params">()</span></span></span><br><span class="line">&gt;&gt;&gt; print (mg)</span><br><span class="line">&lt;generator <span class="tag">object</span> createGenerator at <span class="number">0</span>x10978c9b0&gt;</span><br><span class="line">&gt;&gt;&gt; <span class="keyword">for</span> <span class="tag">i</span> <span class="keyword">in</span> mg:</span><br><span class="line">...     print <span class="tag">i</span></span><br><span class="line">...</span><br><span class="line"><span class="number">0</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="number">4</span></span><br><span class="line">&gt;&gt;&gt; from inspect import isgeneratorfunction</span><br><span class="line">&gt;&gt;&gt; <span class="function"><span class="title">isgeneratorfunction</span><span class="params">(mg)</span></span></span><br><span class="line">False</span><br><span class="line">&gt;&gt;&gt; <span class="function"><span class="title">isgeneratorfunction</span><span class="params">(createGenerator)</span></span></span><br><span class="line">True</span><br><span class="line">&gt;&gt;&gt;</span><br></pre></td></tr></table></figure>
<p>需要注意的是：<em>当你调用这个函数的时候，函数内部的代码并不立马执行</em> ，而是返回一个生成器对象。只有当使用for进行迭代的时候，函数内的代码才会执行。</p>
<h4 id="u63A7_u5236_u8D44_u6E90_u8BBF_u95EE"><a href="#u63A7_u5236_u8D44_u6E90_u8BBF_u95EE" class="headerlink" title="控制资源访问"></a>控制资源访问</h4><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="prompt">&gt;&gt;</span>&gt; <span class="class"><span class="keyword">class</span> <span class="title">Bank</span>():</span></span><br><span class="line">...     crisis = <span class="constant">False</span></span><br><span class="line">...     <span class="function"><span class="keyword">def</span> <span class="title">createAtm</span><span class="params">(<span class="keyword">self</span>)</span>:</span></span><br><span class="line">...             <span class="keyword">while</span> <span class="keyword">not</span> <span class="keyword">self</span>.<span class="symbol">crisis:</span></span><br><span class="line">...                     <span class="keyword">yield</span> <span class="string">"$100"</span></span><br><span class="line">...</span><br><span class="line"><span class="prompt">&gt;&gt;</span>&gt;</span><br><span class="line">&gt;&gt;&gt; bank = <span class="constant">Bank</span>()</span><br><span class="line"><span class="prompt">&gt;&gt;</span>&gt; atm = bank.createAtm()</span><br><span class="line"><span class="prompt">&gt;&gt;</span>&gt; print(atm.next())</span><br><span class="line"><span class="variable">$100</span></span><br><span class="line"><span class="prompt">&gt;&gt;</span>&gt; print(atm.next())</span><br><span class="line"><span class="variable">$100</span></span><br><span class="line"><span class="prompt">&gt;&gt;</span>&gt; print(atm.next())</span><br><span class="line"><span class="variable">$100</span></span><br><span class="line"><span class="prompt">&gt;&gt;</span>&gt; print(atm.next())</span><br><span class="line"><span class="variable">$100</span></span><br><span class="line"><span class="prompt">&gt;&gt;</span>&gt; atm.crisis = <span class="constant">True</span></span><br><span class="line"><span class="constant">Traceback</span> (most recent call last)<span class="symbol">:</span></span><br><span class="line">  <span class="constant">File</span> <span class="string">"&lt;stdin&gt;"</span>, line <span class="number">1</span>, <span class="keyword">in</span> &lt;<span class="class"><span class="keyword">module</span>&gt;</span></span><br><span class="line"><span class="constant">AttributeError</span><span class="symbol">:</span> <span class="string">'generator'</span> object has no attribute <span class="string">'crisis'</span></span><br><span class="line"><span class="prompt">&gt;&gt;</span>&gt; bank.crisis = <span class="constant">True</span></span><br><span class="line"><span class="prompt">&gt;&gt;</span>&gt; print (atm.next())</span><br><span class="line"><span class="constant">Traceback</span> (most recent call last)<span class="symbol">:</span></span><br><span class="line">  <span class="constant">File</span> <span class="string">"&lt;stdin&gt;"</span>, line <span class="number">1</span>, <span class="keyword">in</span> &lt;<span class="class"><span class="keyword">module</span>&gt;</span></span><br><span class="line"><span class="constant">StopIteration</span></span><br><span class="line"><span class="prompt">&gt;&gt;</span>&gt;</span><br></pre></td></tr></table></figure>
<p>如果不使用yield的话，这个类执行createAtm之后就会导致系统资源耗尽（或者说是无限循环）</p>
<h4 id="u4F7F_u7528yield_u53EF_u4EE5_u662F_u7A0B_u5E8F_u975E_u5E38_u4F18_u7F8E_uFF0C_u867D_u7136python_u7A0B_u5E8F_u672C_u8EAB_u5C31_u4F1A_u5F88_u4F18_u7F8E"><a href="#u4F7F_u7528yield_u53EF_u4EE5_u662F_u7A0B_u5E8F_u975E_u5E38_u4F18_u7F8E_uFF0C_u867D_u7136python_u7A0B_u5E8F_u672C_u8EAB_u5C31_u4F1A_u5F88_u4F18_u7F8E" class="headerlink" title="使用yield可以是程序非常优美，虽然python程序本身就会很优美"></a>使用yield可以是程序非常优美，虽然python程序本身就会很优美</h4><h5 id="u751F_u6210_u6590_u6CE2_u90A3_u5951_uFF08Fibonacci_uFF09_u6570_u5217"><a href="#u751F_u6210_u6590_u6CE2_u90A3_u5951_uFF08Fibonacci_uFF09_u6570_u5217" class="headerlink" title="生成斐波那契（Fibonacci）数列"></a>生成斐波那契（Fibonacci）数列</h5><p>许多初学者都会这么写</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; def <span class="function"><span class="title">fab</span><span class="params">(max)</span></span>:</span><br><span class="line">...     n, <span class="tag">a</span>, <span class="tag">b</span> = <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span></span><br><span class="line">...     while n &lt; max:</span><br><span class="line">...             print <span class="tag">b</span></span><br><span class="line">...             <span class="tag">a</span>, <span class="tag">b</span> = <span class="tag">b</span>, <span class="tag">a</span> + <span class="tag">b</span></span><br><span class="line">...             n = n + <span class="number">1</span></span><br><span class="line">...</span><br><span class="line">&gt;&gt;&gt; <span class="function"><span class="title">fab</span><span class="params">(<span class="number">5</span>)</span></span></span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="number">3</span></span><br><span class="line"><span class="number">5</span></span><br><span class="line">&gt;&gt;&gt;</span><br></pre></td></tr></table></figure>
<p>使用yield改写一个版本</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="prompt">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">fab</span><span class="params">(max)</span>:</span></span><br><span class="line"><span class="prompt">... </span>    n, a, b = <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span></span><br><span class="line"><span class="prompt">... </span>    <span class="keyword">while</span> n &lt; max:</span><br><span class="line"><span class="prompt">... </span>            <span class="keyword">yield</span> b</span><br><span class="line"><span class="prompt">... </span>            a, b = b, a + b</span><br><span class="line"><span class="prompt">... </span>            n = n + <span class="number">1</span></span><br><span class="line">...</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line"><span class="prompt">&gt;&gt;&gt; </span><span class="keyword">for</span> n <span class="keyword">in</span> fab(<span class="number">5</span>):</span><br><span class="line"><span class="prompt">... </span>    <span class="keyword">print</span> n</span><br><span class="line">...</span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="number">3</span></span><br><span class="line"><span class="number">5</span></span><br><span class="line">&gt;&gt;&gt;</span><br></pre></td></tr></table></figure>
<h4 id="u7B80_u5355_u63CF_u8FF0_u4E0B"><a href="#u7B80_u5355_u63CF_u8FF0_u4E0B" class="headerlink" title="简单描述下"></a>简单描述下</h4><p>for 语句在碰到生成器 generator 的时候，</p>
<p>调用</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">generator</span><span class="class">.__next__</span>()</span><br></pre></td></tr></table></figure>
<p>获取生成器的返回值。</p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="strong">__next__</span>()</span><br></pre></td></tr></table></figure>
<p>以Fibonacci为例：for每次调用，可以理解为执行了一次generator() 执行到 yield 的时候，生成器返回了n的值并停止。<br>这就像普通函数碰到 return 时一样，剩下的代码都被忽略了。</p>
<p>不同的地方在于，python 会记录这个停止的位置。 当再次执行generator()的时候，python 从这个停止位置开始执行而不是开头， 也就是说这次返回了1。再执行generator()，则返回2，当执行到返回5的时候，已经没有 yield 语句了， 就抛出了 StopIteration 。这和其他迭代器是类似的，当然在for中是不会抛出异常的。</p>
<h4 id="u8FDB_u9636"><a href="#u8FDB_u9636" class="headerlink" title="进阶"></a>进阶</h4><p>在<a href="https://www.python.org/dev/peps/pep-0342/" target="_blank" rel="external">PEP 342</a>中加入了将值传给生成器的支持。PEP 342加入了新的特性，能让生成器在单一语句中实现，生成一个值（像从前一样），接受一个值，或同时生成一个值并接受一个值。<br>我们用前面那个关于素数的函数来展示如何将一个值传给生成器。这一次，我们不再简单地生成比某个数大的素数，而是找出比某个数的等比级数大的最小素数（例如10， 我们要生成比10，100，1000，10000 … 大的最小素数）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="prompt">&gt;&gt;&gt; </span><span class="keyword">import</span> math</span><br><span class="line"><span class="prompt">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">getPrimes</span><span class="params">(number)</span>:</span></span><br><span class="line"><span class="prompt">... </span>    <span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line"><span class="prompt">... </span>            <span class="keyword">if</span> isPrime(number):</span><br><span class="line"><span class="prompt">&gt;&gt;&gt; </span>                    <span class="string">'''</span><br><span class="line"><span class="prompt">... </span>                    yield关键字返回number的值，</span><br><span class="line"><span class="prompt">... </span>                    而other = yield foo "返回foo的值，</span><br><span class="line"><span class="prompt">... </span>                    这个值返回给调用者的同时，将other的值也设置为那个值"。</span><br><span class="line"><span class="prompt">... </span>                    可以通过send方法来将一个值”发送“给生成器。</span><br><span class="line"><span class="prompt">&gt;&gt;&gt; </span>                    '''</span></span><br><span class="line"><span class="prompt">... </span>                    number = <span class="keyword">yield</span> number</span><br><span class="line"><span class="prompt">... </span>            number += <span class="number">1</span></span><br><span class="line">...</span><br><span class="line"><span class="prompt">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">printSuccessivePrimes</span><span class="params">(iterations, base=<span class="number">10</span>)</span>:</span></span><br><span class="line"><span class="prompt">... </span>    printGenerator = getPrimes(base)</span><br><span class="line"><span class="prompt">&gt;&gt;&gt; </span>    <span class="string">'''</span><br><span class="line"><span class="prompt">... </span>    用send来“启动”一个生成器时（就是从生成器函数的第一行代码执行到第一个yield语句的位置），必须发送None。因为此时生成器还没有走到第一个yield语句，如果send一个真实的值，这时是没有人去“接收”它的。一旦生成器启动了，我们就可以像上面那样发送数据了</span><br><span class="line"><span class="prompt">&gt;&gt;&gt; </span>    '''</span></span><br><span class="line"><span class="prompt">... </span>    printGenerator.send(<span class="keyword">None</span>)</span><br><span class="line"><span class="prompt">... </span>    <span class="keyword">for</span> power <span class="keyword">in</span> range(iterations):</span><br><span class="line"><span class="prompt">&gt;&gt;&gt; </span>            <span class="string">'''</span><br><span class="line"><span class="prompt">... </span>            打印的是generator.send的结果，send在发送数据给生成器的同时还返回生成器通过yield生成的值</span><br><span class="line"><span class="prompt">&gt;&gt;&gt; </span>            '''</span></span><br><span class="line"><span class="prompt">... </span>            print(printGenerator.send(base ** power))</span><br><span class="line">...</span><br><span class="line"><span class="prompt">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">isPrime</span><span class="params">(number)</span>:</span></span><br><span class="line"><span class="prompt">... </span>    <span class="keyword">if</span> number &gt; <span class="number">1</span>:</span><br><span class="line"><span class="prompt">... </span>            <span class="keyword">if</span> number == <span class="number">2</span>:</span><br><span class="line"><span class="prompt">... </span>                    <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line"><span class="prompt">... </span>            <span class="keyword">if</span> number % <span class="number">2</span> == <span class="number">0</span>:</span><br><span class="line"><span class="prompt">... </span>                    <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line"><span class="prompt">... </span>            <span class="keyword">for</span> current <span class="keyword">in</span> range(<span class="number">3</span>, int(math.sqrt(number) + <span class="number">1</span>), <span class="number">2</span>):</span><br><span class="line"><span class="prompt">... </span>                    <span class="keyword">if</span> number % current == <span class="number">0</span>:</span><br><span class="line"><span class="prompt">... </span>                            <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line"><span class="prompt">... </span>            <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line"><span class="prompt">... </span>    <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line">...</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line"><span class="prompt">&gt;&gt;&gt; </span>printSuccessivePrimes(<span class="number">10</span>)</span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="number">11</span></span><br><span class="line"><span class="number">101</span></span><br><span class="line"><span class="number">1009</span></span><br><span class="line"><span class="number">10007</span></span><br><span class="line"><span class="number">100003</span></span><br><span class="line"><span class="number">1000003</span></span><br><span class="line"><span class="number">10000019</span></span><br><span class="line"><span class="number">100000007</span></span><br><span class="line"><span class="number">1000000007</span></span><br><span class="line">&gt;&gt;&gt;</span><br></pre></td></tr></table></figure>
<p>Python 使用yield的关键思想</p>
<ul>
<li>generator是用来产生一系列值的</li>
<li>yield则像是generator函数的返回结果</li>
<li>yield唯一所做的另一件事就是保存一个generator函数的状态</li>
<li>generator就是一个特殊类型的迭代器（iterator）</li>
<li>和迭代器相似，我们可以通过使用next()来从generator中获取下一个值</li>
<li>通过隐式地调用next()来忽略一些值</li>
</ul>
<h3 id="u53C2_u8003_u6587_u732E"><a href="#u53C2_u8003_u6587_u732E" class="headerlink" title="参考文献"></a>参考文献</h3><ul>
<li><a href="http://pyzh.readthedocs.org/en/latest/the-python-yield-keyword-explained.html" target="_blank" rel="external">http://pyzh.readthedocs.org/en/latest/the-python-yield-keyword-explained.html</a></li>
<li><a href="http://www.dabeaz.com/coroutines/index.html" target="_blank" rel="external">http://www.dabeaz.com/coroutines/index.html</a></li>
<li><a href="https://docs.python.org/2/tutorial/classes.html" target="_blank" rel="external">https://docs.python.org/2/tutorial/classes.html</a></li>
<li><a href="http://stackoverflow.com/questions/231767/what-does-the-yield-keyword-do-in-python" target="_blank" rel="external">http://stackoverflow.com/questions/231767/what-does-the-yield-keyword-do-in-python</a></li>
<li><a href="http://www.pydanny.com/python-yields-are-fun.html" target="_blank" rel="external">http://www.pydanny.com/python-yields-are-fun.html</a></li>
<li><a href="http://dhcmrlchtdj.github.io/sia/post/2012-11-20/python_yield.html" target="_blank" rel="external">http://dhcmrlchtdj.github.io/sia/post/2012-11-20/python_yield.html</a></li>
<li><a href="http://www.oschina.net/translate/improve-your-python-yield-and-generators-explained" target="_blank" rel="external">http://www.oschina.net/translate/improve-your-python-yield-and-generators-explained</a></li>
<li><a href="http://www.pythonclub.org/python-basic/yield" target="_blank" rel="external">http://www.pythonclub.org/python-basic/yield</a></li>
<li><a href="https://www.ibm.com/developerworks/cn/opensource/os-cn-python-yield/" target="_blank" rel="external">https://www.ibm.com/developerworks/cn/opensource/os-cn-python-yield/</a></li>
<li><a href="https://www.jeffknupp.com/blog/2013/04/07/improve-your-python-yield-and-generators-explained/" target="_blank" rel="external">https://www.jeffknupp.com/blog/2013/04/07/improve-your-python-yield-and-generators-explained/</a></li>
<li><a href="http://pythontips.com/2013/09/29/the-python-yield-keyword-explained/" target="_blank" rel="external">http://pythontips.com/2013/09/29/the-python-yield-keyword-explained/</a></li>
<li><a href="http://blog.jobbole.com/28506/" target="_blank" rel="external">http://blog.jobbole.com/28506/</a></li>
</ul>
]]></content>
    <summary type="html">
    <![CDATA[<!--  -->
<!--  -->
<!--  _ __    __     __      ____    ___     ___   _____   __  __    ___     -->
<!-- /\`'__\/'__`\ /'__`\   /',__\  / _]]>
    </summary>
    
      <category term="Python" scheme="http://reasonpun.com/tags/Python/"/>
    
      <category term="Python" scheme="http://reasonpun.com/categories/Python/"/>
    
      <category term="学习" scheme="http://reasonpun.com/categories/Python/%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Oryx2 管理员文档]]></title>
    <link href="http://reasonpun.com/2015/12/21/Oryx2-Admin-Docs/"/>
    <id>http://reasonpun.com/2015/12/21/Oryx2-Admin-Docs/</id>
    <published>2015-12-21T08:53:46.000Z</published>
    <updated>2016-01-25T03:17:11.000Z</updated>
    <content type="html"><![CDATA[<!--  -->
<!--  -->
<!--  _ __    __     __      ____    ___     ___   _____   __  __    ___     -->
<!-- /\`'__\/'__`\ /'__`\   /',__\  / __`\ /' _ `\/\ '__`\/\ \/\ \ /' _ `\   -->
<!-- \ \ \//\  __//\ \L\.\_/\__, `\/\ \L\ \/\ \/\ \ \ \L\ \ \ \_\ \/\ \/\ \  -->
<!--  \ \_\\ \____\ \__/.\_\/\____/\ \____/\ \_\ \_\ \ ,__/\ \____/\ \_\ \_\ -->
<!--   \/_/ \/____/\/__/\/_/\/___/  \/___/  \/_/\/_/\ \ \/  \/___/  \/_/\/_/ -->
<!--                                                 \ \_\                   -->
<!--                                                  \/_/                   -->
<!--  -->
<h3 id="Oryx_2-1-0__u7CFB_u7EDF_u8981_u6C42"><a href="#Oryx_2-1-0__u7CFB_u7EDF_u8981_u6C42" class="headerlink" title="Oryx 2.1.0 系统要求"></a>Oryx 2.1.0 系统要求</h3><ul>
<li>Java 7 or later (JRE only is required)</li>
<li>A Hadoop cluster running the following components:<ul>
<li>Apache Hadoop 2.6.0 or later</li>
<li>Apache Zookeeper 3.4.5 or later</li>
<li>Apache Kafka 0.8.2 or later (in 0.8.x line)</li>
<li>Apache Spark 1.5.0 or later</li>
</ul>
</li>
</ul>
<h3 id="u670D_u52A1"><a href="#u670D_u52A1" class="headerlink" title="服务"></a>服务</h3><p>Hadoop cluster 服务</p>
<ul>
<li>HDFS</li>
<li>YARN</li>
<li>Zookeeper</li>
<li>Kafka</li>
<li>Spark (on YARN)</li>
</ul>
<p>Note that for CDH, Kafka is available as a parcel from <a href="http://www.cloudera.com/content/cloudera/en/developers/home/cloudera-labs/apache-kafka.html" target="_blank" rel="external">Cloudera Labs</a>.</p>
<p>Kafka brokers 需要配置在集群中，根据实例，需要注意hosts和端口。端口一般会设置为9092，此处和ZK server的端口保持一致，缺省设置为2181。缺省端口会在随后的例子中使用。</p>
<p>多个hosts需要通过都好分割，并需要提供host:port 这种方式，比如 ： your-zk-1:2181,your-zk-2:2181。</p>
<p>同时需要注意，你的ZK实例是否使用了chroot path。这是一个简单的路径前缀，比如 your-zk:2181/your-chroot<br>/kafka 是经常用到作为前缀的。如果没有使用chroot，就可以忽略这个。注意：如果存在多个ZK server，和一个chroot，只需要在最后添加一次chroot即可，比如 your-zk-1:2181,your-zk-2:2181/kafka</p>
<h3 id="u914D_u7F6EKafka"><a href="#u914D_u7F6EKafka" class="headerlink" title="配置Kafka"></a>配置Kafka</h3><p>Oryx 使用2个Kafka topics 做数据传输。</p>
<ul>
<li>一个传输输入数据到批量处理，和实时计算层</li>
<li>另一个同步模型更新到服务层</li>
</ul>
<p>缺省的topics的名称分别为OryxInput 和 OryxUpdate，只有Oryx服务启动后才能创建这两个topics。</p>
<p>输入topic的分区数目会影响消费数据的Spark Streaming 作业的分区数，甚至是并发数。比如，批量处理层读取HDFS上的历史数据分区和Kafka数据。</p>
<p>如果输入topic只有一个分区，且在每个时间间隔内有大量的数据涌入，这是Kafka基于的输入分区相对的需要很长的时间去处理。一个比较合适的经验值是选择一些topic分区，在一个批处理时间间隔内到达的大量数据，大约是一个HDFS 块大小，缺省值是128MB。</p>
<p>提供的oryx-run.sh kafka设置脚本，缺省设置为4个分区，当然了，这个值之后是可以修改的。必须注意不能设置更新topic多于1个分区。</p>
<p>重复因子可以设置为任何值，但是建议最少是2。注意：重复因子数目不能超过Kafka brokers在集群上的数目。所以提供的设置脚本里缺省设置重复因子为1. 之后你可以通过kafka-topics –zookeeper … –alter –topic … –replication-factor N 等修改这些缺省值。</p>
<p>你需要为其中一个或者两个topic配置持续时间。尤其重要的是需要限制更新topic的持续时间，因为实时计算层和服务层需要从启动开始的起点获取整个topic。这个机制并如不输入数据重要，输入数据不会再次从头读取数据。</p>
<p>设置这个值为批量处理层更新间隔的2倍是比较合适。比如设置该值为1天（24 <em> 60 </em> 60 * 1000 = 86400000 ms），设置topic的为86400000ms。这个可以通过oryx-run.sh设置脚本自动设置。</p>
<p>上述两个topics会包含大量信息；尤其是更新topic包含整个序列化的PMML模型。很有可能他会超过Kafka缺省最大消息的大小（kafka消息最大1Mib）。如果有更大的数据则需要设置topic’s max.message.bytes。oryx-run.sh Kafka设置脚本设置更新topic缺省为16Mib。这也是Oryx试图吸入更新topic里的模型默认最大值；更大的模型只会保存文件在HDFS中的路径。请查看属性oryx.update-topic.message.max-size。</p>
<p>Kafka代理的message.max.bytes属性可以控制这个，但是设置这个值会影响到代理管理的所有的topics，甚至包括不良状态的topics。可以查看性能和资源部分了解更多。尤其是，需要注意的是必须设置代理的replica.fetch.max.bytes属性，以防止重复任何非常大的消息。</p>
<blockquote>
<p>There is no per-topic equivalent to this.</p>
</blockquote>
<h3 id="Kafka_u914D_u7F6E_u81EA_u52A8_u8BBE_u7F6E"><a href="#Kafka_u914D_u7F6E_u81EA_u52A8_u8BBE_u7F6E" class="headerlink" title="Kafka配置自动设置"></a>Kafka配置自动设置</h3><p>提供的oryx-run.sh脚本可以打印ZK的当前配置，列出已经存在的Kafka中的topics，如果需要，会创建配置好的输入topics和更新topics。</p>
<p>你需要先创建Oryx配置文件，或者可以拷贝conf/als-example.conf。需要按照要求修改Kafka和ZK的配置文件，比如topic名称。</p>
<p>oryx.conf文件需要和每个层的JAR文件放在同一个目录下，然后执行：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">./oryx-run.sh kafka-setup</span><br><span class="line"></span><br><span class="line">Input  ZK:    your-zk:<span class="number">2181</span></span><br><span class="line">Input  Kafka: your-kafka:<span class="number">9092</span></span><br><span class="line">Input  topic: OryxInput</span><br><span class="line">Update ZK:    your-zk:<span class="number">2181</span></span><br><span class="line">Update Kafka: your-kafka:<span class="number">9092</span></span><br><span class="line">Update topic: OryxUpdate</span><br><span class="line"></span><br><span class="line">All available topics:</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Input topic OryxInput does not exist. Create it? y</span><br><span class="line">Creating topic OryxInput</span><br><span class="line">Created topic <span class="string">"OryxInput"</span>.</span><br><span class="line">Status of topic OryxInput:</span><br><span class="line">Topic:OryxInput	PartitionCount:<span class="number">4</span>	ReplicationFactor:<span class="number">1</span>	Configs:</span><br><span class="line">	Topic: OryxInput	Partition: <span class="number">0</span>	Leader: <span class="number">120</span>	Replicas: <span class="number">120</span>,<span class="number">121</span>	Isr: <span class="number">120</span>,<span class="number">121</span></span><br><span class="line">	Topic: OryxInput	Partition: <span class="number">1</span>	Leader: <span class="number">121</span>	Replicas: <span class="number">121</span>,<span class="number">120</span>	Isr: <span class="number">121</span>,<span class="number">120</span></span><br><span class="line">	Topic: OryxInput	Partition: <span class="number">2</span>	Leader: <span class="number">120</span>	Replicas: <span class="number">120</span>,<span class="number">121</span>	Isr: <span class="number">120</span>,<span class="number">121</span></span><br><span class="line">	Topic: OryxInput	Partition: <span class="number">3</span>	Leader: <span class="number">121</span>	Replicas: <span class="number">121</span>,<span class="number">120</span>	Isr: <span class="number">121</span>,<span class="number">120</span></span><br><span class="line"></span><br><span class="line">Update topic OryxUpdate does not exist. Create it? y</span><br><span class="line">Creating topic OryxUpdate</span><br><span class="line">Created topic <span class="string">"OryxUpdate"</span>.</span><br><span class="line">Updated config <span class="keyword">for</span> topic <span class="string">"OryxUpdate"</span>.</span><br><span class="line">Status of topic OryxUpdate:</span><br><span class="line">Topic:OryxUpdate	PartitionCount:<span class="number">1</span>	ReplicationFactor:<span class="number">1</span>	Configs:retention.ms=<span class="number">86400000</span>,max.message.bytes=<span class="number">16777216</span></span><br><span class="line">	Topic: OryxUpdate	Partition: <span class="number">0</span>	Leader: <span class="number">120</span>	Replicas: <span class="number">120</span>,<span class="number">121</span>	Isr: <span class="number">120</span>,<span class="number">121</span></span><br></pre></td></tr></table></figure>
<p>查看发送到输入和更新topic，监控应用的动作，可以执行：</p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">./oryx-run.sh kafka-tail</span><br><span class="line">Input  <span class="string">ZK:</span>    your-<span class="string">zk:</span><span class="number">2181</span></span><br><span class="line">Input  <span class="string">Kafka:</span> your-<span class="string">kafka:</span><span class="number">9092</span></span><br><span class="line">Input  <span class="string">topic:</span> OryxInput</span><br><span class="line">Update <span class="string">ZK:</span>    your-<span class="string">zk:</span><span class="number">2181</span></span><br><span class="line">Update <span class="string">Kafka:</span> your-<span class="string">kafka:</span><span class="number">9092</span></span><br><span class="line">Update <span class="string">topic:</span> OryxUpdate</span><br><span class="line"></span><br><span class="line">...output...</span><br></pre></td></tr></table></figure>
<p>接着在另外一个窗口，可以接受输入数据，比如将来自终端用户的文档data.csv加入到输入队列，并验证：</p>
<figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./oryx-<span class="keyword">run</span>.<span class="keyword">sh</span> kafka-<span class="keyword">input</span> --<span class="keyword">input</span>-<span class="keyword">file</span> data.csv</span><br></pre></td></tr></table></figure>
<p>如果以上全部成功了，可以关闭这些进程。集群至此已经准备好运行Oryx了。</p>
<h3 id="HDFS_u548C_u6570_u636E_u5C42"><a href="#HDFS_u548C_u6570_u636E_u5C42" class="headerlink" title="HDFS和数据层"></a>HDFS和数据层</h3><p>在Oryx中，Kafka是数据传输的途径，因此数据在Kafka中只是需要暂时存放的。然而输入数据也会持久化到HDFS中以备之后使用。同样的，模型和更新用来为Kafka的更新topic提供数据，模型也会持久化到HDFS以备之后引用。</p>
<p>oryx.batch.storage.data-dir定义了输入数据存放在HDFS中的位置。在这个目录下，子目录标题会以oryx-[timestamp].data形式创建，每一个会通过Spark Straming在批量处理层执行。在这里，时间戳格式格式与Unix相同，且以毫秒为单位。</p>
<p>实际上，和大多数Hadoop中分布式式进程输出的『文件』一样，存在这样的一个子目录，包含了很多以part-开头的文件。每个文件都是序列化文件，通过Writable类序列化Kafka输入topics的键值，Writable类实现自oryx.batch.storage.key-writable-class 和 oryx.batch.storage.message-writable-class类。默认情况下，这是TextWritable，并且keys和消息是以字符串形式被记录下来的。</p>
<p>该目录下的数据会被删除。也就不会再次被批处理层计算使用。尤其是，设置oryx.batch.storage.max-age-data-hours为一个非负数，将会使批处理层自动删除大于给定时间的数据。</p>
<p>同样的，在每个批处理间隔内被批处理层选中的模型会被原始的机器学习应用（扩展子MLUpdate）输出。也会被持久化到oryx.batch.storage.model-dir定义的目录下的子目录中。在这个目录下，子目录的命名都是以时间戳形式实现的，同Unix毫秒形式。</p>
<p>子目录下的内容取决于应用，但是一般会包含以model.pmml命名的PMML模块，并且和模块一起存在的可选的追加文件。</p>
<p>这个目录之所以存在是因为需要记录PMML模块用来归档用或者被其他工具使用。也可按照规则删除其内容。</p>
<h3 id="u6355_u83B7_u9519_u8BEF"><a href="#u6355_u83B7_u9519_u8BEF" class="headerlink" title="捕获错误"></a>捕获错误</h3><p>最后，你可能希望停止其中一个或者几个层的运行，或者重启。服务也可能挂鸟。这到底发生了神马？为啥会这样捏？</p>
<h4 id="u6570_u636E_u4E22_u5931"><a href="#u6570_u636E_u4E22_u5931" class="headerlink" title="数据丢失"></a>数据丢失</h4><p>历史数据存放在HDFS中，理论上会存放多个副本。HDFS会确保数据被靠谱的存放着。当设置了副本，Kafka也被设计为采用副本方式应对故障。</p>
<p>这并没有啥鸟用，这样不能确保数据不会丢失，只能依靠HDFS和Kafka能正常可用罢了。</p>
<h4 id="u670D_u52A1_u5668_u6302_u9E1F"><a href="#u670D_u52A1_u5668_u6302_u9E1F" class="headerlink" title="服务器挂鸟"></a>服务器挂鸟</h4><p>通常情况下，所有的三层服务进程应该会持续的工作，如果不得不停止或者挂鸟的话，服务自己会立即重启。这个可以通过初始化脚本完美完成或者类似机制（尚未实现鸟）</p>
<h5 id="u670D_u52A1_u5C42"><a href="#u670D_u52A1_u5C42" class="headerlink" title="服务层"></a>服务层</h5><p>服务层是无状态的。启动后，他会读取更新topics中的所有的模型和可用更新。当首个可用的模型就绪后，就可以开始应答请求。基于这个原因，需要适当的限制更新topic的持续时间。</p>
<p>服务层的操作不是分布式的，每个实例是独立的，启动和停止不会影响到其他部分。</p>
<h5 id="u5B9E_u65F6_u8BA1_u7B97_u5C42"><a href="#u5B9E_u65F6_u8BA1_u7B97_u5C42" class="headerlink" title="实时计算层"></a>实时计算层</h5><p>实时计算层同样也不存在状态，也会在读取全部的模型和更新topic的更新。只要存在合法的模型，就可以生成更新。同时，从最后一次偏移位置开始读取输入topics。</p>
<p>实时计算层使用Spark和Spark Streaming 做计算。Spark会响应计算过程中失败情况，并重试任务。</p>
<p>Spark Streaming的Kafka集成模块在某些情况下可以恢复接收的故障。<br>如果是整个进程死掉并被重新启动，oryx.id的值被设定以后，系统会自动从上一次Kafka记录的偏移地址开始读取。（否则，将会从上次偏移地址开始，这就意味着实时计算层没有运行的时候，到达的数据就不会生成任何更新。），同样的，如果实时计算层的模型还没有准备好的话，收到的数据也会被忽略。It effectively adopts “at most once” semantics.</p>
<p>由于实时计算层的作用是为最后发布的模型提供approximate, “best effort”的更新。这种行为由于其间接性一般是没有问题，且令人满意的。</p>
<h5 id="u6279_u5904_u7406_u5C42"><a href="#u6279_u5904_u7406_u5C42" class="headerlink" title="批处理层"></a>批处理层</h5><p>批处理层是最复杂的，因为他并生成某些状态：</p>
<ul>
<li>历史数据，总是持久化到HDFS</li>
<li>如果应用选择的话，模型的扩展状态和topics都可以被持久化到HDFS上</li>
</ul>
<p>对于多次或者根本不读取数据是非常敏感的，因为他本来就是生产官方下一代模型的组件</p>
<p>与实时计算层一起，Spark和Spark Streaming在计算过程中可以捕获很多错误情况。也可以管理存储到HDFS中的数据，负责避免两次写入相同数据。</p>
<p>应用负责回复各自的「状态」，一般情况下，建立在Oryx ML层的应用会将状态写入唯一的子目录中，并且在重启后会在新的目录简单的产生一个新状态。前一个状态如果存在的话，也会被完整写入或者被完全忽视。</p>
<p>批处理层也和实时计算层一样，符合『至多一次』的规则。综上，如果整个进程死掉或者被重启，oryx.id被设置的话，则会从Kafka记录的最后一次偏移重新读取，否则会在最后一次偏移处重新读取数据。</p>
]]></content>
    <summary type="html">
    <![CDATA[<!--  -->
<!--  -->
<!--  _ __    __     __      ____    ___     ___   _____   __  __    ___     -->
<!-- /\`'__\/'__`\ /'__`\   /',__\  / _]]>
    </summary>
    
      <category term="Admin" scheme="http://reasonpun.com/tags/Admin/"/>
    
      <category term="Hadoop" scheme="http://reasonpun.com/tags/Hadoop/"/>
    
      <category term="Oryx2" scheme="http://reasonpun.com/tags/Oryx2/"/>
    
      <category term="Spark" scheme="http://reasonpun.com/tags/Spark/"/>
    
      <category term="Spark-Streaming" scheme="http://reasonpun.com/tags/Spark-Streaming/"/>
    
      <category term="documentation" scheme="http://reasonpun.com/tags/documentation/"/>
    
      <category term="数据挖掘" scheme="http://reasonpun.com/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
      <category term="实时计算" scheme="http://reasonpun.com/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Oryx2 简介]]></title>
    <link href="http://reasonpun.com/2015/12/21/Oryx2-Overview/"/>
    <id>http://reasonpun.com/2015/12/21/Oryx2-Overview/</id>
    <published>2015-12-21T03:37:05.000Z</published>
    <updated>2016-01-25T03:17:11.000Z</updated>
    <content type="html"><![CDATA[<!--  -->
<!--  -->
<!--  _ __    __     __      ____    ___     ___   _____   __  __    ___     -->
<!-- /\`'__\/'__`\ /'__`\   /',__\  / __`\ /' _ `\/\ '__`\/\ \/\ \ /' _ `\   -->
<!-- \ \ \//\  __//\ \L\.\_/\__, `\/\ \L\ \/\ \/\ \ \ \L\ \ \ \_\ \/\ \/\ \  -->
<!--  \ \_\\ \____\ \__/.\_\/\____/\ \____/\ \_\ \_\ \ ,__/\ \____/\ \_\ \_\ -->
<!--   \/_/ \/____/\/__/\/_/\/___/  \/___/  \/_/\/_/\ \ \/  \/___/  \/_/\/_/ -->
<!--                                                 \ \_\                   -->
<!--                                                  \/_/                   -->
<!--  -->
<h3 id="u7B80_u4ECB"><a href="#u7B80_u4ECB" class="headerlink" title="简介"></a>简介</h3><img src="/images/oryx/OryxLogoMedium.png" title="oryx2">
<p>Oryx2是专注于进行大规模，实时机器学习框架，遵循lambda规则，基于Apache Spark和Apache Kafka构建。</p>
<p>Oryx 不仅是构建应用程序的框架，而且包含 协同过滤，分类，回归和聚类的打包的端到端的应用。</p>
<p>包含3层</p>
<ul>
<li>lambda层<ul>
<li>批量处理</li>
<li>快速处理</li>
<li>服务</li>
</ul>
</li>
<li>ML抽象层</li>
<li>端到端实现层</li>
</ul>
<p>从另一个角度，可以看成是一系列链接的元素</p>
<ul>
<li>批处理层：依据历史数据进行离线处理</li>
<li>实时处理层：通过增量数据流，实时更新结果</li>
<li>服务层：通过模型的传递实现异步查询API</li>
<li>数据传输层：在外部数据源与处理层之间传输数据</li>
</ul>
<p>The project</p>
<ul>
<li>may be reused tier by tier for example, the packaged app tier can be ignored, and it can be a framework for building new ML applications.</li>
<li>It can be reused layer by layer too: for example, the Speed Layer can be omitted if a deployment does not need incremental updates.</li>
<li>It can be modified piece-by-piece too: the collaborative filtering application’s model-building batch layer could be swapped for a custom implementation based on a new algorithm outside Spark MLlib while retaining the serving and speed layer implementations.</li>
</ul>
<img src="/images/oryx/Architecture.png" title="Architecture">
<h3 id="Lambda_u5C42_u5B9E_u73B0"><a href="#Lambda_u5C42_u5B9E_u73B0" class="headerlink" title="Lambda层实现"></a>Lambda层实现</h3><h4 id="u6570_u636E_u4F20_u8F93"><a href="#u6570_u636E_u4F20_u8F93" class="headerlink" title="数据传输"></a>数据传输</h4><p>  数据传输机制其实就是一个Kafka的Topic。任何一个进程（包含且不局限与服务层）都可以向topic中写入数据，并通过实时处理和批处理层查看。<br>  Kafka Topic也可以用来模型和模型之间的更新，并被实时处理层和服务层消费。</p>
<h4 id="u6279_u5904_u7406_u5C42"><a href="#u6279_u5904_u7406_u5C42" class="headerlink" title="批处理层"></a>批处理层</h4><p>  批处理层是以Spark Streaming进程的方式实现的，运行在Hadoop Cluster节点上，并读取来自Kafka topic的输入数据。 Streaming 进程会有一个很长的运行周期-若干小时甚至一天。会使用Spark存储当前会话数据到HDFS中，然后合并HDFS上的所有历史数据，之后重新初始化构建新的结果数据。并将新的结果重新写入HDFS，同时发不到Kafka更新topic中。</p>
<h4 id="u5B9E_u65F6_u5904_u7406_u5C42"><a href="#u5B9E_u65F6_u5904_u7406_u5C42" class="headerlink" title="实时处理层"></a>实时处理层</h4><p>  实时处理层也是由Spark Streaming进程实现的，同样读取Kafka topic输入数据。但是他存在比较短的运行周期，比如秒级别。会持续消费更新topic中的新模型，并生产新的模型。也会回写更新topic。</p>
<h4 id="u670D_u52A1_u5C42"><a href="#u670D_u52A1_u5C42" class="headerlink" title="服务层"></a>服务层</h4><p>  服务层监听更新topic上的模型以及模型更新。在内存中持久化模型状态。<br>  会暴露顶层方法的 HTTP REST API 用于查询内存中的模型。大部分接口都支持大规模的部署。<br>  每个接口都可以接收新的数据并写入Kafka，以此在实时处理层和批处理层可见。</p>
<h4 id="u914D_u7F6E_u548C_u90E8_u7F72"><a href="#u914D_u7F6E_u548C_u90E8_u7F72" class="headerlink" title="配置和部署"></a>配置和部署</h4><p>  程序是基于Java实现的，依赖</p>
<pre><code>* Spark 1.3.x+
* Hadoop 2.6.x+
* Tomcat 8.x+
* Kafka 0.8.2+
* Zookeeper 等。
</code></pre><p>  配置文件通过 <a href="https://github.com/typesafehub/config" target="_blank" rel="external">Typesafe Config</a> 的方式实现整个系统的部署配置。<br>  包括： 批处理，实时处理，服务层逻辑关键的接口类的实现</p>
<p>  每个层的二进制形式分开进行打包和部署的，每个都是以可执行的Java的jar包的形式存在并包含所有必须的服务。</p>
<h3 id="ML_u5C42_u5B9E_u73B0"><a href="#ML_u5C42_u5B9E_u73B0" class="headerlink" title="ML层实现"></a>ML层实现</h3><p>  ML层对上述通用接口方法做了简单的专一话的实现，实现了通用ML需求，并且对应用暴露了机器学习特有的接入接口。</p>
<p>  举个例子，实现了批量处理层，用于自动更新测试集和训练集进程。可以调用应用提供的函数来评估测试机模型。通过尝试不同的超参数值，选择出最佳结果。通过PMML管理模型的序列号。</p>
<h3 id="u7AEF_u5230_u7AEF_u5E94_u7528_u5B9E_u73B0"><a href="#u7AEF_u5230_u7AEF_u5E94_u7528_u5B9E_u73B0" class="headerlink" title="端到端应用实现"></a>端到端应用实现</h3><p>  除了作为一种框架，Oryx2 包含完整的三中机器学习需要的批处理层，实时处理层，服务层。<br>  开箱即用，或者作为自定义程序的基础：</p>
<pre><code>* 基于最小二乘法的协同过滤/推荐
* 基于k-means的聚类
* 基于随机决策森林的分类和回归
</code></pre><h3 id="u53C2_u8003_u6587_u732E"><a href="#u53C2_u8003_u6587_u732E" class="headerlink" title="参考文献"></a>参考文献</h3><ul>
<li><a href="http://oryx.io/index.html" target="_blank" rel="external">http://oryx.io/index.html</a></li>
<li><a href="http://www.ivanopt.com/oryx-document%E7%BF%BB%E8%AF%91/" target="_blank" rel="external">http://www.ivanopt.com/oryx-document%E7%BF%BB%E8%AF%91/</a></li>
<li><a href="http://jameskinley.tumblr.com/post/37398560534/the-lambda-architecture-principles-for" target="_blank" rel="external">http://jameskinley.tumblr.com/post/37398560534/the-lambda-architecture-principles-for</a></li>
<li><a href="http://dmg.org/pmml/v4-1/GeneralStructure.html" target="_blank" rel="external">http://dmg.org/pmml/v4-1/GeneralStructure.html</a></li>
<li><a href="http://blog.csdn.net/nxcjh321/article/details/24796879" target="_blank" rel="external">http://blog.csdn.net/nxcjh321/article/details/24796879</a></li>
<li><a href="http://youngfor.me/post/recsys/oryx-tui-jian-xi-tong-chu-ti-yan" target="_blank" rel="external">http://youngfor.me/post/recsys/oryx-tui-jian-xi-tong-chu-ti-yan</a></li>
<li><a href="https://github.com/OryxProject/oryx" target="_blank" rel="external">https://github.com/OryxProject/oryx</a></li>
</ul>
]]></content>
    <summary type="html">
    <![CDATA[<!--  -->
<!--  -->
<!--  _ __    __     __      ____    ___     ___   _____   __  __    ___     -->
<!-- /\`'__\/'__`\ /'__`\   /',__\  / _]]>
    </summary>
    
      <category term="Hadoop" scheme="http://reasonpun.com/tags/Hadoop/"/>
    
      <category term="Java" scheme="http://reasonpun.com/tags/Java/"/>
    
      <category term="Oryx2" scheme="http://reasonpun.com/tags/Oryx2/"/>
    
      <category term="Recommendation" scheme="http://reasonpun.com/tags/Recommendation/"/>
    
      <category term="Spark" scheme="http://reasonpun.com/tags/Spark/"/>
    
      <category term="Spark-Streaming" scheme="http://reasonpun.com/tags/Spark-Streaming/"/>
    
      <category term="数据挖掘" scheme="http://reasonpun.com/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
      <category term="实时计算" scheme="http://reasonpun.com/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Genetic Algorithm]]></title>
    <link href="http://reasonpun.com/2015/12/14/Genetic-Algorithm/"/>
    <id>http://reasonpun.com/2015/12/14/Genetic-Algorithm/</id>
    <published>2015-12-14T08:25:10.000Z</published>
    <updated>2016-01-25T03:17:11.000Z</updated>
    <content type="html"><![CDATA[<!--  -->
<!--  -->
<!--  _ __    __     __      ____    ___     ___   _____   __  __    ___     -->
<!-- /\`'__\/'__`\ /'__`\   /',__\  / __`\ /' _ `\/\ '__`\/\ \/\ \ /' _ `\   -->
<!-- \ \ \//\  __//\ \L\.\_/\__, `\/\ \L\ \/\ \/\ \ \ \L\ \ \ \_\ \/\ \/\ \  -->
<!--  \ \_\\ \____\ \__/.\_\/\____/\ \____/\ \_\ \_\ \ ,__/\ \____/\ \_\ \_\ -->
<!--   \/_/ \/____/\/__/\/_/\/___/  \/___/  \/_/\/_/\ \ \/  \/___/  \/_/\/_/ -->
<!--                                                 \ \_\                   -->
<!--                                                  \/_/                   -->
<!--  -->
<p>Genetic Algorithm: 遗传算法</p>
]]></content>
    <summary type="html">
    <![CDATA[<!--  -->
<!--  -->
<!--  _ __    __     __      ____    ___     ___   _____   __  __    ___     -->
<!-- /\`'__\/'__`\ /'__`\   /',__\  / _]]>
    </summary>
    
      <category term="Genetic Algorithm" scheme="http://reasonpun.com/tags/Genetic-Algorithm/"/>
    
      <category term="数据挖掘" scheme="http://reasonpun.com/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[apache-flume-ng-structure]]></title>
    <link href="http://reasonpun.com/2015/12/10/apache-flume-ng-structure/"/>
    <id>http://reasonpun.com/2015/12/10/apache-flume-ng-structure/</id>
    <published>2015-12-10T07:01:57.000Z</published>
    <updated>2016-03-07T08:25:29.000Z</updated>
    <content type="html"><![CDATA[<!--  -->
<!--  -->
<!--  _ __    __     __      ____    ___     ___   _____   __  __    ___     -->
<!-- /\`'__\/'__`\ /'__`\   /',__\  / __`\ /' _ `\/\ '__`\/\ \/\ \ /' _ `\   -->
<!-- \ \ \//\  __//\ \L\.\_/\__, `\/\ \L\ \/\ \/\ \ \ \L\ \ \ \_\ \/\ \/\ \  -->
<!--  \ \_\\ \____\ \__/.\_\/\____/\ \____/\ \_\ \_\ \ ,__/\ \____/\ \_\ \_\ -->
<!--   \/_/ \/____/\/__/\/_/\/___/  \/___/  \/_/\/_/\ \ \/  \/___/  \/_/\/_/ -->
<!--                                                 \ \_\                   -->
<!--                                                  \/_/                   -->
<!--  -->
<h2 id="Apache-flume_NG__u914D_u7F6E"><a href="#Apache-flume_NG__u914D_u7F6E" class="headerlink" title="Apache-flume NG 配置"></a>Apache-flume NG 配置</h2><h3 id="u7B80_u4ECB"><a href="#u7B80_u4ECB" class="headerlink" title="简介"></a>简介</h3><p>  Flume NG是一个分布式、可靠、可用的系统，它能够将不同数据源的海量日志数据进行高效收集、聚合、移动，最后存储到一个中心化数据存储系统中。</p>
<p>  由原来的Flume OG到现在的Flume NG，进行了架构重构，并且现在NG版本完全不兼容原来的OG版本。</p>
<p>  经过架构重构后，Flume NG更像是一个轻量的小工具，非常简单，容易适应各种方式日志收集，并支持failover和负载均衡。</p>
<h3 id="u67B6_u6784_u8BBE_u8BA1_u8981_u70B9"><a href="#u67B6_u6784_u8BBE_u8BA1_u8981_u70B9" class="headerlink" title="架构设计要点"></a>架构设计要点</h3><h4 id="u6838_u5FC3_u6982_u5FF5"><a href="#u6838_u5FC3_u6982_u5FF5" class="headerlink" title="核心概念"></a>核心概念</h4><ul>
<li>Event：一个数据单元，带有一个可选的消息头</li>
<li>Flow：Event从源点到达目的点的迁移的抽象</li>
<li>Client：操作位于源点处的Event，将其发送到Flume Agent</li>
<li>Agent：一个独立的Flume进程，包含组件Source、Channel、Sink</li>
<li>Source：用来消费传递到该组件的Event</li>
<li>Channel：中转Event的一个临时存储，保存有Source组件传递过来的Event</li>
<li>Sink：从Channel中读取并移除Event，将Event传递到Flow Pipeline中的下一个Agent（如果有的话）</li>
</ul>
<h4 id="u67B6_u6784_u56FE"><a href="#u67B6_u6784_u56FE" class="headerlink" title="架构图"></a>架构图</h4><pre><code><img src="/images/flume-ng/flume-ng-architecture.png" title="flume-ng总体结构图">
</code></pre><h4 id="u57FA_u672C_u6D41_u7A0B"><a href="#u57FA_u672C_u6D41_u7A0B" class="headerlink" title="基本流程"></a>基本流程</h4><p>外部系统产生日志，直接通过Flume的Agent的Source组件将事件（如日志行）发送到中间临时的channel组件，最后传递给Sink组件，HDFS Sink组件可以直接把数据存储到HDFS集群上。</p>
<h4 id="u5355Agent"><a href="#u5355Agent" class="headerlink" title="单Agent"></a>单Agent</h4><p>  一个最基本Flow的配置，格式如下：</p>
  <figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># list the sources, sinks and channels for the agent</span></span><br><span class="line"><span class="variable">&lt;Agent&gt;</span>.sources = <span class="variable">&lt;Source1&gt;</span> <span class="variable">&lt;Source2&gt;</span></span><br><span class="line"><span class="variable">&lt;Agent&gt;</span>.sinks = <span class="variable">&lt;Sink1&gt;</span> <span class="variable">&lt;Sink2&gt;</span></span><br><span class="line"><span class="variable">&lt;Agent&gt;</span>.channels = <span class="variable">&lt;Channel1&gt;</span> <span class="variable">&lt;Channel2&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># set channel for source</span></span><br><span class="line"><span class="variable">&lt;Agent&gt;</span>.sources.<span class="variable">&lt;Source1&gt;</span>.channels = <span class="variable">&lt;Channel1&gt;</span> <span class="variable">&lt;Channel2&gt;</span> ...</span><br><span class="line"><span class="variable">&lt;Agent&gt;</span>.sources.<span class="variable">&lt;Source2&gt;</span>.channels = <span class="variable">&lt;Channel1&gt;</span> <span class="variable">&lt;Channel2&gt;</span> ...</span><br><span class="line"></span><br><span class="line"><span class="comment"># set channel for sink</span></span><br><span class="line"><span class="variable">&lt;Agent&gt;</span>.sinks.<span class="variable">&lt;Sink1&gt;</span>.channel = <span class="variable">&lt;Channel1&gt;</span></span><br><span class="line"><span class="variable">&lt;Agent&gt;</span>.sinks.<span class="variable">&lt;Sink2&gt;</span>.channel = <span class="variable">&lt;Channel2&gt;</span></span><br></pre></td></tr></table></figure>
<p>  尖括号里面的，我们可以根据实际需求或业务来修改名称。</p>
<p>  下面详细说明：</p>
<ul>
<li><agent> 表示配置一个Agent的名称，一个Agent肯定有一个名称。</agent></li>
<li><source1>和<source2>是Agent的Source组件的名称，消费传递过来的Event。</source2></source1></li>
<li><channel1>和<channel2>是Agent的Channel组件的名称。</channel2></channel1></li>
<li><p><sink1>与<sink2>是Agent的Sink组件的名称，从Channel中消费（移除）Event。</sink2></sink1></p>
<p>上面配置内容中</p>
</li>
<li><p>第一组中配置Source、Sink、Channel，它们的值可以有1个或者多个；</p>
</li>
<li>第二组中配置Source将把数据存储（Put）到哪一个Channel中，可以存储到1个或多个Channel中，<br>同一个Source将数据存储到多个Channel中，实际上是Replication；</li>
<li>第三组中配置Sink从哪一个Channel中取（Task）数据，一个Sink只能从一个Channel中取数据。</li>
</ul>
<h4 id="u591A_u4E2AAgent_u987A_u5E8F_u8FDE_u63A5"><a href="#u591A_u4E2AAgent_u987A_u5E8F_u8FDE_u63A5" class="headerlink" title="多个Agent顺序连接"></a>多个Agent顺序连接</h4>  <img src="/images/flume-ng/flume-multiseq-agents.png" title="flume-ng 多个Agent顺序连接">
<p>  可以将多个Agent顺序连接起来，将最初的数据源经过收集，存储到最终的存储系统中。这是最简单的情况，一般情况下，应该控制这种顺序连接的Agent的数量，因为数据流经的路径变长了，如果不考虑failover的话，出现故障将影响整个Flow上的Agent收集服务。</p>
<h4 id="u591A_u4E2AAgent_u7684_u6570_u636E_u6C47_u805A_u5230_u540C_u4E00_u4E2AAgent"><a href="#u591A_u4E2AAgent_u7684_u6570_u636E_u6C47_u805A_u5230_u540C_u4E00_u4E2AAgent" class="headerlink" title="多个Agent的数据汇聚到同一个Agent"></a>多个Agent的数据汇聚到同一个Agent</h4>  <img src="/images/flume-ng/flume-join-agent.png" title="flume-ng 多个Agent的数据汇聚到同一个Agent">
<p>  这种情况应用的场景比较多，比如要收集Web网站的用户行为日志，Web网站为了可用性使用的负载均衡的集群模式，每个节点都产生用户行为日志，可以为每个节点都配置一个Agent来单独收集日志数据，然后多个Agent将数据最终汇聚到一个用来存储数据存储系统，如HDFS上。</p>
<h4 id="u591A_u8DEF_uFF08Multiplexing_uFF09Agent"><a href="#u591A_u8DEF_uFF08Multiplexing_uFF09Agent" class="headerlink" title="多路（Multiplexing）Agent"></a>多路（Multiplexing）Agent</h4>  <img src="/images/flume-ng/flume-multiplexing-agent.png" title="flume-ng 多路（Multiplexing）Agent">
<p>  这种模式，有两种方式</p>
<ul>
<li><p>一种是用来复制（Replication）</p>
<ul>
<li><p>Replication方式，可以将最前端的数据源复制多份，分别传递到多个channel中，每个channel接收到的数据都是相同的，配置格式</p>
<figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># List the sources, sinks and channels for the agent</span></span><br><span class="line"><span class="variable">&lt;Agent&gt;</span>.sources = <span class="variable">&lt;Source1&gt;</span></span><br><span class="line"><span class="variable">&lt;Agent&gt;</span>.sinks = <span class="variable">&lt;Sink1&gt;</span> <span class="variable">&lt;Sink2&gt;</span></span><br><span class="line"><span class="variable">&lt;Agent&gt;</span>.channels = <span class="variable">&lt;Channel1&gt;</span> <span class="variable">&lt;Channel2&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># set list of channels for source (separated by space)</span></span><br><span class="line"><span class="variable">&lt;Agent&gt;</span>.sources.<span class="variable">&lt;Source1&gt;</span>.channels = <span class="variable">&lt;Channel1&gt;</span> <span class="variable">&lt;Channel2&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># set channel for sinks</span></span><br><span class="line"><span class="variable">&lt;Agent&gt;</span>.sinks.<span class="variable">&lt;Sink1&gt;</span>.channel = <span class="variable">&lt;Channel1&gt;</span></span><br><span class="line"><span class="variable">&lt;Agent&gt;</span>.sinks.<span class="variable">&lt;Sink2&gt;</span>.channel = <span class="variable">&lt;Channel2&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="variable">&lt;Agent&gt;</span>.sources.<span class="variable">&lt;Source1&gt;</span>.selector.type = replicating</span><br></pre></td></tr></table></figure>
<p>使用的Replication方式，Source1会将数据分别存储到Channel1和Channel2，这两个channel里面存储的数据是相同的，然后数据被传递到Sink1和Sink2。</p>
</li>
</ul>
</li>
<li><p>另一种是用来分流（Multiplexing）</p>
<ul>
<li><p>Multiplexing方式，selector可以根据header的值来确定数据传递到哪一个channel</p>
<figure class="highlight pf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Mapping for multiplexing selector</span></span><br><span class="line"><span class="variable">&lt;Agent&gt;</span>.sources.<span class="variable">&lt;Source1&gt;</span>.selector.type = multiplexing</span><br><span class="line"><span class="variable">&lt;Agent&gt;</span>.sources.<span class="variable">&lt;Source1&gt;</span>.selector.header = <span class="variable">&lt;someHeader&gt;</span></span><br><span class="line"><span class="variable">&lt;Agent&gt;</span>.sources.<span class="variable">&lt;Source1&gt;</span>.selector.mapping.<span class="variable">&lt;Value1&gt;</span> = <span class="variable">&lt;Channel1&gt;</span></span><br><span class="line"><span class="variable">&lt;Agent&gt;</span>.sources.<span class="variable">&lt;Source1&gt;</span>.selector.mapping.<span class="variable">&lt;Value2&gt;</span> = <span class="variable">&lt;Channel1&gt;</span> <span class="variable">&lt;Channel2&gt;</span></span><br><span class="line"><span class="variable">&lt;Agent&gt;</span>.sources.<span class="variable">&lt;Source1&gt;</span>.selector.mapping.<span class="variable">&lt;Value3&gt;</span> = <span class="variable">&lt;Channel2&gt;</span></span><br><span class="line"><span class="comment">#...</span></span><br><span class="line"></span><br><span class="line"><span class="variable">&lt;Agent&gt;</span>.sources.<span class="variable">&lt;Source1&gt;</span>.selector.<span class="keyword">default</span> = <span class="variable">&lt;Channel2&gt;</span></span><br></pre></td></tr></table></figure>
<p>上面selector的type的值为multiplexing，同时配置selector的header信息，还配置了多个selector的mapping的值，即header的值：如果header的值为Value1、Value2，数据从Source1路由到Channel1；如果header的值为Value2、Value3，数据从Source1路由到Channel2。</p>
</li>
</ul>
</li>
</ul>
<h4 id="u5B9E_u73B0load_balance_u529F_u80FD"><a href="#u5B9E_u73B0load_balance_u529F_u80FD" class="headerlink" title="实现load balance功能"></a>实现load balance功能</h4>  <img src="/images/flume-ng/flume-load-balance-agents.png" title="实现load balance功能">
<p>  Load balancing Sink Processor能够实现load balance功能，上图Agent1是一个路由节点，<br>  负责将Channel暂存的Event均衡到对应的多个Sink组件上，而每个Sink组件分别连接到一个独立的Agent上</p>
  <figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a1<span class="class">.sinkgroups</span> = g1</span><br><span class="line">a1<span class="class">.sinkgroups</span><span class="class">.g1</span><span class="class">.sinks</span> = k1 k2 k3</span><br><span class="line">a1<span class="class">.sinkgroups</span><span class="class">.g1</span><span class="class">.processor</span><span class="class">.type</span> = load_balance</span><br><span class="line">a1<span class="class">.sinkgroups</span><span class="class">.g1</span><span class="class">.processor</span><span class="class">.backoff</span> = true</span><br><span class="line">a1<span class="class">.sinkgroups</span><span class="class">.g1</span><span class="class">.processor</span><span class="class">.selector</span> = round_robin</span><br><span class="line">a1<span class="class">.sinkgroups</span><span class="class">.g1</span><span class="class">.processor</span><span class="class">.selector</span><span class="class">.maxTimeOut</span>=<span class="number">10000</span></span><br></pre></td></tr></table></figure>
<h4 id="u5B9E_u73B0failover_u80FD"><a href="#u5B9E_u73B0failover_u80FD" class="headerlink" title="实现failover能"></a>实现failover能</h4><p>  Failover Sink Processor能够实现failover功能，具体流程类似load balance，<br>  但是内部处理机制与load balance完全不同：Failover Sink Processor维护一个优先级Sink组件列表，只要有一个Sink组件可用，<br>  Event就被传递到下一个组件。如果一个Sink能够成功处理Event，则会加入到一个Pool中，否则会被移出Pool并计算失败次数，设置一个惩罚因子</p>
  <figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a1<span class="class">.sinkgroups</span> = g1</span><br><span class="line">a1<span class="class">.sinkgroups</span><span class="class">.g1</span><span class="class">.sinks</span> = k1 k2 k3</span><br><span class="line">a1<span class="class">.sinkgroups</span><span class="class">.g1</span><span class="class">.processor</span><span class="class">.type</span> = failover</span><br><span class="line">a1<span class="class">.sinkgroups</span><span class="class">.g1</span><span class="class">.processor</span><span class="class">.priority</span><span class="class">.k1</span> = <span class="number">5</span></span><br><span class="line">a1<span class="class">.sinkgroups</span><span class="class">.g1</span><span class="class">.processor</span><span class="class">.priority</span><span class="class">.k2</span> = <span class="number">7</span></span><br><span class="line">a1<span class="class">.sinkgroups</span><span class="class">.g1</span><span class="class">.processor</span><span class="class">.priority</span><span class="class">.k3</span> = <span class="number">6</span></span><br><span class="line">a1<span class="class">.sinkgroups</span><span class="class">.g1</span><span class="class">.processor</span><span class="class">.maxpenalty</span> = <span class="number">20000</span></span><br></pre></td></tr></table></figure>
<h3 id="u5B89_u88C5_u914D_u7F6E"><a href="#u5B89_u88C5_u914D_u7F6E" class="headerlink" title="安装配置"></a>安装配置</h3><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下载二进制包</span></span><br><span class="line">[mofun_mining<span class="variable">@i</span>-tev02vc1 ~]<span class="variable">$ </span>wget <span class="string">"http://apache.arvixe.com/flume/1.6.0/apache-flume-1.6.0-bin.tar.gz"</span></span><br><span class="line">[mofun_mining<span class="variable">@i</span>-tev02vc1 ~]<span class="variable">$ </span>tar xvzf apache-flume-<span class="number">1.6</span>.<span class="number">0</span>-bin.tar.gz</span><br><span class="line">[mofun_mining<span class="variable">@i</span>-tev02vc1 ~]<span class="variable">$ </span>mv apache-flume-<span class="number">1.6</span>.<span class="number">0</span>-bin /usr/local/</span><br><span class="line"><span class="comment"># 修改配置文件</span></span><br><span class="line">[mofun_mining<span class="variable">@i</span>-qe32ajmq conf]<span class="variable">$ </span>pwd</span><br><span class="line">/usr/local/apache-flume-<span class="number">1.6</span>.<span class="number">0</span>-bin/conf</span><br><span class="line">[mofun_mining<span class="variable">@i</span>-qe32ajmq conf]<span class="variable">$ </span>sudo cp flume-conf.properties.template flume-conf.properties</span><br></pre></td></tr></table></figure>
<p>采用 Avro Source+Memory Channel+HDFS Sink 方式</p>
<ul>
<li><p>服务器（日志汇总服务器agent）端配置文件</p>
<figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">[mofun_mining@i-tev02vc1 ~]$ <span class="keyword">cd</span> /usr/<span class="keyword">local</span>/apache-flume-1.6.0-bin/<span class="keyword">conf</span>/</span><br><span class="line">[mofun_mining@i-tev02vc1 <span class="keyword">conf</span>]$ <span class="keyword">ls</span></span><br><span class="line">flume-<span class="keyword">conf</span>.properties  flume-<span class="keyword">conf</span>.properties.template  flume-env.ps1.template  flume-env.<span class="keyword">sh</span>  flume-env.<span class="keyword">sh</span>.template  log4j.properties</span><br><span class="line">[mofun_mining@i-tev02vc1 <span class="keyword">conf</span>]$ <span class="keyword">pwd</span></span><br><span class="line">/usr/<span class="keyword">local</span>/apache-flume-1.6.0-bin/<span class="keyword">conf</span></span><br><span class="line">[mofun_mining@i-tev02vc1 <span class="keyword">conf</span>]$ sudo vim flume-<span class="keyword">conf</span>.properties</span><br><span class="line"># example.<span class="keyword">conf</span>: A single-node Flume configuration</span><br><span class="line"></span><br><span class="line"># Name the components <span class="keyword">on</span> this agent</span><br><span class="line">agent1.sources = r1</span><br><span class="line">agent1.sinks = k1</span><br><span class="line">agent1.channels = c1</span><br><span class="line"></span><br><span class="line"># <span class="keyword">Describe</span>/configure the source</span><br><span class="line">agent1.sources.r1.<span class="keyword">type</span> = avro</span><br><span class="line">agent1.sources.r1.bind = 192.168.1.33</span><br><span class="line">agent1.sources.r1.port = 41414</span><br><span class="line">agent1.sources.r1.channels = c1</span><br><span class="line"></span><br><span class="line"># <span class="keyword">Describe</span> the sink</span><br><span class="line">agent1.sinks.k1.<span class="keyword">type</span> = hdfs</span><br><span class="line">agent1.sinks.k1.channel = c1</span><br><span class="line">agent1.sinks.k1.hdfs.fileType = DataStream</span><br><span class="line">agent1.sinks.k1.hdfs.useLocalTimeStamp = true</span><br><span class="line">agent1.sinks.k1.hdfs.path = /flume/events/%Y-%<span class="keyword">m</span>-%<span class="literal">d</span></span><br><span class="line">#agent1.sinks.k1.hdfs.round = true</span><br><span class="line">#agent1.sinks.k1.hdfs.roundValue = 10</span><br><span class="line">#agent1.sinks.k1.hdfs.roundUnit = minute</span><br><span class="line">agent1.sinks.k1.hdfs.rollCount = 5000</span><br><span class="line">agent1.sinks.k1.hdfs.rollSize = 0</span><br><span class="line">agent1.sinks.k1.hdfs.rollInterval= 0</span><br><span class="line"></span><br><span class="line"># <span class="keyword">Use</span> a channel <span class="keyword">which</span> buffers events <span class="keyword">in</span> <span class="keyword">memory</span></span><br><span class="line">agent1.channels.c1.<span class="keyword">type</span> = <span class="keyword">memory</span></span><br><span class="line">agent1.channels.c1.capacity = 10000</span><br><span class="line">agent1.channels.c1.transactionCapacity = 1000</span><br></pre></td></tr></table></figure>
</li>
<li><p>客户端（日志收集agent）</p>
<figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">[reason@i-qunray9x <span class="keyword">conf</span>]$ <span class="keyword">cd</span> /usr/<span class="keyword">local</span>/apache-flume-1.6.0-bin/<span class="keyword">conf</span>/</span><br><span class="line">[reason@i-qunray9x <span class="keyword">conf</span>]$ <span class="keyword">pwd</span></span><br><span class="line">/usr/<span class="keyword">local</span>/apache-flume-1.6.0-bin/<span class="keyword">conf</span></span><br><span class="line">[reason@i-qunray9x <span class="keyword">conf</span>]$ sudo vim flume-<span class="keyword">conf</span>.properties</span><br><span class="line"></span><br><span class="line"># example.<span class="keyword">conf</span>: A single-node Flume configuration</span><br><span class="line"></span><br><span class="line"># Name the components <span class="keyword">on</span> this agent</span><br><span class="line">agent1.sources = r1</span><br><span class="line">agent1.sinks = k1</span><br><span class="line">agent1.channels = c1</span><br><span class="line"></span><br><span class="line"># <span class="keyword">Describe</span>/configure the source</span><br><span class="line">agent1.sources.r1.<span class="keyword">type</span> = exec</span><br><span class="line">agent1.sources.r1.command = tail -<span class="keyword">n</span> 0 -F /home/reason/1.txt</span><br><span class="line">agent1.sources.r1.channels = c1</span><br><span class="line"></span><br><span class="line"># <span class="keyword">Describe</span> the sink</span><br><span class="line">agent1.sinks.k1.<span class="keyword">type</span> = avro</span><br><span class="line">agent1.sinks.k1.channel = c1</span><br><span class="line">agent1.sinks.k1.hdfs.useLocalTimeStamp = true</span><br><span class="line">agent1.sinks.k1.hdfs.path = /flume/events/%Y-%<span class="keyword">m</span>-%<span class="literal">d</span></span><br><span class="line">agent1.sinks.k1.hostname=192.168.1.33</span><br><span class="line">agent1.sinks.k1.port = 41414</span><br><span class="line"></span><br><span class="line"># <span class="keyword">Use</span> a channel <span class="keyword">which</span> buffers events <span class="keyword">in</span> <span class="keyword">memory</span></span><br><span class="line">agent1.channels.c1.<span class="keyword">type</span> = <span class="keyword">memory</span></span><br><span class="line">agent1.channels.c1.capacity = 5000</span><br><span class="line">agent1.channels.c1.transactionCapacity = 500</span><br></pre></td></tr></table></figure>
</li>
<li><p>启动服务器</p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[mofun_mining<span class="annotation">@i</span>-tev02vc1 conf]$</span><br><span class="line"><span class="regexp">/usr/</span>local<span class="regexp">/apache-flume-1.6.0-bin/</span>bin<span class="regexp">/flume-ng agent -c ./</span>conf<span class="regexp">/ -f /</span>usr<span class="regexp">/local/</span>apache-flume-<span class="number">1.6</span><span class="number">.0</span>-bin<span class="regexp">/conf/</span>flume-conf.properties -n agent1 -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>
</li>
<li><p>启动客户端</p>
<figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[reason@i-qunray9x <span class="keyword">conf</span>]$</span><br><span class="line">/usr/<span class="keyword">local</span>/apache-flume-1.6.0-bin/bin/flume-ng agent -c <span class="keyword">conf</span> -f /usr/<span class="keyword">local</span>/apache-flume-1.6.0-bin/<span class="keyword">conf</span>/flume-<span class="keyword">conf</span>.properties -<span class="keyword">n</span> agent1</span><br></pre></td></tr></table></figure>
</li>
<li><p>测试</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[mofun_mining@i-r6cuv8iq ~]$ hdfs dfs -ls /flume/events/<span class="number">2015</span>-<span class="number">12</span>-<span class="number">15</span></span><br><span class="line">Found <span class="number">40</span> items</span><br><span class="line">-rw-r--r--   <span class="number">2</span> mofun_mining supergroup      <span class="number">34844</span> <span class="number">2015</span>-<span class="number">12</span>-<span class="number">15</span> <span class="number">17</span>:<span class="number">39</span> /flume/events/<span class="number">2015</span>-<span class="number">12</span>-<span class="number">15</span>/FlumeData<span class="number">.1450172340281</span></span><br><span class="line">-rw-r--r--   <span class="number">2</span> mofun_mining supergroup      <span class="number">34850</span> <span class="number">2015</span>-<span class="number">12</span>-<span class="number">15</span> <span class="number">17</span>:<span class="number">39</span> /flume/events/<span class="number">2015</span>-<span class="number">12</span>-<span class="number">15</span>/FlumeData<span class="number">.1450172340282</span></span><br><span class="line">-rw-r--r--   <span class="number">2</span> mofun_mining supergroup      <span class="number">34850</span> <span class="number">2015</span>-<span class="number">12</span>-<span class="number">15</span> <span class="number">17</span>:<span class="number">39</span> /flume/events/<span class="number">2015</span>-<span class="number">12</span>-<span class="number">15</span>/FlumeData<span class="number">.1450172340283</span></span><br><span class="line">-rw-r--r--   <span class="number">2</span> mofun_mining supergroup      <span class="number">34850</span> <span class="number">2015</span>-<span class="number">12</span>-<span class="number">15</span> <span class="number">17</span>:<span class="number">39</span> /flume/events/<span class="number">2015</span>-<span class="number">12</span>-<span class="number">15</span>/FlumeData<span class="number">.1450172340284</span></span><br><span class="line">-rw-r--r--   <span class="number">2</span> mofun_mining supergroup      <span class="number">34850</span> <span class="number">2015</span>-<span class="number">12</span>-<span class="number">15</span> <span class="number">17</span>:<span class="number">39</span> /flume/events/<span class="number">2015</span>-<span class="number">12</span>-<span class="number">15</span>/FlumeData<span class="number">.1450172340285</span></span><br><span class="line">-rw-r--r--   <span class="number">2</span> mofun_mining supergroup      <span class="number">34850</span> <span class="number">2015</span>-<span class="number">12</span>-<span class="number">15</span> <span class="number">17</span>:<span class="number">39</span> /flume/events/<span class="number">2015</span>-<span class="number">12</span>-<span class="number">15</span>/FlumeData<span class="number">.1450172340286</span></span><br><span class="line">-rw-r--r--   <span class="number">2</span> mofun_mining supergroup      <span class="number">34850</span> <span class="number">2015</span>-<span class="number">12</span>-<span class="number">15</span> <span class="number">17</span>:<span class="number">39</span> /flume/events/<span class="number">2015</span>-<span class="number">12</span>-<span class="number">15</span>/FlumeData<span class="number">.1450172340287</span></span><br><span class="line">-rw-r--r--   <span class="number">2</span> mofun_mining supergroup      <span class="number">34850</span> <span class="number">2015</span>-<span class="number">12</span>-<span class="number">15</span> <span class="number">17</span>:<span class="number">39</span> /flume/events/<span class="number">2015</span>-<span class="number">12</span>-<span class="number">15</span>/FlumeData<span class="number">.1450172340288</span></span><br><span class="line">-rw-r--r--   <span class="number">2</span> mofun_mining supergroup      <span class="number">34850</span> <span class="number">2015</span>-<span class="number">12</span>-<span class="number">15</span> <span class="number">17</span>:<span class="number">39</span> /flume/events/<span class="number">2015</span>-<span class="number">12</span>-<span class="number">15</span>/FlumeData<span class="number">.1450172340289</span></span><br><span class="line">-rw-r--r--   <span class="number">2</span> mofun_mining supergroup      <span class="number">34850</span> <span class="number">2015</span>-<span class="number">12</span>-<span class="number">15</span> <span class="number">17</span>:<span class="number">39</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>此时，通过nginx实时产生的日志，即可实时插入到hdfs中了。</p>
</li>
</ul>
<h3 id="u53C2_u8003_u6587_u732E"><a href="#u53C2_u8003_u6587_u732E" class="headerlink" title="参考文献"></a>参考文献</h3><ul>
<li><a href="http://shiyanjun.cn/archives/915.html" target="_blank" rel="external">http://shiyanjun.cn/archives/915.html</a></li>
<li><a href="http://my.oschina.net/leejun2005/blog/288136" target="_blank" rel="external">http://my.oschina.net/leejun2005/blog/288136</a></li>
<li><a href="http://tech.meituan.com/mt-log-system-optimization.html" target="_blank" rel="external">http://tech.meituan.com/mt-log-system-optimization.html</a></li>
<li><a href="http://www.ixirong.com/2015/05/18/how-to-install-flume-ng/" target="_blank" rel="external">http://www.ixirong.com/2015/05/18/how-to-install-flume-ng/</a></li>
<li><a href="https://flume.apache.org/FlumeUserGuide.html#setting-up-an-agent" target="_blank" rel="external">https://flume.apache.org/FlumeUserGuide.html#setting-up-an-agent</a></li>
<li><a href="http://m.blog.csdn.net/blog/xueliang1029/24039459" target="_blank" rel="external">http://m.blog.csdn.net/blog/xueliang1029/24039459</a></li>
</ul>
]]></content>
    <summary type="html">
    <![CDATA[<!--  -->
<!--  -->
<!--  _ __    __     __      ____    ___     ___   _____   __  __    ___     -->
<!-- /\`'__\/'__`\ /'__`\   /',__\  / _]]>
    </summary>
    
      <category term="Apache" scheme="http://reasonpun.com/tags/Apache/"/>
    
      <category term="Flume-NG" scheme="http://reasonpun.com/tags/Flume-NG/"/>
    
      <category term="HDFS" scheme="http://reasonpun.com/tags/HDFS/"/>
    
      <category term="数据挖掘" scheme="http://reasonpun.com/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[apache_kafka_structure]]></title>
    <link href="http://reasonpun.com/2015/12/08/apache-kafka-structure/"/>
    <id>http://reasonpun.com/2015/12/08/apache-kafka-structure/</id>
    <published>2015-12-08T08:30:08.000Z</published>
    <updated>2015-12-25T03:33:24.000Z</updated>
    <content type="html"><![CDATA[<!--  -->
<!--  -->
<!--  _ __    __     __      ____    ___     ___   _____   __  __    ___     -->
<!-- /\`'__\/'__`\ /'__`\   /',__\  / __`\ /' _ `\/\ '__`\/\ \/\ \ /' _ `\   -->
<!-- \ \ \//\  __//\ \L\.\_/\__, `\/\ \L\ \/\ \/\ \ \ \L\ \ \ \_\ \/\ \/\ \  -->
<!--  \ \_\\ \____\ \__/.\_\/\____/\ \____/\ \_\ \_\ \ ,__/\ \____/\ \_\ \_\ -->
<!--   \/_/ \/____/\/__/\/_/\/___/  \/___/  \/_/\/_/\ \ \/  \/___/  \/_/\/_/ -->
<!--                                                 \ \_\                   -->
<!--                                                  \/_/                   -->
<!--  -->
<h4 id="Apache_Kafka_u6D88_u606F_u670D_u52A1"><a href="#Apache_Kafka_u6D88_u606F_u670D_u52A1" class="headerlink" title="Apache Kafka消息服务"></a>Apache Kafka消息服务</h4><ul>
<li><p>参考地址 <a href="http://kafka.apache.org/documentation.html#brokerconfigs" target="_blank" rel="external">http://kafka.apache.org/documentation.html</a></p>
</li>
<li><p>消息队列的分类</p>
<ul>
<li><p>点对点</p>
<p>生产者生产消息发送到Queue中，消费者消费Queue中的消息，其中：</p>
<ul>
<li>Queue中不再存储已经被消费的消息</li>
<li>Queue支持多个消费者，但是同一个消息，只能被一个消费者消费</li>
</ul>
</li>
<li><p>发布/订阅</p>
<p>生产者（生产）将消息发布到topic中，同时多个消费者（消费）订阅该消息。和点对点方式不同的是，发布到topic的消息会被所有订阅者消费</p>
</li>
</ul>
</li>
<li><p>简介</p>
<p>背景 Kafka使用Scala语言编写，是一个分布式，分区的，支持多副本，多订阅者的日志系统。</p>
<p>目前支持Java，Python，C++， PHP等</p>
<ul>
<li>总体结构</li>
</ul>
<img src="/images/kafka.0.9.0/structure.png" title="kafka总体结构图">
</li>
</ul>
<ul>
<li><p>名词解释</p>
<ul>
<li><p>Producer</p>
<p>  消息生产者，就是向kafka broker发消息的客户端</p>
</li>
<li><p>Consumer</p>
<p>  消息消费者，向kafka broker取消息的客户端</p>
</li>
<li><p>Topic</p>
<p>  是一个消息队列？</p>
</li>
<li><p>Consumer Group （CG）</p>
<ul>
<li>这是Kafka用来实现一个Topic消息的广播（发给所有的Consumer）和单播（发给任意一个Consumer）的手段</li>
<li>一个Topic可以有多个CG</li>
<li>Topic的消息会复制（不是真的复制，是概念上的）到所有的CG，但每个CG只会把消息发给该CG中的一个consumer</li>
<li>如果需要实现广播，只要每个Consumer有一个独立的CG就可以了</li>
<li>要实现单播只要所有的Consumer在同一个CG</li>
<li><p>用CG还可以将Consumer进行自由的分组而不需要多次发送消息到不同的topic</p>
</li>
<li><p>Broker</p>
<ul>
<li>一台Kafka服务器就是一个Broker</li>
<li>一个集群由多个Broker组成。一个Broker可以容纳多个Topic</li>
</ul>
</li>
<li><p>Partition</p>
<p>为了实现扩展性，一个非常大的Topic可以分布到多个Broker（即服务器）上，一个Topic可以分为多个Partition，每个Partition是一个有序的队列。Prtition中的每条消息都会被分配一个有序的id（Offset）。Kafka只保证按一个Partition中的顺序将消息发给Consumer，不保证一个Topic的整体（多个Partition间）的顺序。</p>
</li>
<li><p>Offset</p>
<ul>
<li>kafka的存储文件都是按照offset.kafka来命名，用offset做名字的好处是方便查找</li>
<li>例如你想找位于2049的位置，只要找到2048.kafka的文件即可，当然the first offset就是00000000000.kafka</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>特性</p>
<ul>
<li>通过O(1)的磁盘数据结构提供消息的持久化，这种结构对于即使数以TB的消息存储也能够保持长时间的稳定性能</li>
<li>高吞吐量：即使是非常普通的硬件Kafka也可以支持每秒数十万的消息</li>
<li>支持 <em>同步</em> 和 <em>异步</em> 复制两种HA</li>
<li>Consumer客户端<ul>
<li>pull</li>
<li>随机读</li>
<li>利用sendfile系统调用</li>
<li>zero-copy</li>
<li>批量拉数据</li>
</ul>
</li>
<li>消费状态保存在客户端</li>
<li>消息存储顺序写</li>
<li>数据迁移、扩容对用户透明</li>
<li>支持Hadoop并行数据加载</li>
<li>支持online和offline的场景</li>
<li>持久化：通过将数据持久化到硬盘以及replication防止数据丢失</li>
<li>scale out：无需停机即可扩展机器</li>
<li>定期删除机制，支持设定partitions的segment file保留时间</li>
</ul>
</li>
<li><p>可靠性（一致性)</p>
<p>传统的MQ系统通常都是通过broker和consumer间的确认（ack）机制实现的，并在broker保存消息分发的状态，即使这样一致性也是很难保证的。</p>
<p>Kafka的做法是由consumer自己保存状态，也不要任何确认。这样虽然consumer负担更重，但其实更灵活了。因为不管consumer上任何原因导致需要重新处理消息，都可以再次从broker获得。</p>
</li>
<li><p>可扩展性</p>
<p>Kafka 使用Zookeeper实现动态的集群扩展，不需要更改客户端（生产者和消费者）的配置。broker会在ZK注册并保持相关的元数据更新。而客户端会在ZK上注册相关的watcher，一旦ZK发生变化，客户端能及时做出相应调整。这样可以保证变更broker时，各个broker之间能自动实现负载均衡。</p>
</li>
<li><p>设计目标</p>
<p>高吞吐量</p>
<ul>
<li>数据磁盘持久化：消息不在内存中cache，直接写入到磁盘，充分利用磁盘的顺序读写性能</li>
<li>zero-copy：减少IO操作步骤</li>
<li>支持数据批量发送和拉取</li>
<li>支持数据压缩</li>
<li>Topic划分为多个partition，提高并行处理能力</li>
</ul>
</li>
<li><p>Producer负载均衡和HA机制</p>
<ul>
<li>producer根据用户指定的算法，将消息发送到指定的partition。</li>
<li>存在多个partiiton，每个partition有自己的replica，每个replica分布在不同的Broker节点上。</li>
<li>多个partition需要选取出lead partition，lead partition负责读写，并由zookeeper负责fail over。</li>
<li>通过zookeeper管理broker与consumer的动态加入与离开。</li>
</ul>
</li>
<li><p>Consumer的pull机制</p>
<p>由于broker会持久化数据，broker没有cache压力，因此，consumer比较适合才去pull的方式消费数据：</p>
<ul>
<li>简化kafka设计，降低了难度</li>
<li>Consumer根据消费能力自主控制消息拉取速度</li>
<li>Consumer根据自身情况自主选择消费模式，例如批量，重复消费，从制定partition或位置(offset)开始消费等</li>
</ul>
</li>
<li><p>Consumer与Topic关系以及机制</p>
<p>每个group包含多个consumer。对于topic中的一条特定消息，只会被订阅此Topic每个group中的一个consumer消费，那么一个group中的所有consumer将会交错的消费整个Topic。</p>
<p>如果所有的consumer都具有相同的group（类似JMS queue），消息将有所有的consumer负载均衡</p>
<p>如果所有的consumer都具有不同的group，那么这就是『发布-订阅』，消息将会广播给所有消费者</p>
<p>在Kafka中，一个partition中的消息只会被group中的一个consumer消费（同一时刻）；每个group中consumer消息消费互相独立；<br>一个group是一个『订阅』者，一个Topic中的每个partition只会被一个『订阅』者中的一个consumer消费，但是一个consumer可以同事消费多个partitions中的消息。</p>
<p>Kafka只能保证一个partition中的消息被某个consumer消费是顺序的，但是从Topic角度，当有多个partitions时，消息仍不是全局有序的</p>
<p>一个group中包含多个consumer，这样的话不仅能提高topic中消息的并发消费能力，还能提高『故障容错』性，如果group中的某个consumer失效，那么其消费的partition将会被其他consumer接管</p>
<p>Kafka的设计原理决定，对于一个Topic，同一个group中不能有多于partition个数的consumer同时消费，否则将意味着某些consumer将无法得到消息</p>
</li>
<li><p>Producer均衡算法</p>
<p>Kafka集群中的任何一个broker，都可以向producer提供metadata，这些metadata中包含『集群中存货的servers/partition leaders』，当producer获取到metadata后，会和topic下所有的partition leader保持socker连接；消息由producer直接通过socker发送到broker</p>
<blockquote>
<p>中间不会经过任何『路由层』，即，消息被路由到哪个partition上，是有producer决定的<br>在producer端的配置文件中，可以指定partition的路由方式：『random』，『key-hash』等</p>
</blockquote>
</li>
<li><p>Consumer均衡算法</p>
<p>当一个group中，有consumer加入或者离开时，会触发partitions均衡。均衡的最终目的，是提升topic的并发消费能力。</p>
<ul>
<li>假如topic1,具有如下partitions: P0,P1,P2,P3</li>
<li>加入group中,有如下consumer: C0,C1</li>
<li>首先根据partition索引号对partitions排序: P0,P1,P2,P3</li>
<li>根据consumer.id排序: C0,C1</li>
<li>计算倍数: M = [P0,P1,P2,P3].size / [C0,C1].size,本例值M=2(向上取整)</li>
<li>然后依次分配partitions: C0 = [P0,P1],C1=[P2,P3],即Ci = [P(i <em> M),P((i + 1) </em> M -1)]</li>
</ul>
</li>
<li><p>Broker集群内broker之间replica机制</p>
<p>replication策略是基于partiton，而不是topic</p>
<blockquote>
<p>kafka将每个partition复制到多个server上<br>任何一个partition有一个leader和任意数量的follower<br>备份的数量可以由broker配置文件设定<br>leader处理所有的read-write请求，负责跟踪所有的follower状态，<br>如果follower『落后』太多或者失效，leader会把它从replicas同步列表中删除<br>follower需要和leader保持同步，follower就像一个consumer，消费信息并保存在本地日志中<br>当所有的follower都将一个消息保存成功，此消息才能被认为是『committed』，<br>此时consumer才能消费它，这种策略要求leader和follower之间保持良好的网络环境<br>只要ZK集群存活，即使只存活一个replica，仍可以保证消息的正常发送和接收</p>
</blockquote>
<ul>
<li>Kafka判定一个follower存活的条件<ul>
<li>和ZK保持良好的链接</li>
<li>及时跟进leader，不能落后太多</li>
</ul>
</li>
</ul>
<blockquote>
<p>如果此replicas落后太多，它会继续在leader中fetch数据，然后加入同步列表中，<br>Kafka不会更换宿主，只有这样才能保证replicas足够快，才能保证producer发布消息时接收ACK的延迟较小</p>
</blockquote>
<ul>
<li>当leader失效，需要考虑负载均衡，partition leader较少的broker更有可能成为新的leader，因为<ul>
<li>不能采用『投票多数派』的算法，因为这种算法对于『网络稳定性/投票参与者数量』要求较高</li>
<li>Kafka集群设计中，容忍N-1个replicas失效</li>
<li>每个partiton中所有的replica信息都可以在ZK中获得，那么选择leader是非常简单的</li>
<li>选择follower时需要注意：避免新的leader server上承载的partiton leader的个数过多，否则此server将承受更多的IO压力</li>
</ul>
</li>
</ul>
</li>
<li><p>总结</p>
<ul>
<li>Producer端直接连接broker列表，从列表中返回TopicMetadataResponse，该Metadata包含Topic下每个partition leader建立socket连接并发送消息。</li>
<li>Broker端使用ZK用来注册broker信息，以及监控partition leader存活性。</li>
<li>Consumer端使用ZK用来注册consumer信息，其中包括consumer消费的partition列表等，同时也用来发现broker列表，并和partition leader建立socket连接，并获取消息。</li>
</ul>
</li>
</ul>
<h4 id="Kafka_u5728Zookeeper_u4E2D_u5B58_u50A8_u7ED3_u6784"><a href="#Kafka_u5728Zookeeper_u4E2D_u5B58_u50A8_u7ED3_u6784" class="headerlink" title="Kafka在Zookeeper中存储结构"></a>Kafka在Zookeeper中存储结构</h4><ul>
<li><p>结构图</p>
<img src="/images/kafka.0.9.0/kafka_in_zk.png" title="kafka在ZK中的存储结构图">
</li>
</ul>
<h4 id="Kafka__u5B89_u88C5_u548C_u914D_u7F6E"><a href="#Kafka__u5B89_u88C5_u548C_u914D_u7F6E" class="headerlink" title="Kafka 安装和配置"></a>Kafka 安装和配置</h4><h4 id="u53C2_u8003_u6587_u732E"><a href="#u53C2_u8003_u6587_u732E" class="headerlink" title="参考文献"></a>参考文献</h4><ul>
<li><a href="http://blog.csdn.net/zhongwen7710/article/details/41252649" target="_blank" rel="external">http://blog.csdn.net/zhongwen7710/article/details/41252649</a></li>
<li><a href="http://kafka.apache.org/documentation.html#brokerconfigs" target="_blank" rel="external">http://kafka.apache.org/documentation.html#brokerconfigs</a></li>
</ul>
]]></content>
    <summary type="html">
    <![CDATA[<!--  -->
<!--  -->
<!--  _ __    __     __      ____    ___     ___   _____   __  __    ___     -->
<!-- /\`'__\/'__`\ /'__`\   /',__\  / _]]>
    </summary>
    
      <category term="Apache" scheme="http://reasonpun.com/tags/Apache/"/>
    
      <category term="HDFS" scheme="http://reasonpun.com/tags/HDFS/"/>
    
      <category term="Kafka" scheme="http://reasonpun.com/tags/Kafka/"/>
    
      <category term="数据挖掘" scheme="http://reasonpun.com/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[kafka文档简述]]></title>
    <link href="http://reasonpun.com/2015/12/05/kafa-documentation/"/>
    <id>http://reasonpun.com/2015/12/05/kafa-documentation/</id>
    <published>2015-12-05T08:47:08.000Z</published>
    <updated>2015-12-25T03:33:49.000Z</updated>
    <content type="html"><![CDATA[<!--  -->
<!--  -->
<!--  _ __    __     __      ____    ___     ___   _____   __  __    ___     -->
<!-- /\`'__\/'__`\ /'__`\   /',__\  / __`\ /' _ `\/\ '__`\/\ \/\ \ /' _ `\   -->
<!-- \ \ \//\  __//\ \L\.\_/\__, `\/\ \L\ \/\ \/\ \ \ \L\ \ \ \_\ \/\ \/\ \  -->
<!--  \ \_\\ \____\ \__/.\_\/\____/\ \____/\ \_\ \_\ \ ,__/\ \____/\ \_\ \_\ -->
<!--   \/_/ \/____/\/__/\/_/\/___/  \/___/  \/_/\/_/\ \ \/  \/___/  \/_/\/_/ -->
<!--                                                 \ \_\                   -->
<!--                                                  \/_/                   -->
<!--  -->
<h3 id="u7FFB_u8BD1ing"><a href="#u7FFB_u8BD1ing" class="headerlink" title="翻译ing"></a>翻译ing</h3><h2 id="u672F_u8BED_u8868"><a href="#u672F_u8BED_u8868" class="headerlink" title="术语表"></a>术语表</h2><table>
<thead>
<tr>
<th>名词术语</th>
<th>解释</th>
</tr>
</thead>
<tbody>
<tr>
<td>topic</td>
<td>主题</td>
</tr>
<tr>
<td>producer</td>
<td>生产者</td>
</tr>
<tr>
<td>comsumer</td>
<td>消费者</td>
</tr>
<tr>
<td>broker</td>
<td>代理</td>
</tr>
<tr>
<td>partition</td>
<td>分区</td>
</tr>
<tr>
<td>offset</td>
<td>下标</td>
</tr>
<tr>
<td>round-robin</td>
<td>轮询</td>
</tr>
<tr>
<td>key</td>
<td>键</td>
</tr>
<tr>
<td>queuing</td>
<td>排队</td>
</tr>
<tr>
<td>publish-subscribe</td>
<td>发布-订阅</td>
</tr>
<tr>
<td>the consumer group</td>
<td>消费者组</td>
</tr>
<tr>
<td>subscriber</td>
<td>订阅者</td>
</tr>
<tr>
<td>guarantee</td>
<td>保证（?）</td>
</tr>
</tbody>
</table>
<h2 id="Kafka_0-9-0__u6587_u6863"><a href="#Kafka_0-9-0__u6587_u6863" class="headerlink" title="Kafka 0.9.0 文档"></a>Kafka 0.9.0 文档</h2><ol>
<li>开始<ul>
<li>1.1 简介</li>
<li>1.2 用例</li>
<li>1.3 快速入门</li>
<li>1.4 生态圈</li>
<li>1.5 升级</li>
</ul>
</li>
<li>API<ul>
<li>2.1 生产者 API</li>
<li>2.2 消费者 API<ul>
<li>2.2.1 Old High Level Consumer API</li>
<li>2.2.2 Old Simple Consumer API</li>
<li>2.2.3 New Consumer API</li>
</ul>
</li>
</ul>
</li>
<li>配置<ul>
<li>3.1 代理配置</li>
<li>3.2 生产者配置</li>
<li>3.3 消费者配置<ul>
<li>3.3.1 Old Consumer Configs</li>
<li>3.3.2 New Consumer Configs</li>
<li>3.4 Kafka 连接配置</li>
</ul>
</li>
</ul>
</li>
<li>设计<ul>
<li>4.1 Motivation</li>
<li>4.2 持久化</li>
<li>4.3 Efficiency</li>
<li>4.4 生产者</li>
<li>4.5 消费者</li>
<li>4.6 Message Delivery Semantics</li>
<li>4.7 Replication</li>
<li>4.8 Log Compaction</li>
<li>4.9 Quotas</li>
</ul>
</li>
<li>实现<ul>
<li>5.1 API 设计</li>
<li>5.2 网络层</li>
<li>5.3 消息</li>
<li>5.4 消息格式</li>
<li>5.5 日志</li>
<li>5.6 分布式</li>
</ul>
</li>
<li>Operations<ul>
<li>6.1 Basic Kafka Operations<ul>
<li>Adding and removing topics</li>
<li>Modifying topics</li>
<li>Graceful shutdown</li>
<li>Balancing leadership</li>
<li>Checking consumer position</li>
<li>Mirroring data between clusters</li>
<li>Expanding your cluster</li>
<li>Decommissioning brokers</li>
<li>Increasing replication factor</li>
</ul>
</li>
<li>6.2 Datacenters</li>
<li>6.3 Important Configs<ul>
<li>Important Server Configs</li>
<li>Important Client Configs</li>
<li>A Production Server Configs</li>
</ul>
</li>
<li>6.4 Java Version</li>
<li>6.5 Hardware and OS<ul>
<li>OS</li>
<li>Disks and Filesystems</li>
<li>Application vs OS Flush Management</li>
<li>Linux Flush Behavior</li>
<li>Ext4 Notes</li>
</ul>
</li>
<li>6.6 Monitoring</li>
<li>6.7 ZooKeeper<ul>
<li>Stable Version</li>
<li>Operationalization</li>
</ul>
</li>
</ul>
</li>
<li>安全性<ul>
<li>7.1 Security Overview</li>
<li>7.2 Encryption and Authentication using SSL</li>
<li>7.3 Authentication using SASL</li>
<li>7.4 Authorization and ACLs</li>
<li>7.5 ZooKeeper Authentication<ul>
<li>New Clusters</li>
<li>Migrating Clusters</li>
<li>Migrating the ZooKeeper Ensemble</li>
</ul>
</li>
</ul>
</li>
<li>Kafka Connect<ul>
<li>8.1 Overview</li>
<li>8.2 User Guide</li>
<li>8.3 Connector Development Guide</li>
</ul>
</li>
</ol>
<h2 id="1-__u5F00_u59CB"><a href="#1-__u5F00_u59CB" class="headerlink" title="1. 开始"></a>1. 开始</h2><h3 id="1-1__u7B80_u4ECB"><a href="#1-1__u7B80_u4ECB" class="headerlink" title="1.1 简介"></a>1.1 简介</h3><p>kafka是一个分布式的，分区的，复用的日志提交服务。它以一种独特的设计方式提供消息传递系统的功能。</p>
<p>这是什么意思呢？</p>
<p>First let’s review some basic messaging terminology:</p>
<ul>
<li>Kafka maintains feeds of messages in categories called topics.</li>
<li>We’ll call processes that publish messages to a Kafka topic producers.</li>
<li>We’ll call processes that subscribe to topics and process the feed of published messages consumers..</li>
<li>Kafka is run as a cluster comprised of one or more servers each of which is called a broker.</li>
</ul>
<p>So, at a high level, producers send messages over the network to the Kafka cluster which in turn serves them up to consumers like this:</p>
<img src="/images/kafka.0.9.0/producer_consumer.png" width="258" height="180" title="生产者消费者关系">
<p>Communication between the clients and the servers is done with a simple, high-performance, language agnostic TCP protocol. We provide a Java client for Kafka, but clients are available in many languages.</p>
<p>服务器端和客户端的通讯是通过一个简单的，高效的，TCP协议无关的语言实现的。不仅提供了Java客户端，还提供了其他很多语言的支持。</p>
<h4 id="Topics_and_Logs"><a href="#Topics_and_Logs" class="headerlink" title="Topics and Logs"></a>Topics and Logs</h4><p>Let’s first dive into the high-level abstraction Kafka provides—the topic.<br>A topic is a category or feed name to which messages are published. For each topic, the Kafka cluster maintains a partitioned log that looks like this:</p>
<img src="/images/kafka.0.9.0/producer_consumer.png" width="258" height="180" title="生产者消费者关系">
<p>Each partition is an ordered, immutable sequence of messages that is continually appended to—a commit log. The messages in the partitions are each assigned a sequential id number called the offset that uniquely identifies each message within the partition.<br>The Kafka cluster retains all published messages—whether or not they have been consumed—for a configurable period of time. For example if the log retention is set to two days, then for the two days after a message is published it is available for consumption, after which it will be discarded to free up space. Kafka’s performance is effectively constant with respect to data size so retaining lots of data is not a problem.</p>
<p>In fact the only metadata retained on a per-consumer basis is the position of the consumer in the log, called the “offset”. This offset is controlled by the consumer: normally a consumer will advance its offset linearly as it reads messages, but in fact the position is controlled by the consumer and it can consume messages in any order it likes. For example a consumer can reset to an older offset to reprocess.</p>
<p>This combination of features means that Kafka consumers are very cheap—they can come and go without much impact on the cluster or on other consumers. For example, you can use our command line tools to “tail” the contents of any topic without changing what is consumed by any existing consumers.</p>
<p>The partitions in the log serve several purposes. First, they allow the log to scale beyond a size that will fit on a single server. Each individual partition must fit on the servers that host it, but a topic may have many partitions so it can handle an arbitrary amount of data. Second they act as the unit of parallelism—more on that in a bit.</p>
<h4 id="Distribution"><a href="#Distribution" class="headerlink" title="Distribution"></a>Distribution</h4><p>The partitions of the log are distributed over the servers in the Kafka cluster with each server handling data and requests for a share of the partitions. Each partition is replicated across a configurable number of servers for fault tolerance.<br>Each partition has one server which acts as the “leader” and zero or more servers which act as “followers”. The leader handles all read and write requests for the partition while the followers passively replicate the leader. If the leader fails, one of the followers will automatically become the new leader. Each server acts as a leader for some of its partitions and a follower for others so load is well balanced within the cluster.</p>
<h4 id="Producers"><a href="#Producers" class="headerlink" title="Producers"></a>Producers</h4><p>Producers publish data to the topics of their choice. The producer is responsible for choosing which message to assign to which partition within the topic. This can be done in a round-robin fashion simply to balance load or it can be done according to some semantic partition function (say based on some key in the message). More on the use of partitioning in a second.</p>
<h4 id="Consumers"><a href="#Consumers" class="headerlink" title="Consumers"></a>Consumers</h4><p>Messaging traditionally has two models: queuing and publish-subscribe. In a queue, a pool of consumers may read from a server and each message goes to one of them; in publish-subscribe the message is broadcast to all consumers. Kafka offers a single consumer abstraction that generalizes both of these—the consumer group.<br>Consumers label themselves with a consumer group name, and each message published to a topic is delivered to one consumer instance within each subscribing consumer group. Consumer instances can be in separate processes or on separate machines.</p>
<p>If all the consumer instances have the same consumer group, then this works just like a traditional queue balancing load over the consumers.</p>
<p>If all the consumer instances have different consumer groups, then this works like publish-subscribe and all messages are broadcast to all consumers.</p>
<p>More commonly, however, we have found that topics have a small number of consumer groups, one for each “logical subscriber”. Each group is composed of many consumer instances for scalability and fault tolerance. This is nothing more than publish-subscribe semantics where the subscriber is cluster of consumers instead of a single process.</p>
<p>Kafka has stronger ordering guarantees than a tranditional messageing system, too.</p>
<img src="/images/kafka.0.9.0/kafka_cluster.png" title="kafka_cluster">
<p>A traditional queue retains messages in-order on the server, and if multiple consumers consume from the queue then the server hands out messages in the order they are stored. However, although the server hands out messages in order, the messages are delivered asynchronously to consumers, so they may arrive out of order on different consumers. This effectively means the ordering of the messages is lost in the presence of parallel consumption. Messaging systems often work around this by having a notion of “exclusive consumer” that allows only one process to consume from a queue, but of course this means that there is no parallelism in processing.</p>
<p>Kafka does it better. By having a notion of parallelism—the partition—within the topics, Kafka is able to provide both ordering guarantees and load balancing over a pool of consumer processes. This is achieved by assigning the partitions in the topic to the consumers in the consumer group so that each partition is consumed by exactly one consumer in the group. By doing this we ensure that the consumer is the only reader of that partition and consumes the data in order. Since there are many partitions this still balances the load over many consumer instances. Note however that there cannot be more consumer instances in a consumer group than partitions.</p>
<p>Kafka only provides a total order over messages within a partition, not between different partitions in a topic. Per-partition ordering combined with the ability to partition data by key is sufficient for most applications. However, if you require a total order over messages this can be achieved with a topic that has only one partition, though this will mean only one consumer process per consumer group.</p>
<h4 id="Guarantees"><a href="#Guarantees" class="headerlink" title="Guarantees"></a>Guarantees</h4><p>At a high-level Kafka gives the following guarantees:</p>
<ul>
<li>Messages sent by a producer to a particular topic partition will be appended in the order they are sent. That is, if a message M1 is sent by the same producer as a message M2, and M1 is sent first, then M1 will have a lower offset than M2 and appear earlier in the log.</li>
<li>A consumer instance sees messages in the order they are stored in the log.</li>
<li>For a topic with replication factor N, we will tolerate up to N-1 server failures without losing any messages committed to the log.<br>More details on these guarantees are given in the design section of the documentation.</li>
</ul>
<h3 id="1-2_Use_Cases"><a href="#1-2_Use_Cases" class="headerlink" title="1.2 Use Cases"></a>1.2 Use Cases</h3><p>Here is a description of a few of the popular use cases for Apache Kafka. For an overview of a number of these areas in action, see this blog post.</p>
<h4 id="Messaging"><a href="#Messaging" class="headerlink" title="Messaging"></a>Messaging</h4><p>Kafka works well as a replacement for a more traditional message broker. Message brokers are used for a variety of reasons (to decouple processing from data producers, to buffer unprocessed messages, etc). In comparison to most messaging systems Kafka has better throughput, built-in partitioning, replication, and fault-tolerance which makes it a good solution for large scale message processing applications.<br>In our experience messaging uses are often comparatively low-throughput, but may require low end-to-end latency and often depend on the strong durability guarantees Kafka provides.</p>
<p>In this domain Kafka is comparable to traditional messaging systems such as ActiveMQ or RabbitMQ.</p>
<h4 id="Website_Activity_Tracking"><a href="#Website_Activity_Tracking" class="headerlink" title="Website Activity Tracking"></a>Website Activity Tracking</h4><p>The original use case for Kafka was to be able to rebuild a user activity tracking pipeline as a set of real-time publish-subscribe feeds. This means site activity (page views, searches, or other actions users may take) is published to central topics with one topic per activity type. These feeds are available for subscription for a range of use cases including real-time processing, real-time monitoring, and loading into Hadoop or offline data warehousing systems for offline processing and reporting.<br>Activity tracking is often very high volume as many activity messages are generated for each user page view.</p>
<h4 id="Metrics"><a href="#Metrics" class="headerlink" title="Metrics"></a>Metrics</h4><p>Kafka is often used for operational monitoring data. This involves aggregating statistics from distributed applications to produce centralized feeds of operational data.</p>
<h4 id="Log_Aggregation"><a href="#Log_Aggregation" class="headerlink" title="Log Aggregation"></a>Log Aggregation</h4><p>Many people use Kafka as a replacement for a log aggregation solution. Log aggregation typically collects physical log files off servers and puts them in a central place (a file server or HDFS perhaps) for processing. Kafka abstracts away the details of files and gives a cleaner abstraction of log or event data as a stream of messages. This allows for lower-latency processing and easier support for multiple data sources and distributed data consumption. In comparison to log-centric systems like Scribe or Flume, Kafka offers equally good performance, stronger durability guarantees due to replication, and much lower end-to-end latency.</p>
<h4 id="Stream_Processing"><a href="#Stream_Processing" class="headerlink" title="Stream Processing"></a>Stream Processing</h4><p>Many users end up doing stage-wise processing of data where data is consumed from topics of raw data and then aggregated, enriched, or otherwise transformed into new Kafka topics for further consumption. For example a processing flow for article recommendation might crawl article content from RSS feeds and publish it to an “articles” topic; further processing might help normalize or deduplicate this content to a topic of cleaned article content; a final stage might attempt to match this content to users. This creates a graph of real-time data flow out of the individual topics. Storm and Samza are popular frameworks for implementing these kinds of transformations.</p>
<h4 id="Event_Sourcing"><a href="#Event_Sourcing" class="headerlink" title="Event Sourcing"></a>Event Sourcing</h4><p>Event sourcing is a style of application design where state changes are logged as a time-ordered sequence of records. Kafka’s support for very large stored log data makes it an excellent backend for an application built in this style.</p>
<h4 id="Commit_Log"><a href="#Commit_Log" class="headerlink" title="Commit Log"></a>Commit Log</h4><p>Kafka can serve as a kind of external commit-log for a distributed system. The log helps replicate data between nodes and acts as a re-syncing mechanism for failed nodes to restore their data. The log compaction feature in Kafka helps support this usage. In this usage Kafka is similar to Apache BookKeeper project.</p>
<h3 id="1-3_Quick_Start"><a href="#1-3_Quick_Start" class="headerlink" title="1.3 Quick Start"></a>1.3 Quick Start</h3><p>This tutorial assumes you are starting fresh and have no existing Kafka or ZooKeeper data.</p>
<h4 id="Step_1_3A_Download_the_code"><a href="#Step_1_3A_Download_the_code" class="headerlink" title="Step 1: Download the code"></a>Step 1: Download the code</h4><p>Download the 0.9.0.0 release and un-tar it.</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="tag">tar</span> <span class="tag">-xzf</span> <span class="tag">kafka_2</span><span class="class">.11-0</span><span class="class">.9</span><span class="class">.0</span><span class="class">.0</span><span class="class">.tgz</span></span><br><span class="line">&gt; <span class="tag">cd</span> <span class="tag">kafka_2</span><span class="class">.11-0</span><span class="class">.9</span><span class="class">.0</span><span class="class">.0</span></span><br></pre></td></tr></table></figure>
<h4 id="Step_2_3A_Start_the_server"><a href="#Step_2_3A_Start_the_server" class="headerlink" title="Step 2: Start the server"></a>Step 2: Start the server</h4><p>Kafka uses ZooKeeper so you need to first start a ZooKeeper server if you don’t already have one. You can use the convenience script packaged with kafka to get a quick-and-dirty single-node ZooKeeper instance.</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/zookeeper-server-start<span class="class">.sh</span> config/zookeeper<span class="class">.properties</span></span><br><span class="line">[<span class="number">2013</span>-<span class="number">04</span>-<span class="number">22</span> <span class="number">15</span>:<span class="number">01</span>:<span class="number">37</span>,<span class="number">495</span>] INFO Reading configuration from: config/zookeeper<span class="class">.properties</span> (org<span class="class">.apache</span><span class="class">.zookeeper</span><span class="class">.server</span><span class="class">.quorum</span><span class="class">.QuorumPeerConfig</span>)</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<h4 id="Step_3_3A_Create_a_topic"><a href="#Step_3_3A_Create_a_topic" class="headerlink" title="Step 3: Create a topic"></a>Step 3: Create a topic</h4><p>Let’s create a topic named “test” with a single partition and only one replica:</p>
<figure class="highlight brainfuck"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="comment">bin/kafka</span><span class="literal">-</span><span class="comment">topics</span><span class="string">.</span><span class="comment">sh</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">create</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">zookeeper</span> <span class="comment">localhost:2181</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">replication</span><span class="literal">-</span><span class="comment">factor</span> <span class="comment">1</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">partitions</span> <span class="comment">1</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">topic</span> <span class="comment">test</span></span><br></pre></td></tr></table></figure>
<p>We can now see that topic if we run the list topic command:</p>
<figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/kafka-topics.sh <span class="comment">--list --zookeeper localhost:2181</span></span><br><span class="line">test</span><br><span class="line">Alternatively, instead <span class="operator">of</span> manually creating topics you can also configure your brokers <span class="built_in">to</span> auto-<span class="built_in">create</span> topics when <span class="operator">a</span> non-existent topic is published <span class="built_in">to</span>.</span><br></pre></td></tr></table></figure>
<h4 id="Step_4_3A_Send_some_messages"><a href="#Step_4_3A_Send_some_messages" class="headerlink" title="Step 4: Send some messages"></a>Step 4: Send some messages</h4><p>Kafka comes with a command line client that will take input from a file or from standard input and send it out as messages to the Kafka cluster. By default each line will be sent as a separate message.<br>Run the producer and then type a few messages into the console to send to the server.</p>
<figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/kafka-console-producer.<span class="keyword">sh</span> --broker-<span class="keyword">list</span> localhos<span class="variable">t:9092</span> --topic test</span><br><span class="line">This <span class="keyword">is</span> <span class="keyword">a</span> message</span><br><span class="line">This <span class="keyword">is</span> another message</span><br></pre></td></tr></table></figure>
<h4 id="Step_5_3A_Start_a_consumer"><a href="#Step_5_3A_Start_a_consumer" class="headerlink" title="Step 5: Start a consumer"></a>Step 5: Start a consumer</h4><p>Kafka also has a command line consumer that will dump out messages to standard output.</p>
<figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/kafka-console-consumer.sh <span class="comment">--zookeeper localhost:2181 --topic test --from-beginning</span></span><br><span class="line">This is <span class="operator">a</span> message</span><br><span class="line">This is another message</span><br><span class="line">If you have <span class="keyword">each</span> <span class="operator">of</span> <span class="operator">the</span> above commands running <span class="operator">in</span> <span class="operator">a</span> different terminal <span class="keyword">then</span> you should now be able <span class="built_in">to</span> type messages <span class="keyword">into</span> <span class="operator">the</span> producer terminal <span class="operator">and</span> see them appear <span class="operator">in</span> <span class="operator">the</span> consumer terminal.</span><br></pre></td></tr></table></figure>
<p>All of the command line tools have additional options; running the command with no arguments will display usage information documenting them in more detail.</p>
<h4 id="Step_6_3A_Setting_up_a_multi-broker_cluster"><a href="#Step_6_3A_Setting_up_a_multi-broker_cluster" class="headerlink" title="Step 6: Setting up a multi-broker cluster"></a>Step 6: Setting up a multi-broker cluster</h4><p>So far we have been running against a single broker, but that’s no fun. For Kafka, a single broker is just a cluster of size one, so nothing much changes other than starting a few more broker instances. But just to get feel for it, let’s expand our cluster to three nodes (still all on our local machine).<br>First we make a config file for each of the brokers:</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; cp config/server<span class="class">.properties</span> config/server-<span class="number">1</span><span class="class">.properties</span></span><br><span class="line">&gt; cp config/server<span class="class">.properties</span> config/server-<span class="number">2</span>.properties</span><br></pre></td></tr></table></figure>
<p>Now edit these new files and set the following properties:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">config/server-<span class="number">1.</span>properties:</span><br><span class="line">    broker.id=<span class="number">1</span></span><br><span class="line">    port=<span class="number">9093</span></span><br><span class="line">    <span class="built_in">log</span>.dir=/tmp/kafka-logs-<span class="number">1</span></span><br><span class="line"></span><br><span class="line">config/server-<span class="number">2.</span>properties:</span><br><span class="line">    broker.id=<span class="number">2</span></span><br><span class="line">    port=<span class="number">9094</span></span><br><span class="line">    <span class="built_in">log</span>.dir=/tmp/kafka-logs-<span class="number">2</span></span><br></pre></td></tr></table></figure>
<p>The broker.id property is the unique and permanent name of each node in the cluster. We have to override the port and log directory only because we are running these all on the same machine and we want to keep the brokers from all trying to register on the same port or overwrite each others data.</p>
<p>We already have Zookeeper and our single node started, so we just need to start the two new nodes:</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/kafka-server-start<span class="class">.sh</span> config/server-<span class="number">1</span><span class="class">.properties</span> &amp;</span><br><span class="line">...</span><br><span class="line">&gt; bin/kafka-server-start<span class="class">.sh</span> config/server-<span class="number">2</span><span class="class">.properties</span> &amp;</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>Now create a new topic with a replication factor of three:</p>
<figure class="highlight brainfuck"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="comment">bin/kafka</span><span class="literal">-</span><span class="comment">topics</span><span class="string">.</span><span class="comment">sh</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">create</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">zookeeper</span> <span class="comment">localhost:2181</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">replication</span><span class="literal">-</span><span class="comment">factor</span> <span class="comment">3</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">partitions</span> <span class="comment">1</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">topic</span> <span class="comment">my</span><span class="literal">-</span><span class="comment">replicated</span><span class="literal">-</span><span class="comment">topic</span></span><br></pre></td></tr></table></figure>
<p>Okay but now that we have a cluster how can we know which broker is doing what? To see that run the “describe topics” command:</p>
<figure class="highlight brainfuck"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="comment">bin/kafka</span><span class="literal">-</span><span class="comment">topics</span><span class="string">.</span><span class="comment">sh</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">describe</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">zookeeper</span> <span class="comment">localhost:2181</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">topic</span> <span class="comment">my</span><span class="literal">-</span><span class="comment">replicated</span><span class="literal">-</span><span class="comment">topic</span></span><br></pre></td></tr></table></figure>
<p>Topic:my-replicated-topic    PartitionCount:1    ReplicationFactor:3    Configs:</p>
<pre><code>Topic: my-replicated-topic    Partition: 0    Leader: 1    Replicas: 1,2,0    Isr: 1,2,0
</code></pre><p>Here is an explanation of output. The first line gives a summary of all the partitions, each additional line gives information about one partition. Since we have only one partition for this topic there is only one line.</p>
<ul>
<li>“leader” is the node responsible for all reads and writes for the given partition. Each node will be the leader for a randomly selected portion of the partitions.</li>
<li>“replicas” is the list of nodes that replicate the log for this partition regardless of whether they are the leader or even if they are currently alive.</li>
<li>“isr” is the set of “in-sync” replicas. This is the subset of the replicas list that is currently alive and caught-up to the leader.</li>
</ul>
<p>Note that in my example node 1 is the leader for the only partition of the topic.</p>
<p>We can run the same command on the original topic we created to see where it is:</p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/kafka-topics.sh --describe --zookeeper <span class="string">localhost:</span><span class="number">2181</span> --topic test</span><br><span class="line"><span class="label"></span><br><span class="line">Topic:</span>test	<span class="string">PartitionCount:</span><span class="number">1</span>	<span class="string">ReplicationFactor:</span><span class="number">1</span>	<span class="string">Configs:</span></span><br><span class="line"><span class="label">	Topic:</span> test	<span class="string">Partition:</span> <span class="number">0</span>	<span class="string">Leader:</span> <span class="number">0</span>	<span class="string">Replicas:</span> <span class="number">0</span>	<span class="string">Isr:</span> <span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>So there is no surprise there—the original topic has no replicas and is on server 0, the only server in our cluster when we created it.</p>
<p>Let’s publish a few messages to our new topic:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/kafka-console-producer.sh --broker-<span class="built_in">list</span> localhost:<span class="number">9092</span> --topic my-replicated-topic</span><br><span class="line">...</span><br><span class="line">my test message <span class="number">1</span></span><br><span class="line">my test message <span class="number">2</span></span><br><span class="line">^C</span><br></pre></td></tr></table></figure>
<p>Now let’s consume these messages:</p>
<figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/kafka-console-consumer.sh <span class="comment">--zookeeper localhost:2181 --from-beginning --topic my-replicated-topic</span></span><br><span class="line">...</span><br><span class="line"><span class="keyword">my</span> test message <span class="number">1</span></span><br><span class="line"><span class="keyword">my</span> test message <span class="number">2</span></span><br><span class="line">^C</span><br></pre></td></tr></table></figure>
<p>Now let’s test out fault-tolerance. Broker 1 was acting as the leader so let’s kill it:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; ps | grep server-<span class="number">1.</span>properties</span><br><span class="line"><span class="number">7564</span> ttys002    <span class="number">0</span>:<span class="number">15.91</span> /System/Library/Frameworks/JavaVM.framework/Versions/<span class="number">1.6</span>/Home/bin/java...</span><br><span class="line">&gt; kill -<span class="number">9</span> <span class="number">7564</span></span><br></pre></td></tr></table></figure>
<p>Leadership has switched to one of the slaves and node 1 is no longer in the in-sync replica set:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/kafka-topics.sh --describe --zookeeper localhost:<span class="number">2181</span> --topic my-replicated-topic</span><br><span class="line">Topic:my-replicated-topic	PartitionCount:<span class="number">1</span>	ReplicationFactor:<span class="number">3</span>	Configs:</span><br><span class="line">	Topic: my-replicated-topic	Partition: <span class="number">0</span>	Leader: <span class="number">2</span>	Replicas: <span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>	Isr: <span class="number">2</span>,<span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>But the messages are still be available for consumption even though the leader that took the writes originally is down:</p>
<figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/kafka-console-consumer.sh <span class="comment">--zookeeper localhost:2181 --from-beginning --topic my-replicated-topic</span></span><br><span class="line">...</span><br><span class="line"><span class="keyword">my</span> test message <span class="number">1</span></span><br><span class="line"><span class="keyword">my</span> test message <span class="number">2</span></span><br><span class="line">^C</span><br></pre></td></tr></table></figure>
<h4 id="Step_7_3A_Use_Kafka_Connect_to_import/export_data"><a href="#Step_7_3A_Use_Kafka_Connect_to_import/export_data" class="headerlink" title="Step 7: Use Kafka Connect to import/export data"></a>Step 7: Use Kafka Connect to import/export data</h4><p>Writing data from the console and writing it back to the console is a convenient place to start, but you’ll probably want to use data from other sources or export data from Kafka to other systems. For many systems, instead of writing custom integration code you can use Kafka Connect to import or export data. Kafka Connect is a tool included with Kafka that imports and exports data to Kafka. It is an extensible tool that runs connectors, which implement the custom logic for interacting with an external system. In this quickstart we’ll see how to run Kafka Connect with simple connectors that import data from a file to a Kafka topic and export data from a Kafka topic to a file. First, we’ll start by creating some seed data to test with:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="built_in">echo</span> <span class="operator">-e</span> <span class="string">"foo\nbar"</span> &gt; test.txt</span><br></pre></td></tr></table></figure>
<p>Next, we’ll start two connectors running in standalone mode, which means they run in a single, local, dedicated process. We provide three configuration files as parameters. The first is always the configuration for the Kafka Connect process, containing common configuration such as the Kafka brokers to connect to and the serialization format for data. The remaining configuration files each specify a connector to create. These files include a unique connector name, the connector class to instantiate, and any other configuration required by the connector.</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/connect-standalone<span class="class">.sh</span> config/connect-standalone<span class="class">.properties</span> config/connect-file-source<span class="class">.properties</span> config/connect-file-sink.properties</span><br></pre></td></tr></table></figure>
<p>These sample configuration files, included with Kafka, use the default local cluster configuration you started earlier and create two connectors: the first is a source connector that reads lines from an input file and produces each to a Kafka topic and the second is a sink connector that reads messages from a Kafka topic and produces each as a line in an output file. During startup you’ll see a number of log messages, including some indicating that the connectors are being instantiated. Once the Kafka Connect process has started, the source connector should start reading lines from</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">test</span><span class="class">.txt</span></span><br></pre></td></tr></table></figure>
<p>and producing them to the topic</p>
<figure class="highlight perl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">connect</span>-test</span><br></pre></td></tr></table></figure>
<p>, and the sink connector should start reading messages from the topic</p>
<figure class="highlight perl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">connect</span>-test</span><br></pre></td></tr></table></figure>
<p>and write them to the file</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">test<span class="class">.sink</span><span class="class">.txt</span></span><br></pre></td></tr></table></figure>
<p>. We can verify the data has been delivered through the entire pipeline by examining the contents of the output file:</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; cat test<span class="class">.sink</span><span class="class">.txt</span></span><br><span class="line">foo</span><br><span class="line">bar</span><br></pre></td></tr></table></figure>
<p>Note that the data is being stored in the Kafka topic</p>
<figure class="highlight perl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">connect</span>-test</span><br></pre></td></tr></table></figure>
<p>, so we can also run a console consumer to see the data in the topic (or use custom consumer code to process it):</p>
<figure class="highlight livescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/kafka-<span class="built_in">console</span>-consumer.sh --zookeeper <span class="attribute">localhost</span>:<span class="number">2181</span> --topic connect-test --<span class="keyword">from</span>-beginning</span><br><span class="line">&#123;<span class="string">"schema"</span>:&#123;<span class="string">"type"</span>:<span class="string">"string"</span>,<span class="string">"optional"</span>:<span class="literal">false</span>&#125;,<span class="string">"payload"</span>:<span class="string">"foo"</span>&#125;</span><br><span class="line">&#123;<span class="string">"schema"</span>:&#123;<span class="string">"type"</span>:<span class="string">"string"</span>,<span class="string">"optional"</span>:<span class="literal">false</span>&#125;,<span class="string">"payload"</span>:<span class="string">"bar"</span>&#125;</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>The connectors continue to process data, so we can add data to the file and see it move through the pipeline:</p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; echo <span class="string">"Another line"</span> <span class="prompt">&gt;&gt; </span>test.txt</span><br></pre></td></tr></table></figure>
<p>You should see the line appear in the console consumer output and in the sink file.</p>
<h3 id="1-4_Ecosystem"><a href="#1-4_Ecosystem" class="headerlink" title="1.4 Ecosystem"></a>1.4 Ecosystem</h3><p>There are a plethora of tools that integrate with Kafka outside the main distribution. The ecosystem page lists many of these, including stream processing systems, Hadoop integration, monitoring, and deployment tools.</p>
<h2 id="2-_API"><a href="#2-_API" class="headerlink" title="2. API"></a>2. API</h2><p>Apache Kafka includes new java clients (in the org.apache.kafka.clients package). These are meant to supplant the older Scala clients, but for compatability they will co-exist for some time. These clients are available in a seperate jar with minimal dependencies, while the old Scala clients remain packaged with the server.</p>
<h3 id="2-1_Producer_API"><a href="#2-1_Producer_API" class="headerlink" title="2.1 Producer API"></a>2.1 Producer API</h3><p>We encourage all new development to use the new Java producer. This client is production tested and generally both faster and more fully featured than the previous Scala client. You can use this client by adding a dependency on the client jar using the following example maven co-ordinates (you can change the version numbers with new releases):</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"> <span class="tag">&lt;<span class="title">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">groupId</span>&gt;</span>org.apache.kafka<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>kafka-clients<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">version</span>&gt;</span>0.9.0.0<span class="tag">&lt;/<span class="title">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>Examples showing how to use the producer are given in the <a href="http://kafka.apache.org/090/javadoc/index.html?org/apache/kafka/clients/producer/KafkaProducer.html" target="_blank" rel="external">javadocs</a>.<br>For those interested in the legacy Scala producer api, information can be found <a href="http://kafka.apache.org/081/documentation.html#producerapi" target="_blank" rel="external">here</a>.</p>
<h3 id="2-2_Consumer_API"><a href="#2-2_Consumer_API" class="headerlink" title="2.2 Consumer API"></a>2.2 Consumer API</h3><p>As of the 0.9.0 release we have added a new Java consumer to replace our existing high-level ZooKeeper-based consumer and low-level consumer APIs. This client is considered beta quality. To ensure a smooth upgrade paths for users, we still maintain the old 0.8 consumer clients that continue to work on an 0.9 Kafka cluster. In the following sections we introduce both the old 0.8 consumer APIs (both high-level ConsumerConnector and low-level SimpleConsumer) and the new Java consumer API respectively.</p>
<h4 id="2-2-1_Old_High_Level_Consumer_API"><a href="#2-2-1_Old_High_Level_Consumer_API" class="headerlink" title="2.2.1 Old High Level Consumer API"></a>2.2.1 Old High Level Consumer API</h4><figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line">class Consumer &#123;</span><br><span class="line">  /<span class="keyword">*</span><span class="keyword">*</span></span><br><span class="line">   <span class="keyword">*</span>  Create a ConsumerConnector</span><br><span class="line">   <span class="keyword">*</span></span><br><span class="line">   <span class="keyword">*</span>  <span class="comment">@param config  at the minimum, need to specify the groupid of the consumer and the zookeeper</span></span><br><span class="line">   <span class="keyword">*</span>                 connection string zookeeper.connect.</span><br><span class="line">   <span class="keyword">*</span>/</span><br><span class="line">  public static kafka.javaapi.consumer.ConsumerConnector createJavaConsumerConnector(ConsumerConfig config);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/<span class="keyword">*</span><span class="keyword">*</span></span><br><span class="line"> <span class="keyword">*</span>  V: type of the message</span><br><span class="line"> <span class="keyword">*</span>  K: type of the optional key assciated with the message</span><br><span class="line"> <span class="keyword">*</span>/</span><br><span class="line">public interface kafka.javaapi.consumer.ConsumerConnector &#123;</span><br><span class="line">  /<span class="keyword">*</span><span class="keyword">*</span></span><br><span class="line">   <span class="keyword">*</span>  Create a list of message streams of type T for each topic.</span><br><span class="line">   <span class="keyword">*</span></span><br><span class="line">   <span class="keyword">*</span>  <span class="comment">@param topicCountMap  a map of (topic, #streams) pair</span></span><br><span class="line">   <span class="keyword">*</span>  <span class="comment">@param decoder a decoder that converts from Message to T</span></span><br><span class="line">   <span class="keyword">*</span>  <span class="comment">@return a map of (topic, list of  KafkaStream) pairs.</span></span><br><span class="line">   <span class="keyword">*</span>          The number of items in the list is <span class="comment">#streams. Each stream supports</span></span><br><span class="line">   <span class="keyword">*</span>          an iterator over message/metadata pairs.</span><br><span class="line">   <span class="keyword">*</span>/</span><br><span class="line">  public <span class="variable">&lt;K,V&gt;</span> Map<span class="variable">&lt;String, List&lt;KafkaStream&lt;K,V&gt;</span>&gt;&gt;</span><br><span class="line">    createMessageStreams(Map<span class="variable">&lt;String, Integer&gt;</span> topicCountMap, Decoder<span class="variable">&lt;K&gt;</span> keyDecoder, Decoder<span class="variable">&lt;V&gt;</span> valueDecoder);</span><br><span class="line"></span><br><span class="line">  /<span class="keyword">*</span><span class="keyword">*</span></span><br><span class="line">   <span class="keyword">*</span>  Create a list of message streams of type T for each topic, using the default decoder.</span><br><span class="line">   <span class="keyword">*</span>/</span><br><span class="line">  public Map<span class="variable">&lt;String, List&lt;KafkaStream&lt;byte[], byte[]&gt;</span>&gt;&gt; createMessageStreams(Map<span class="variable">&lt;String, Integer&gt;</span> topicCountMap);</span><br><span class="line"></span><br><span class="line">  /<span class="keyword">*</span><span class="keyword">*</span></span><br><span class="line">   <span class="keyword">*</span>  Create a list of message streams for topics matching a wildcard.</span><br><span class="line">   <span class="keyword">*</span></span><br><span class="line">   <span class="keyword">*</span>  <span class="comment">@param topicFilter a TopicFilter that specifies which topics to</span></span><br><span class="line">   <span class="keyword">*</span>                    subscribe to (encapsulates a whitelist or a blacklist).</span><br><span class="line">   <span class="keyword">*</span>  <span class="comment">@param numStreams the number of message streams to return.</span></span><br><span class="line">   <span class="keyword">*</span>  <span class="comment">@param keyDecoder a decoder that decodes the message key</span></span><br><span class="line">   <span class="keyword">*</span>  <span class="comment">@param valueDecoder a decoder that decodes the message itself</span></span><br><span class="line">   <span class="keyword">*</span>  <span class="comment">@return a list of KafkaStream. Each stream supports an</span></span><br><span class="line">   <span class="keyword">*</span>          iterator over its MessageAndMetadata elements.</span><br><span class="line">   <span class="keyword">*</span>/</span><br><span class="line">  public <span class="variable">&lt;K,V&gt;</span> List<span class="variable">&lt;KafkaStream&lt;K,V&gt;</span>&gt;</span><br><span class="line">    createMessageStreamsByFilter(TopicFilter topicFilter, int numStreams, Decoder<span class="variable">&lt;K&gt;</span> keyDecoder, Decoder<span class="variable">&lt;V&gt;</span> valueDecoder);</span><br><span class="line"></span><br><span class="line">  /<span class="keyword">*</span><span class="keyword">*</span></span><br><span class="line">   <span class="keyword">*</span>  Create a list of message streams for topics matching a wildcard, using the default decoder.</span><br><span class="line">   <span class="keyword">*</span>/</span><br><span class="line">  public List<span class="variable">&lt;KafkaStream&lt;byte[], byte[]&gt;</span>&gt; createMessageStreamsByFilter(TopicFilter topicFilter, int numStreams);</span><br><span class="line"></span><br><span class="line">  /<span class="keyword">*</span><span class="keyword">*</span></span><br><span class="line">   <span class="keyword">*</span>  Create a list of message streams for topics matching a wildcard, using the default decoder, with one stream.</span><br><span class="line">   <span class="keyword">*</span>/</span><br><span class="line">  public List<span class="variable">&lt;KafkaStream&lt;byte[], byte[]&gt;</span>&gt; createMessageStreamsByFilter(TopicFilter topicFilter);</span><br><span class="line"></span><br><span class="line">  /<span class="keyword">*</span><span class="keyword">*</span></span><br><span class="line">   <span class="keyword">*</span>  Commit the offsets of all topic/partitions connected by this connector.</span><br><span class="line">   <span class="keyword">*</span>/</span><br><span class="line">  public void commitOffsets();</span><br><span class="line"></span><br><span class="line">  /<span class="keyword">*</span><span class="keyword">*</span></span><br><span class="line">   <span class="keyword">*</span>  Shut down the connector</span><br><span class="line">   <span class="keyword">*</span>/</span><br><span class="line">  public void shutdown();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>You can follow this <a href="https://cwiki.apache.org/confluence/display/KAFKA/Consumer+Group+Example" target="_blank" rel="external">example</a> to learn how to use the high level consumer api.</p>
<h4 id="2-2-2_Old_Simple_Consumer_API"><a href="#2-2-2_Old_Simple_Consumer_API" class="headerlink" title="2.2.2 Old Simple Consumer API"></a>2.2.2 Old Simple Consumer API</h4><figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">class kafka.javaapi.consumer.SimpleConsumer &#123;</span><br><span class="line">  /<span class="keyword">*</span><span class="keyword">*</span></span><br><span class="line">   <span class="keyword">*</span>  Fetch a set of messages from a topic.</span><br><span class="line">   <span class="keyword">*</span></span><br><span class="line">   <span class="keyword">*</span>  <span class="comment">@param request specifies the topic name, topic partition, starting byte offset, maximum bytes to be fetched.</span></span><br><span class="line">   <span class="keyword">*</span>  <span class="comment">@return a set of fetched messages</span></span><br><span class="line">   <span class="keyword">*</span>/</span><br><span class="line">  public FetchResponse fetch(kafka.javaapi.FetchRequest request);</span><br><span class="line"></span><br><span class="line">  /<span class="keyword">*</span><span class="keyword">*</span></span><br><span class="line">   <span class="keyword">*</span>  Fetch metadata for a sequence of topics.</span><br><span class="line">   <span class="keyword">*</span></span><br><span class="line">   <span class="keyword">*</span>  <span class="comment">@param request specifies the versionId, clientId, sequence of topics.</span></span><br><span class="line">   <span class="keyword">*</span>  <span class="comment">@return metadata for each topic in the request.</span></span><br><span class="line">   <span class="keyword">*</span>/</span><br><span class="line">  public kafka.javaapi.TopicMetadataResponse send(kafka.javaapi.TopicMetadataRequest request);</span><br><span class="line"></span><br><span class="line">  /<span class="keyword">*</span><span class="keyword">*</span></span><br><span class="line">   <span class="keyword">*</span>  Get a list of valid offsets (up to maxSize) before the given time.</span><br><span class="line">   <span class="keyword">*</span></span><br><span class="line">   <span class="keyword">*</span>  <span class="comment">@param request a [[kafka.javaapi.OffsetRequest]] object.</span></span><br><span class="line">   <span class="keyword">*</span>  <span class="comment">@return a [[kafka.javaapi.OffsetResponse]] object.</span></span><br><span class="line">   <span class="keyword">*</span>/</span><br><span class="line">  public kafka.javaapi.OffsetResponse getOffsetsBefore(OffsetRequest request);</span><br><span class="line"></span><br><span class="line">  /<span class="keyword">*</span><span class="keyword">*</span></span><br><span class="line">   <span class="keyword">*</span> Close the SimpleConsumer.</span><br><span class="line">   <span class="keyword">*</span>/</span><br><span class="line">  public void close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>For most applications, the high level consumer Api is good enough. Some applications want features not exposed to the high level consumer yet (e.g., set initial offset when restarting the consumer). They can instead use our low level SimpleConsumer Api. The logic will be a bit more complicated and you can follow the example in <a href="https://cwiki.apache.org/confluence/display/KAFKA/0.8.0+SimpleConsumer+Example" target="_blank" rel="external">here</a>.</p>
<h4 id="2-2-3_New_Consumer_API"><a href="#2-2-3_New_Consumer_API" class="headerlink" title="2.2.3 New Consumer API"></a>2.2.3 New Consumer API</h4><p>This new unified consumer API removes the distinction between the 0.8 high-level and low-level consumer APIs. You can use this client by adding a dependency on the client jar using the following example maven co-ordinates (you can change the version numbers with new releases):</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"> <span class="tag">&lt;<span class="title">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">groupId</span>&gt;</span>org.apache.kafka<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>kafka-clients<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">version</span>&gt;</span>0.9.0.0<span class="tag">&lt;/<span class="title">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>Examples showing how to use the consumer are given in the <a href="http://kafka.apache.org/090/javadoc/index.html?org/apache/kafka/clients/consumer/KafkaConsumer.html" target="_blank" rel="external">javadocs</a>.</p>
<h2 id="3-_Configuration"><a href="#3-_Configuration" class="headerlink" title="3. Configuration"></a>3. Configuration</h2><p>Kafka uses key-value pairs in the <a href="http://en.wikipedia.org/wiki/.properties" target="_blank" rel="external">property file format</a> for configuration. These values can be supplied either from a file or programmatically.</p>
<h3 id="3-1_Broker_Configs"><a href="#3-1_Broker_Configs" class="headerlink" title="3.1 Broker Configs"></a>3.1 Broker Configs</h3><p>The essential configurations are the following:</p>
<ul>
<li>broker.id</li>
<li>log.dirs</li>
<li>zookeeper.connect</li>
</ul>
<p>Topic-level configurations and defaults are discussed in more detail</p>
<p>Configurations pertinent to topics have both a global default as well an optional per-topic override. If no per-topic configuration is given the global default is used. The override can be set at topic creation time by giving one or more –config options. This example creates a topic named my-topic with a custom max message size and flush rate:</p>
<figure class="highlight brainfuck"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="comment">bin/kafka</span><span class="literal">-</span><span class="comment">topics</span><span class="string">.</span><span class="comment">sh</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">zookeeper</span> <span class="comment">localhost:2181</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">create</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">topic</span> <span class="comment">my</span><span class="literal">-</span><span class="comment">topic</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">partitions</span> <span class="comment">1</span></span><br><span class="line">        <span class="literal">-</span><span class="literal">-</span><span class="comment">replication</span><span class="literal">-</span><span class="comment">factor</span> <span class="comment">1</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">config</span> <span class="comment">max</span><span class="string">.</span><span class="comment">message</span><span class="string">.</span><span class="comment">bytes=64000</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">config</span> <span class="comment">flush</span><span class="string">.</span><span class="comment">messages=1</span></span><br></pre></td></tr></table></figure>
<p>Overrides can also be changed or set later using the alter topic command. This example updates the max message size for my-topic:</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/kafka-topics<span class="class">.sh</span> --zookeeper localhost:<span class="number">2181</span> --alter --topic my-topic</span><br><span class="line">    --config max<span class="class">.message</span><span class="class">.bytes</span>=<span class="number">128000</span></span><br></pre></td></tr></table></figure>
<p>To remove an override you can do</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/kafka-topics<span class="class">.sh</span> --zookeeper localhost:<span class="number">2181</span> --alter --topic my-topic</span><br><span class="line">    --deleteConfig max<span class="class">.message</span><span class="class">.bytes</span></span><br></pre></td></tr></table></figure>
<p>The following are the topic-level configurations. The server’s default configuration for this property is given under the Server Default Property heading, setting this default in the server config allows you to change the default given to topics that have no override specified.</p>
<h2 id="4-_Design"><a href="#4-_Design" class="headerlink" title="4. Design"></a>4. Design</h2><h3 id="4-1_Motivation"><a href="#4-1_Motivation" class="headerlink" title="4.1 Motivation"></a>4.1 Motivation</h3><p>We designed Kafka to be able to act as a unified platform for handling all the real-time data feeds a large company might have. To do this we had to think through a fairly broad set of use cases.</p>
<p>It would have to have high-throughput to support high volume event streams such as real-time log aggregation.</p>
<p>It would need to deal gracefully with large data backlogs to be able to support periodic data loads from offline systems.</p>
<p>It also meant the system would have to handle low-latency delivery to handle more traditional messaging use-cases.</p>
<p>We wanted to support partitioned, distributed, real-time processing of these feeds to create new, derived feeds. This motivated our partitioning and consumer model.</p>
<p>Finally in cases where the stream is fed into other data systems for serving we knew the system would have to be able to guarantee fault-tolerance in the presence of machine failures.</p>
<p>Supporting these uses led use to a design with a number of unique elements, more akin to a database log then a traditional messaging system. We will outline some elements of the design in the following sections.</p>
<h3 id="4-2_Persistence"><a href="#4-2_Persistence" class="headerlink" title="4.2 Persistence"></a>4.2 Persistence</h3><p><em>Don’t fear the filesystem!</em></p>
<p>Kafka relies heavily on the filesystem for storing and caching messages. There is a general perception that “disks are slow” which makes people skeptical that a persistent structure can offer competitive performance. In fact disks are both much slower and much faster than people expect depending on how they are used; and a properly designed disk structure can often be as fast as the network.</p>
<p>The key fact about disk performance is that the throughput of hard drives has been diverging from the latency of a disk seek for the last decade. As a result the performance of linear writes on a <a href="http://en.wikipedia.org/wiki/Non-RAID_drive_architectures" target="_blank" rel="external">JBOD</a> configuration with six 7200rpm SATA RAID-5 array is about 600MB/sec but the performance of random writes is only about 100k/sec—a difference of over 6000X. These linear reads and writes are the most predictable of all usage patterns, and are heavily optimized by the operating system. A modern operating system provides read-ahead and write-behind techniques that prefetch data in large block multiples and group smaller logical writes into large physical writes. A further discussion of this issue can be found in this <a href="http://queue.acm.org/detail.cfm?id=1563874" target="_blank" rel="external">ACM Queue article</a>; they actually find that <a href="http://deliveryimages.acm.org/10.1145/1570000/1563874/jacobs3.jpg" target="_blank" rel="external">sequential disk access can in some cases be faster than random memory access</a>!</p>
<p>To compensate for this performance divergence modern operating systems have become increasingly aggressive in their use of main memory for disk caching. A modern OS will happily divert all free memory to disk caching with little performance penalty when the memory is reclaimed. All disk reads and writes will go through this unified cache. This feature cannot easily be turned off without using direct I/O, so even if a process maintains an in-process cache of the data, this data will likely be duplicated in OS pagecache, effectively storing everything twice.</p>
<p>Furthermore we are building on top of the JVM, and anyone who has spent any time with Java memory usage knows two things:</p>
<ol>
<li>The memory overhead of objects is very high, often doubling the size of the data stored (or worse).</li>
<li>Java garbage collection becomes increasingly fiddly and slow as the in-heap data increases.</li>
</ol>
<p>As a result of these factors using the filesystem and relying on pagecache is superior to maintaining an in-memory cache or other structure—we at least double the available cache by having automatic access to all free memory, and likely double again by storing a compact byte structure rather than individual objects. Doing so will result in a cache of up to 28-30GB on a 32GB machine without GC penalties. Furthermore this cache will stay warm even if the service is restarted, whereas the in-process cache will need to be rebuilt in memory (which for a 10GB cache may take 10 minutes) or else it will need to start with a completely cold cache (which likely means terrible initial performance). This also greatly simplifies the code as all logic for maintaining coherency between the cache and filesystem is now in the OS, which tends to do so more efficiently and more correctly than one-off in-process attempts. If your disk usage favors linear reads then read-ahead is effectively pre-populating this cache with useful data on each disk read.</p>
<p>This suggests a design which is very simple: rather than maintain as much as possible in-memory and flush it all out to the filesystem in a panic when we run out of space, we invert that. All data is immediately written to a persistent log on the filesystem without necessarily flushing to disk. In effect this just means that it is transferred into the kernel’s pagecache.</p>
<p>This style of pagecache-centric design is described in an article on the design of Varnish here (along with a healthy dose of arrogance).</p>
<p>Intuitively a persistent queue could be built on simple reads and appends to files as is commonly the case with logging solutions. This structure has the advantage that all operations are O(1) and reads do not block writes or each other. This has obvious performance advantages since the performance is completely decoupled from the data size—one server can now take full advantage of a number of cheap, low-rotational speed 1+TB SATA drives. Though they have poor seek performance, these drives have acceptable performance for large reads and writes and come at 1/3 the price and 3x the capacity.</p>
<p>Having access to virtually unlimited disk space without any performance penalty means that we can provide some features not usually found in a messaging system. For example, in Kafka, instead of attempting to deleting messages as soon as they are consumed, we can retain messages for a relative long period (say a week). This leads to a great deal of flexibility for consumers, as we will describe.</p>
<h2 id="u53C2_u8003_u6587_u732E"><a href="#u53C2_u8003_u6587_u732E" class="headerlink" title="参考文献"></a>参考文献</h2><ul>
<li><a href="https://github.com/docs2cn/apache-kafka-docs/blob/master/index.md" target="_blank" rel="external">https://github.com/docs2cn/apache-kafka-docs/blob/master/index.md</a></li>
<li><a href="http://kafka.apache.org/documentation.html" target="_blank" rel="external">http://kafka.apache.org/documentation.html</a></li>
<li><a href="http://www.cnblogs.com/lzqhss/p/4434901.html" target="_blank" rel="external">http://www.cnblogs.com/lzqhss/p/4434901.html</a></li>
<li><a href="http://gfstart.blog.51cto.com/5081306/1414597" target="_blank" rel="external">http://gfstart.blog.51cto.com/5081306/1414597</a></li>
<li><a href="http://yclod.com/kafka-jie-shao/" target="_blank" rel="external">http://yclod.com/kafka-jie-shao/</a></li>
<li><a href="http://www.oschina.net/translate/kafka-design" target="_blank" rel="external">http://www.oschina.net/translate/kafka-design</a></li>
<li><a href="http://blog.csdn.net/beitiandijun/article/details/40582541" target="_blank" rel="external">http://blog.csdn.net/beitiandijun/article/details/40582541</a></li>
<li><a href="http://www.infoq.com/cn/articles/apache-kafka" target="_blank" rel="external">http://www.infoq.com/cn/articles/apache-kafka</a></li>
<li><a href="http://www.doczj.com/doc/20781790b52acfc788ebc955.html" target="_blank" rel="external">http://www.doczj.com/doc/20781790b52acfc788ebc955.html</a></li>
<li><a href="https://baniuyao.gitbooks.io/kafka-0-8-docs-chinese/content/1.1-introduction.html" target="_blank" rel="external">https://baniuyao.gitbooks.io/kafka-0-8-docs-chinese/content/1.1-introduction.html</a></li>
<li><a href="http://www.infoq.com/cn/articles/kafka-analysis-part-1" target="_blank" rel="external">http://www.infoq.com/cn/articles/kafka-analysis-part-1</a></li>
</ul>
]]></content>
    <summary type="html">
    <![CDATA[<!--  -->
<!--  -->
<!--  _ __    __     __      ____    ___     ___   _____   __  __    ___     -->
<!-- /\`'__\/'__`\ /'__`\   /',__\  / _]]>
    </summary>
    
      <category term="Documentation" scheme="http://reasonpun.com/tags/Documentation/"/>
    
      <category term="HDFS" scheme="http://reasonpun.com/tags/HDFS/"/>
    
      <category term="Kafka" scheme="http://reasonpun.com/tags/Kafka/"/>
    
      <category term="数据挖掘" scheme="http://reasonpun.com/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[linux shell 获得以前日期]]></title>
    <link href="http://reasonpun.com/2015/12/05/linux-shell-%E8%8E%B7%E5%BE%97%E4%BB%A5%E5%89%8D%E6%97%A5%E6%9C%9F/"/>
    <id>http://reasonpun.com/2015/12/05/linux-shell-获得以前日期/</id>
    <published>2015-12-05T07:26:55.000Z</published>
    <updated>2016-01-18T03:06:13.000Z</updated>
    <content type="html"><![CDATA[<!--  -->
<!--  -->
<!--  _ __    __     __      ____    ___     ___   _____   __  __    ___     -->
<!-- /\`'__\/'__`\ /'__`\   /',__\  / __`\ /' _ `\/\ '__`\/\ \/\ \ /' _ `\   -->
<!-- \ \ \//\  __//\ \L\.\_/\__, `\/\ \L\ \/\ \/\ \ \ \L\ \ \ \_\ \/\ \/\ \  -->
<!--  \ \_\\ \____\ \__/.\_\/\____/\ \____/\ \_\ \_\ \ ,__/\ \____/\ \_\ \_\ -->
<!--   \/_/ \/____/\/__/\/_/\/___/  \/___/  \/_/\/_/\ \ \/  \/___/  \/_/\/_/ -->
<!--                                                 \ \_\                   -->
<!--                                                  \/_/                   -->
<!--  -->
<p>在linux shell里，我想获得以前的日期，<br>1、比如，去年的上个月的昨天的日期：（今天是2009年2月2日，也就是2008年1月1日）</p>
<figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">reasonpun<span class="variable">@reasonpun</span><span class="symbol">:~</span><span class="variable">$ </span>logRecordDate=<span class="string">"`date -d "</span>-<span class="number">1</span> year -<span class="number">1</span> month -<span class="number">1</span> day<span class="string">" "</span>+%<span class="constant">Y_%</span>m<span class="constant">_</span>%d<span class="string">"`"</span></span><br><span class="line">reasonpun<span class="variable">@reasonpun</span><span class="symbol">:~</span><span class="variable">$ </span>echo <span class="variable">$logRecordDate</span></span><br><span class="line"><span class="number">2008_01_01</span></span><br><span class="line">reasonpun<span class="variable">@reasonpun</span><span class="symbol">:~</span>$</span><br></pre></td></tr></table></figure>
<p>2、上个月的今天：</p>
<figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">reasonpun@reasonpun:~$ logRecordDate=<span class="string">"`date -d "</span>-<span class="number">1</span> month<span class="string">" "</span>+<span class="decorator">%Y_</span><span class="decorator">%m_</span><span class="decorator">%d</span><span class="string">"`"</span></span><br><span class="line">reasonpun@reasonpun:~$ echo <span class="variable">$logRecordDate</span></span><br><span class="line"><span class="number">2009_01_02</span></span><br></pre></td></tr></table></figure>
<p>3、去年的今天：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">reasonpun@reasonpun:~$ logRecordDate=<span class="string">"`date -d "</span>-<span class="number">1</span> yea<span class="string">r" "</span>+%Y_%m_%d<span class="string">"`"</span></span><br><span class="line">reasonpun@reasonpun:~$ echo $logRecordDate</span><br><span class="line"><span class="number">2008</span>_02_02</span><br></pre></td></tr></table></figure>
<p>4、上个月的昨天：</p>
<figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">reasonpun@reasonpun:~$ logRecordDate=<span class="string">"`date -d "</span>-<span class="number">1</span> month -<span class="number">1</span> day<span class="string">" "</span>+<span class="decorator">%Y_</span><span class="decorator">%m_</span><span class="decorator">%d</span><span class="string">"`"</span></span><br><span class="line">reasonpun@reasonpun:~$ echo <span class="variable">$logRecordDate</span></span><br><span class="line"><span class="number">2009_01_01</span></span><br></pre></td></tr></table></figure>
<p>5、根据时间戳转换成日期</p>
<figure class="highlight perl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[mofun_mining<span class="variable">@i</span>-qe32ajmq ~]$ date -d <span class="variable">@1434847028</span> <span class="string">"+<span class="variable">%Y</span>-<span class="variable">%m</span>-<span class="variable">%d</span>"</span></span><br><span class="line"><span class="number">2015</span>-<span class="number">06</span>-<span class="number">21</span></span><br></pre></td></tr></table></figure>
<p>其他的类推～～呵呵，还是希望大家给测测其他日期会不会出错呵呵。多谢～～～<br>鼓捣之环境：ubuntu8.04</p>
<p>另附上windows下获得前一天的日期：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line">@echo off</span><br><span class="line"><span class="built_in">set</span> td=%date:~<span class="number">2</span>,<span class="number">2</span>%%date:~<span class="number">5</span>,<span class="number">2</span>%%date:~<span class="number">8</span>,<span class="number">2</span>%</span><br><span class="line"><span class="built_in">set</span> dy=%date:~<span class="number">0</span>,<span class="number">4</span>%</span><br><span class="line"><span class="built_in">set</span> dm=%date:~<span class="number">5</span>,<span class="number">2</span>%</span><br><span class="line"><span class="built_in">set</span> dd=%date:~<span class="number">8</span>,<span class="number">2</span>%</span><br><span class="line"><span class="built_in">set</span> da=%date:~<span class="number">8</span>,<span class="number">2</span>%</span><br><span class="line"><span class="keyword">if</span> %dm%%dd%==<span class="number">0101</span> <span class="keyword">goto</span> L01</span><br><span class="line"><span class="keyword">if</span> %dm%%dd%==<span class="number">0201</span> <span class="keyword">goto</span> L02</span><br><span class="line"><span class="keyword">if</span> %dm%%dd%==<span class="number">0301</span> <span class="keyword">goto</span> L07</span><br><span class="line"><span class="keyword">if</span> %dm%%dd%==<span class="number">0401</span> <span class="keyword">goto</span> L02</span><br><span class="line"><span class="keyword">if</span> %dm%%dd%==<span class="number">0501</span> <span class="keyword">goto</span> L04</span><br><span class="line"><span class="keyword">if</span> %dm%%dd%==<span class="number">0601</span> <span class="keyword">goto</span> L02</span><br><span class="line"><span class="keyword">if</span> %dm%%dd%==<span class="number">0701</span> <span class="keyword">goto</span> L04</span><br><span class="line"><span class="keyword">if</span> %dm%%dd%==<span class="number">0801</span> <span class="keyword">goto</span> L02</span><br><span class="line"><span class="keyword">if</span> %dm%%dd%==<span class="number">0901</span> <span class="keyword">goto</span> L02</span><br><span class="line"><span class="keyword">if</span> %dm%%dd%==<span class="number">1001</span> <span class="keyword">goto</span> L05</span><br><span class="line"><span class="keyword">if</span> %dm%%dd%==<span class="number">1101</span> <span class="keyword">goto</span> L03</span><br><span class="line"><span class="keyword">if</span> %dm%%dd%==<span class="number">1201</span> <span class="keyword">goto</span> L06</span><br><span class="line"><span class="keyword">if</span> %dd%==<span class="number">02</span> <span class="keyword">goto</span> L10</span><br><span class="line"><span class="keyword">if</span> %dd%==<span class="number">03</span> <span class="keyword">goto</span> L10</span><br><span class="line"><span class="keyword">if</span> %dd%==<span class="number">04</span> <span class="keyword">goto</span> L10</span><br><span class="line"><span class="keyword">if</span> %dd%==<span class="number">05</span> <span class="keyword">goto</span> L10</span><br><span class="line"><span class="keyword">if</span> %dd%==<span class="number">06</span> <span class="keyword">goto</span> L10</span><br><span class="line"><span class="keyword">if</span> %dd%==<span class="number">07</span> <span class="keyword">goto</span> L10</span><br><span class="line"><span class="keyword">if</span> %dd%==<span class="number">08</span> <span class="keyword">goto</span> L10</span><br><span class="line"><span class="keyword">if</span> %dd%==<span class="number">09</span> <span class="keyword">goto</span> L10</span><br><span class="line"><span class="keyword">if</span> %dd%==<span class="number">10</span> <span class="keyword">goto</span> L11</span><br><span class="line"><span class="built_in">set</span> /A dd=dd-<span class="number">1</span></span><br><span class="line"><span class="built_in">set</span> dt=%dy%-%dm%-%dd%</span><br><span class="line"><span class="keyword">goto</span> END</span><br><span class="line">:L10</span><br><span class="line"><span class="built_in">set</span> /A dd=%dd:~<span class="number">1</span>,<span class="number">1</span>%-<span class="number">1</span></span><br><span class="line"><span class="built_in">set</span> dt=%dy%-%dm%-<span class="number">0</span>%dd%</span><br><span class="line"><span class="keyword">goto</span> END</span><br><span class="line">:L11</span><br><span class="line"><span class="built_in">set</span> dt=%dy%-%dm%-<span class="number">09</span></span><br><span class="line"><span class="keyword">goto</span> END</span><br><span class="line">:L02</span><br><span class="line"><span class="built_in">set</span> /A dm=%dm:~<span class="number">1</span>,<span class="number">1</span>%-<span class="number">1</span></span><br><span class="line"><span class="built_in">set</span> dt=%dy%-<span class="number">0</span>%dm%-<span class="number">31</span></span><br><span class="line"><span class="keyword">goto</span> END</span><br><span class="line">:L04</span><br><span class="line"><span class="built_in">set</span> /A dm=dm-<span class="number">1</span></span><br><span class="line"><span class="built_in">set</span> dt=%dy%-<span class="number">0</span>%dm%-<span class="number">30</span></span><br><span class="line"><span class="keyword">goto</span> END</span><br><span class="line">:L05</span><br><span class="line"><span class="built_in">set</span> dt=%dy%-<span class="number">09</span>-<span class="number">30</span></span><br><span class="line"><span class="keyword">goto</span> END</span><br><span class="line">:L03</span><br><span class="line"><span class="built_in">set</span> dt=%dy%-<span class="number">10</span>-<span class="number">31</span></span><br><span class="line"><span class="keyword">goto</span> END</span><br><span class="line">:L06</span><br><span class="line"><span class="built_in">set</span> dt=%dy%-<span class="number">11</span>-<span class="number">30</span></span><br><span class="line"><span class="keyword">goto</span> END</span><br><span class="line">:L01</span><br><span class="line"><span class="built_in">set</span> /A dy=dy-<span class="number">1</span></span><br><span class="line"><span class="built_in">set</span> dt=%dy%-<span class="number">12</span>-<span class="number">31</span></span><br><span class="line"><span class="keyword">goto</span> END</span><br><span class="line">:L07</span><br><span class="line"><span class="built_in">set</span> /A <span class="string">"dd=dy%%4"</span></span><br><span class="line"><span class="keyword">if</span> not %dd%==<span class="number">0</span> <span class="keyword">goto</span> L08</span><br><span class="line"><span class="built_in">set</span> /A <span class="string">"dd=dy%%100"</span></span><br><span class="line"><span class="keyword">if</span> not %dd%==<span class="number">0</span> <span class="keyword">goto</span> L09</span><br><span class="line"><span class="built_in">set</span> /A <span class="string">"dd=dy%%400"</span></span><br><span class="line"><span class="keyword">if</span> %dd%==<span class="number">0</span> <span class="keyword">goto</span> L09</span><br><span class="line">:L08</span><br><span class="line"><span class="built_in">set</span> dt=%dy%-<span class="number">02</span>-<span class="number">28</span></span><br><span class="line"><span class="keyword">goto</span> END</span><br><span class="line">:L09</span><br><span class="line"><span class="built_in">set</span> dt=%dy%-<span class="number">02</span>-<span class="number">29</span></span><br><span class="line"><span class="keyword">goto</span> END</span><br><span class="line">:END</span><br><span class="line"><span class="built_in">set</span> dateTime=<span class="number">20</span>%dt:~<span class="number">2</span>,<span class="number">2</span>%%dt:~<span class="number">5</span>,<span class="number">2</span>%%dt:~<span class="number">8</span>,<span class="number">2</span>%</span><br></pre></td></tr></table></figure>
]]></content>
    <summary type="html">
    <![CDATA[<!--  -->
<!--  -->
<!--  _ __    __     __      ____    ___     ___   _____   __  __    ___     -->
<!-- /\`'__\/'__`\ /'__`\   /',__\  / _]]>
    </summary>
    
      <category term="Linux" scheme="http://reasonpun.com/tags/Linux/"/>
    
      <category term="Shell" scheme="http://reasonpun.com/tags/Shell/"/>
    
      <category term="系统维护" scheme="http://reasonpun.com/categories/%E7%B3%BB%E7%BB%9F%E7%BB%B4%E6%8A%A4/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Docker中搭建Spark计算框架]]></title>
    <link href="http://reasonpun.com/2015/12/04/spark-in-docker/"/>
    <id>http://reasonpun.com/2015/12/04/spark-in-docker/</id>
    <published>2015-12-04T09:48:17.000Z</published>
    <updated>2016-01-25T03:17:11.000Z</updated>
    <content type="html"><![CDATA[<!--  -->
<!--  -->
<!--  _ __    __     __      ____    ___     ___   _____   __  __    ___     -->
<!-- /\`'__\/'__`\ /'__`\   /',__\  / __`\ /' _ `\/\ '__`\/\ \/\ \ /' _ `\   -->
<!-- \ \ \//\  __//\ \L\.\_/\__, `\/\ \L\ \/\ \/\ \ \ \L\ \ \ \_\ \/\ \/\ \  -->
<!--  \ \_\\ \____\ \__/.\_\/\____/\ \____/\ \_\ \_\ \ ,__/\ \____/\ \_\ \_\ -->
<!--   \/_/ \/____/\/__/\/_/\/___/  \/___/  \/_/\/_/\ \ \/  \/___/  \/_/\/_/ -->
<!--                                                 \ \_\                   -->
<!--                                                  \/_/                   -->
<!--  -->
<h4 id="u5B89_u88C5_u597DDocker_u4E4B_u540E"><a href="#u5B89_u88C5_u597DDocker_u4E4B_u540E" class="headerlink" title="安装好Docker之后"></a>安装好Docker之后</h4><h4 id="u5148_u62C9_u53D6_u4E00_u4E2A_u5B98_u65B9_u7684_u57FA_u672C_u955C_u50CFubuntu"><a href="#u5148_u62C9_u53D6_u4E00_u4E2A_u5B98_u65B9_u7684_u57FA_u672C_u955C_u50CFubuntu" class="headerlink" title="先拉取一个官方的基本镜像ubuntu"></a>先拉取一个官方的基本镜像ubuntu</h4><p>docker pull ubuntu</p>
<p>我们将在这个基础镜像上运行容器，将这个容器当成一个普通的ubuntu虚拟机来操作部署spark，最后将配置好的容器commit为一个镜像，之后就可以通过这个镜像运行n个节点来完成集群的搭建</p>
<h4 id="u4E0B_u8F7D_u5B8Cubuntu_u955C_u50CF_u4E4B_u540E_u8FD0_u884C"><a href="#u4E0B_u8F7D_u5B8Cubuntu_u955C_u50CF_u4E4B_u540E_u8FD0_u884C" class="headerlink" title="下载完ubuntu镜像之后运行"></a>下载完ubuntu镜像之后运行</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[reason@localhost ~]$ docker images</span><br><span class="line">REPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZE</span><br><span class="line">mofun/spark         v2<span class="number">.0</span>                b2dacac3e132        About an hour ago   <span class="number">1.508</span> GB</span><br><span class="line">mofun/spark         v1<span class="number">.3</span>                <span class="number">4</span>d2f33ca61ee        <span class="number">18</span> hours ago        <span class="number">1.506</span> GB</span><br><span class="line">ubuntu              latest              <span class="number">0</span>a17decee413        <span class="number">6</span> days ago          <span class="number">188.3</span> MB</span><br><span class="line">docker/whalesay     latest              ded5e192a685        <span class="number">4</span> months ago        <span class="number">247</span> MB</span><br><span class="line">[reason@localhost ~]$</span><br></pre></td></tr></table></figure>
<h4 id="u8FD0_u884Cubuntu_u5BB9_u5668"><a href="#u8FD0_u884Cubuntu_u5BB9_u5668" class="headerlink" title="运行ubuntu容器"></a>运行ubuntu容器</h4><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[reason<span class="variable">@localhost</span> ~]<span class="variable">$ </span>docker run -v /home/docker/software/<span class="symbol">:/software</span> -it ubuntu</span><br><span class="line">root<span class="variable">@f4c0a9d42852</span><span class="symbol">:/</span><span class="comment">#</span></span><br></pre></td></tr></table></figure>
<h4 id="u5728_u5BB9_u5668_u4E2D_u5B89_u88C5ssh"><a href="#u5728_u5BB9_u5668_u4E2D_u5B89_u88C5ssh" class="headerlink" title="在容器中安装ssh"></a>在容器中安装ssh</h4><figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">[reason<span class="comment">@localhost ~]$ docker run -v /home/docker/software/:/software -it ubuntu</span></span><br><span class="line">root<span class="comment">@3970c1e5466e:/# apt-get install ssh</span></span><br><span class="line">Reading package lists... Done</span><br><span class="line">Building dependency tree</span><br><span class="line">Reading state information... Done</span><br><span class="line"></span><br><span class="line"><span class="comment"># ssh默认配置root无法登陆</span></span><br><span class="line">root<span class="comment">@3970c1e5466e:~/.ssh# vim /etc/ssh/sshd_config</span></span><br><span class="line">root<span class="comment">@3970c1e5466e:~/.ssh#</span></span><br><span class="line"><span class="comment"># 将 /etc/ssh/sshd_config中PermitRootLogin no/without_passwd 改为yes</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成访问密钥</span></span><br><span class="line">root<span class="comment">@3970c1e5466e:~# ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa</span></span><br><span class="line">Generating public/private rsa key pair.</span><br><span class="line">Created directory '/root/.ssh'.</span><br><span class="line">Your identification has been saved in /root/.ssh/id_rsa.</span><br><span class="line">Your public key has been saved in /root/.ssh/id_rsa.pub.</span><br><span class="line">The key fingerprint is:</span><br><span class="line">b5:d1:e1:dd:98:7d:be:cc:55:69:c6:e7:67:80:d8:d3 root<span class="comment">@3970c1e5466e</span></span><br><span class="line">The key's randomart image is:</span><br><span class="line">+--[ RSA 2048]----+</span><br><span class="line">|<span class="string">            .    </span>|</span><br><span class="line">|<span class="string">           = =.=.</span>|</span><br><span class="line">|<span class="string">          + * E=*</span>|</span><br><span class="line">|<span class="string">         . o .o++</span>|</span><br><span class="line">|<span class="string">        S .     *</span>|</span><br><span class="line">|<span class="string">              o.+</span>|</span><br><span class="line">|<span class="string">               + </span>|</span><br><span class="line">|<span class="string">                 </span>|</span><br><span class="line">|<span class="string">                 </span>|</span><br><span class="line">+-----------------+</span><br><span class="line">root<span class="comment">@3970c1e5466e:~#</span></span><br><span class="line">root<span class="comment">@3970c1e5466e:~# cd ~/.ssh/</span></span><br><span class="line">root<span class="comment">@3970c1e5466e:~/.ssh# cat id_rsa.pub &gt;&gt; authorized_keys</span></span><br><span class="line">root<span class="comment">@3970c1e5466e:~/.ssh#</span></span><br><span class="line">root<span class="comment">@3970c1e5466e:~/.ssh# vim ~/.bashrc</span></span><br><span class="line"><span class="comment">#加入/usr/sbin/sshd</span></span><br><span class="line"><span class="comment">#如果在启动容器的时候还是无法启动ssh的话，</span></span><br><span class="line"><span class="comment"># 在/etc/rc.local文件中也加入</span></span><br><span class="line">root<span class="comment">@3970c1e5466e:~/.ssh# vim /etc/rc.local</span></span><br><span class="line"><span class="comment">#加入</span></span><br><span class="line">root<span class="comment">@3970c1e5466e:~/.ssh# /usr/sbin/sshd</span></span><br><span class="line">Missing privilege separation directory: /var/run/sshd</span><br><span class="line">root<span class="comment">@3970c1e5466e:~/.ssh# mkdir /var/run/sshd</span></span><br><span class="line">root<span class="comment">@3970c1e5466e:~/.ssh# /usr/sbin/sshd</span></span><br><span class="line">root<span class="comment">@3970c1e5466e:~/.ssh#</span></span><br><span class="line"><span class="comment"># 开启ssh服务后验证是否可以使用，打印出当前时间</span></span><br><span class="line">root<span class="comment">@3970c1e5466e:~/.ssh# ssh localhost date</span></span><br><span class="line">The authenticity of host 'localhost (::1)' can't be established.</span><br><span class="line">ECDSA key fingerprint is ab:43:27:e6:1c:44:be:2c:f1:17:27:90:6d:2c:68:86.</span><br><span class="line">Are you sure you want to continue connecting (yes/no)? yes</span><br><span class="line">Warning: Permanently added 'localhost' (ECDSA) to the list of known hosts.</span><br><span class="line">Mon Oct 19 05:57:44 UTC 2015</span><br><span class="line">root<span class="comment">@3970c1e5466e:~/.ssh#</span></span><br><span class="line"><span class="comment"># ssh安装完毕</span></span><br></pre></td></tr></table></figure>
<h4 id="u5B89_u88C5JDK"><a href="#u5B89_u88C5JDK" class="headerlink" title="安装JDK"></a>安装JDK</h4><p>可以使用apt-get方式直接下载安装jdk（不推荐，下载速度慢，有可能还会失败）<br>这里选择从网上下载完jdk-8u60-linux-xx.bin之后<br>将其传到Ubuntu宿主机中，在运行容器的时候使用-v参数将宿主机上的目录映射到容器中，这样在容器中就可以访问到宿主机中的文件了</p>
<figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">如果提示不能安装.bin文件，使用以下命令即可解决</span><br><span class="line">root<span class="variable">@3970c1e5466e</span><span class="symbol">:~/</span>.ssh<span class="comment"># apt-get update</span></span><br><span class="line">root<span class="variable">@3970c1e5466e</span><span class="symbol">:~/</span>.ssh<span class="comment"># apt-getinstall g++-multilib</span></span><br></pre></td></tr></table></figure>
<h4 id="u5B89_u88C5Zookeeper"><a href="#u5B89_u88C5Zookeeper" class="headerlink" title="安装Zookeeper"></a>安装Zookeeper</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">将下载好的zookeeper-<span class="number">3.4</span><span class="number">.6</span>.tar.gz上传</span><br><span class="line">root@<span class="number">3970</span>c1e5466e:~/.ssh<span class="preprocessor"># mv /software/zookeeper-<span class="number">3.4</span><span class="number">.6</span>.tar.gz /usr/local/zookeeper-<span class="number">3.4</span><span class="number">.6</span></span></span><br><span class="line">root@<span class="number">3970</span>c1e5466e:~/.ssh<span class="preprocessor"># tar -zxvf zookeeper-<span class="number">3.4</span><span class="number">.6</span>.tar.gz</span></span><br><span class="line">root@<span class="number">3970</span>c1e5466e:~/.ssh<span class="preprocessor"># cd /usr/local/zookeeper-<span class="number">3.4</span><span class="number">.6</span>/conf/</span></span><br><span class="line">root@<span class="number">3970</span>c1e5466e:~/.ssh<span class="preprocessor"># cp zoo_sample.cfgzoo.cfgvim zoo.cfg</span></span><br><span class="line">root@<span class="number">3970</span>c1e5466e:~/.ssh<span class="preprocessor"># vim zoo.cfg</span></span><br><span class="line"><span class="preprocessor">#修改：dataDir=/root/zookeeper/tmp</span></span><br><span class="line"><span class="preprocessor">#在最后添加：</span></span><br><span class="line">server<span class="number">.1</span>=cloud4:<span class="number">2888</span>:<span class="number">3888</span></span><br><span class="line">server<span class="number">.2</span>=cloud5:<span class="number">2888</span>:<span class="number">3888</span></span><br><span class="line">server<span class="number">.3</span>=cloud6:<span class="number">2888</span>:<span class="number">3888</span></span><br><span class="line"></span><br><span class="line"><span class="preprocessor">#保存退出，然后创建一个tmp文件夹</span></span><br><span class="line">root@<span class="number">3970</span>c1e5466e:~/.ssh<span class="preprocessor"># mkdir /data/zookeeper/tmp</span></span><br><span class="line"><span class="preprocessor">#再创建一个空文件</span></span><br><span class="line">root@<span class="number">3970</span>c1e5466e:~/.ssh<span class="preprocessor"># touch /data/zookeeper/tmp/myid</span></span><br><span class="line"><span class="preprocessor">#最后向该文件写入ID</span></span><br><span class="line">root@<span class="number">3970</span>c1e5466e:~/.ssh<span class="preprocessor"># echo <span class="number">1</span>&gt; /data/zookeeper/tmp/myid</span></span><br></pre></td></tr></table></figure>
<h4 id="u5B89_u88C5Hadoop"><a href="#u5B89_u88C5Hadoop" class="headerlink" title="安装Hadoop"></a>安装Hadoop</h4><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">root<span class="variable">@3970c1e5466e</span><span class="symbol">:~/</span>.ssh<span class="comment"># mv /software/hadoop-2.6.1.tar.gz /usr/local/</span></span><br><span class="line">root<span class="variable">@3970c1e5466e</span><span class="symbol">:~/</span>.ssh<span class="comment"># tar -zxvf hadoop-2.2.0-64bit.tar.gz</span></span><br><span class="line">root<span class="variable">@3970c1e5466e</span><span class="symbol">:~/</span>.ssh<span class="comment"># cd /usr/local/hadoop/etc/hadoop</span></span><br></pre></td></tr></table></figure>
<p>更改hadoop-env.sh</p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">root<span class="variable">@3970c1e5466e</span><span class="symbol">:/data/test</span><span class="comment"># cd /usr/local/hadoop-2.6.1/</span></span><br><span class="line">root<span class="variable">@3970c1e5466e</span><span class="symbol">:/usr/local/hadoop-</span><span class="number">2.6</span>.<span class="number">1</span><span class="comment"># ls</span></span><br><span class="line"><span class="constant">LICENSE</span>.txt  <span class="constant">NOTICE</span>.txt  <span class="constant">README</span>.txt  bin  etc  <span class="keyword">include</span>  lib  libexec  logs  sbin  share</span><br><span class="line">root<span class="variable">@3970c1e5466e</span><span class="symbol">:/usr/local/hadoop-</span><span class="number">2.6</span>.<span class="number">1</span><span class="comment"># cd etc/hadoop/</span></span><br><span class="line">root<span class="variable">@3970c1e5466e</span><span class="symbol">:/usr/local/hadoop-</span><span class="number">2.6</span>.<span class="number">1</span>/etc/hadoop<span class="comment"># vim hadoop-env.sh</span></span><br><span class="line"><span class="comment">#加入java环境变量</span></span><br><span class="line">export <span class="constant">JAVA_HOME</span>=<span class="regexp">/usr/local</span><span class="regexp">/jdk1.8.0_60</span></span><br></pre></td></tr></table></figure>
<p>修改core-site.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">root@3970c1e5466e:/usr/local/hadoop-2.6.1/etc/hadoop# vim core-site.xml</span><br><span class="line"></span><br><span class="line"><span class="pi">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span></span><br><span class="line"><span class="pi">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">value</span>&gt;</span>hdfs://ns1<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">value</span>&gt;</span>/data/hadoop/tmp<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">name</span>&gt;</span>ha.zookeeper.quorum<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">value</span>&gt;</span>cloud4:2181,cloud5:2182,cloud6:2183<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>修改hdfs-site.xml, mapred-site.xml, yarn-site.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line">root@3970c1e5466e:/usr/local/hadoop-2.6.1/etc/hadoop# vim hdfs-site.xml</span><br><span class="line"><span class="pi">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span></span><br><span class="line"><span class="pi">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="title">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.nameservices<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">value</span>&gt;</span>ns1<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.ha.namenodes.ns1<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">value</span>&gt;</span>nn1,nn2<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.namenode.rpc-address.ns1.nn1<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">value</span>&gt;</span>cloud1:9000<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.namenode.http-address.ns1.nn1<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">value</span>&gt;</span>cloud1:50070<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.namenode.rpc-address.ns1.nn2<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">value</span>&gt;</span>cloud2:9000<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.namenode.http-address.ns1.nn2<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">value</span>&gt;</span>cloud2:50070<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.namenode.shared.edits.dir<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">value</span>&gt;</span>qjournal://cloud4:8485;cloud5:8485;cloud6:8485/ns1<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.journalnode.edits.dir<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">value</span>&gt;</span>/data/hadoop/journal<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.ha.automatic-failover.enabled<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">value</span>&gt;</span>true<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.client.failover.proxy.provider.ns1<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">value</span>&gt;</span></span><br><span class="line">org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</span><br><span class="line"><span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.ha.fencing.methods<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">value</span>&gt;</span></span><br><span class="line">sshfence</span><br><span class="line"><span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.ha.fencing.ssh.private-key-files<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">value</span>&gt;</span>/root/.ssh/id_rsa<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.ha.fencing.ssh.connect-timeout<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">value</span>&gt;</span>30000<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">value</span>&gt;</span>file:///data/hadoop/workspace/hdfs/name<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">value</span>&gt;</span>file:///data/hadoop/workspace/hdfs/data<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="title">value</span>&gt;</span>2<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">root@3970c1e5466e:/usr/local/hadoop-2.6.1/etc/hadoop# mv mapred-site.xml.template mapred-site.xml</span><br><span class="line">root@3970c1e5466e:/usr/local/hadoop-2.6.1/etc/hadoop# vim mapred-site.xml</span><br><span class="line"><span class="pi">&lt;?xml version="1.0"?&gt;</span></span><br><span class="line"><span class="pi">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">root@3970c1e5466e:/usr/local/hadoop-2.6.1/etc/hadoop# vim yarn-site.xml</span><br><span class="line"><span class="pi">&lt;?xml version="1.0"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">value</span>&gt;</span>cloud3<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="title">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">root<span class="variable">@3970c1e5466e</span><span class="symbol">:/usr/local/hadoop-</span><span class="number">2.6</span>.<span class="number">1</span>/etc/hadoop<span class="comment"># vim slaves</span></span><br><span class="line">cloud1</span><br><span class="line">cloud2</span><br><span class="line">cloud3</span><br><span class="line">cloud4</span><br><span class="line">cloud5</span><br><span class="line">cloud6</span><br></pre></td></tr></table></figure>
<h4 id="u5B89_u88C5Spark"><a href="#u5B89_u88C5Spark" class="headerlink" title="安装Spark"></a>安装Spark</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">root@<span class="number">3970</span>c1e5466e:~/.ssh<span class="preprocessor"># mv /software/scala-<span class="number">2.11</span><span class="number">.7</span> /usr/local/</span></span><br><span class="line">root@<span class="number">3970</span>c1e5466e:~/.ssh<span class="preprocessor"># tar -zxvf scala-<span class="number">2.11</span><span class="number">.7</span>.tgz</span></span><br><span class="line">root@<span class="number">3970</span>c1e5466e:~/.ssh<span class="preprocessor"># vim ~/.bashrc</span></span><br><span class="line"><span class="keyword">export</span> JAVA_HOME=/usr/local/jdk1<span class="number">.8</span><span class="number">.0</span>_60</span><br><span class="line"><span class="keyword">export</span> HADOOP_HOME=/usr/local/hadoop-<span class="number">2.6</span><span class="number">.1</span></span><br><span class="line"><span class="keyword">export</span> SCALA_HOME=/usr/local/scala-<span class="number">2.11</span><span class="number">.7</span></span><br><span class="line"><span class="keyword">export</span> SPARK_HOME=/usr/local/spark-<span class="number">1.5</span><span class="number">.1</span>-bin-hadoop2<span class="number">.6</span></span><br><span class="line"><span class="keyword">export</span> PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$SCALA_HOME/bin:$SPARK_HOME/bin</span><br><span class="line"></span><br><span class="line">root@<span class="number">3970</span>c1e5466e:~/.ssh<span class="preprocessor"># mv /software/spark-<span class="number">1.5</span><span class="number">.1</span>-bin-hadoop2<span class="number">.6</span>.tgz /usr/local/</span></span><br><span class="line">root@<span class="number">3970</span>c1e5466e:~/.ssh<span class="preprocessor"># tar -zxvf spark-<span class="number">1.5</span><span class="number">.1</span>-bin-hadoop2<span class="number">.6</span></span></span><br></pre></td></tr></table></figure>
<p>编辑配置文件</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">root@<span class="number">3970</span>c1e5466e:~/.ssh<span class="preprocessor"># cd /usr/local/spark-<span class="number">1.5</span><span class="number">.1</span>-bin-hadoop2<span class="number">.6</span>/</span></span><br><span class="line">root@<span class="number">3970</span>c1e5466e:/usr/local/spark-<span class="number">1.5</span><span class="number">.1</span>-bin-hadoop2<span class="number">.6</span><span class="preprocessor"># cd conf/</span></span><br><span class="line">root@<span class="number">3970</span>c1e5466e:/usr/local/spark-<span class="number">1.5</span><span class="number">.1</span>-bin-hadoop2<span class="number">.6</span><span class="preprocessor"># vim slaves</span></span><br><span class="line">cloud1</span><br><span class="line">cloud2</span><br><span class="line">cloud3</span><br><span class="line">cloud4</span><br><span class="line">cloud5</span><br><span class="line">cloud6</span><br><span class="line">root@<span class="number">3970</span>c1e5466e:/usr/local/spark-<span class="number">1.5</span><span class="number">.1</span>-bin-hadoop2<span class="number">.6</span><span class="preprocessor"># mv spark-env.sh.template spark-env.sh</span></span><br><span class="line">root@<span class="number">3970</span>c1e5466e:/usr/local/spark-<span class="number">1.5</span><span class="number">.1</span>-bin-hadoop2<span class="number">.6</span><span class="preprocessor"># vim ~/spark/conf/spark-env.sh</span></span><br><span class="line"><span class="keyword">export</span> SPARK_MASTER_IP=cloud1</span><br><span class="line"><span class="keyword">export</span> SPARK_WORKER_MEMORY=<span class="number">128</span>m</span><br><span class="line"><span class="keyword">export</span> JAVA_HOME=/usr/local/jdk1<span class="number">.8</span><span class="number">.0</span>_60</span><br><span class="line"><span class="keyword">export</span> SCALA_HOME=/usr/local/scala-<span class="number">2.11</span><span class="number">.7</span></span><br><span class="line"><span class="keyword">export</span> SPARK_HOME=/usr/local/spark-<span class="number">1.5</span><span class="number">.1</span>-bin-hadoop2<span class="number">.6</span></span><br><span class="line"><span class="keyword">export</span> HADOOP_CONF_DIR=/usr/local/hadoop-<span class="number">2.6</span><span class="number">.1</span>/etc/hadoop</span><br><span class="line"><span class="keyword">export</span> SPARK_LIBRARY_PATH=$$SPARK_HOME/lib</span><br><span class="line"><span class="keyword">export</span> SCALA_LIBRARY_PATH=$SPARK_LIBRARY_PATH</span><br><span class="line"><span class="keyword">export</span> SPARK_WORKER_CORES=<span class="number">1</span></span><br><span class="line"><span class="keyword">export</span> SPARK_WORKER_INSTANCES=<span class="number">1</span></span><br><span class="line"><span class="keyword">export</span> SPARK_MASTER_PORT=<span class="number">7077</span></span><br></pre></td></tr></table></figure>
<p>此时配置已经基本完成，所以需要<strong>保存镜像</strong></p>
<figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[reason<span class="variable">@localhost</span> ~]<span class="variable">$ </span>sudo docker commit -m <span class="string">"mofun spark first commit"</span> -a <span class="string">"reason"</span> cloud1 mofun/<span class="symbol">spark:</span>v1.<span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>查看执行过的镜像</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[reason@localhost ~]$ sudo docker ps -a</span><br><span class="line">CONTAINER ID        IMAGE                 COMMAND                  CREATED             STATUS                         PORTS               NAMES</span><br><span class="line">d4e581ba6af8        mofun/spark:v1<span class="number">.0</span>   <span class="string">"/bin/bash"</span>              <span class="number">11</span> <span class="function">seconds ago      <span class="title">Exited</span> <span class="params">(<span class="number">0</span>)</span> 7 seconds ago                           hungry_pasteur</span></span><br></pre></td></tr></table></figure>
<p>启动容器</p>
<figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 后台模式运行</span></span><br><span class="line">[reason@localhost ~]$ docker <span class="command">run</span> -d <span class="comment">--name cloud2 -h cloud2 -it mofun/spark:v2.0 /bin/bash</span></span><br><span class="line"></span><br><span class="line">需要使用刚才制作的镜像启动<span class="number">6</span>个容器</span><br><span class="line">[reason@localhost ~]$ docker <span class="command">run</span> <span class="comment">--name cloud1 -h cloud1 -it mofun/spark:v2.0 /bin/bash</span></span><br><span class="line">[reason@localhost ~]$ docker <span class="command">run</span> <span class="comment">--name cloud2 -h cloud2 -it mofun/spark:v2.0 /bin/bash</span></span><br><span class="line">[reason@localhost ~]$ docker <span class="command">run</span> <span class="comment">--name cloud3 -h cloud3 -it mofun/spark:v2.0 /bin/bash</span></span><br><span class="line">[reason@localhost ~]$ docker <span class="command">run</span> <span class="comment">--name cloud4 -h cloud4 -it mofun/spark:v2.0 /bin/bash</span></span><br><span class="line">[reason@localhost ~]$ docker <span class="command">run</span> <span class="comment">--name cloud5 -h cloud5 -it mofun/spark:v2.0 /bin/bash</span></span><br><span class="line">[reason@localhost ~]$ docker <span class="command">run</span> <span class="comment">--name cloud6 -h cloud6 -it mofun/spark:v2.0 /bin/bash</span></span><br></pre></td></tr></table></figure>
<p>做最后的修改</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor">#在cloud5~cloud6中分别手动修改myid</span></span><br><span class="line">root@cloud5:~<span class="preprocessor"># echo <span class="number">2</span> &gt;  /data/zookeeper/myid</span></span><br><span class="line">root@cloud5:~<span class="preprocessor"># echo <span class="number">2</span> &gt; /usr/local/zookeeper-<span class="number">3.4</span><span class="number">.6</span>/tmp/myid</span></span><br><span class="line">root@cloud6:~<span class="preprocessor"># echo <span class="number">3</span> &gt;  /data/zookeeper/myid</span></span><br><span class="line">root@cloud6:~<span class="preprocessor"># echo <span class="number">3</span> &gt; /usr/local/zookeeper-<span class="number">3.4</span><span class="number">.6</span>/tmp/myid</span></span><br></pre></td></tr></table></figure>
<p>启动zookeeper集群</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor"># 启动zookeeper集群（分别在cloud4、cloud5、cloud6上启动zk）</span></span><br><span class="line"><span class="preprocessor"># 全部节点启动后，再执行zkServer.sh status</span></span><br><span class="line"><span class="preprocessor"># 返回正常</span></span><br><span class="line"><span class="preprocessor"># root@cloud6:/usr/local/zookeeper-<span class="number">3.4</span><span class="number">.6</span>/bin# ./zkServer.sh status</span></span><br><span class="line"><span class="preprocessor"># JMX enabled by default</span></span><br><span class="line"><span class="preprocessor"># Using config: /usr/local/zookeeper-<span class="number">3.4</span><span class="number">.6</span>/bin/../conf/zoo.cfg</span></span><br><span class="line"><span class="preprocessor"># Mode: follower</span></span><br><span class="line">root@cloud5:~<span class="preprocessor"># /usr/local/zookeeper-<span class="number">3.4</span><span class="number">.6</span>/bin/zkServer.sh start</span></span><br><span class="line"><span class="preprocessor"># 当<span class="number">3</span>个节点服务正常启动后</span></span><br><span class="line"><span class="preprocessor"># 使用status查看是否启动</span></span><br><span class="line">root@cloud5:~<span class="preprocessor"># /usr/local/zookeeper-<span class="number">3.4</span><span class="number">.6</span>/bin/zkServer.sh status</span></span><br><span class="line">JMX enabled by <span class="keyword">default</span></span><br><span class="line">Using config: /usr/local/zookeeper-<span class="number">3.4</span><span class="number">.6</span>/bin/../conf/zoo.cfg</span><br><span class="line">Mode: follower</span><br><span class="line">root@cloud5:~<span class="preprocessor">#</span></span><br></pre></td></tr></table></figure>
<p>进入cloud1，开启hadoop和spark服务</p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动journalnode（在cloud1上启动所有journalnode，注意：是调用的hadoop-daemons.sh这个脚本，注意是复数s的那个脚本）</span></span><br><span class="line"><span class="comment"># 运行jps命令检验，cloud4、cloud5、cloud6上多了JournalNode进程</span></span><br><span class="line">root<span class="variable">@cloud1</span><span class="symbol">:/usr/local/hadoop-</span><span class="number">2.6</span>.<span class="number">1</span>/sbin<span class="comment"># pwd</span></span><br><span class="line">/usr/local/hadoop-<span class="number">2.6</span>.<span class="number">1</span>/sbin</span><br><span class="line">root<span class="variable">@cloud1</span><span class="symbol">:/usr/local/hadoop-</span><span class="number">2.6</span>.<span class="number">1</span>/sbin<span class="comment">#</span></span><br><span class="line">root<span class="variable">@cloud1</span><span class="symbol">:/usr/local/hadoop-</span><span class="number">2.6</span>.<span class="number">1</span>/sbin<span class="comment"># hadoop-daemons.sh start journalnode</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 格式化ZK(在cloud1上执行即可，在bin目录下)</span></span><br><span class="line">root<span class="variable">@cloud1</span><span class="symbol">:/usr/local/hadoop-</span><span class="number">2.6</span>.<span class="number">1</span>/bin<span class="comment"># hdfs zkfc -formatZK</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 进入节点cloud4，查看zookeeper信息</span></span><br><span class="line">root<span class="variable">@cloud4</span><span class="symbol">:/usr/local/zookeeper-</span><span class="number">3.4</span>.<span class="number">6</span>/bin<span class="comment"># pwd</span></span><br><span class="line">/usr/local/zookeeper-<span class="number">3.4</span>.<span class="number">6</span>/bin</span><br><span class="line">root<span class="variable">@cloud4</span><span class="symbol">:/usr/local/zookeeper-</span><span class="number">3.4</span>.<span class="number">6</span>/bin<span class="comment"># ls</span></span><br><span class="line"><span class="constant">README</span>.txt  zkCleanup.sh  zkCli.cmd  zkCli.sh  zkEnv.cmd  zkEnv.sh  zkServer.cmd  zkServer.sh  zookeeper.out</span><br><span class="line">root<span class="variable">@cloud4</span><span class="symbol">:/usr/local/zookeeper-</span><span class="number">3.4</span>.<span class="number">6</span>/bin<span class="comment">#</span></span><br><span class="line">root<span class="variable">@cloud4</span><span class="symbol">:/usr/local/zookeeper-</span><span class="number">3.4</span>.<span class="number">6</span>/bin<span class="comment"># ./zkCli.sh</span></span><br><span class="line"><span class="constant">Connecting</span> to <span class="symbol">localhost:</span><span class="number">2181</span></span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span> 09<span class="symbol">:</span><span class="number">34</span><span class="symbol">:</span><span class="number">34</span>,<span class="number">485</span> [<span class="symbol">myid:</span>] - <span class="constant">INFO</span>  [<span class="symbol">main:</span><span class="constant">Environment</span><span class="variable">@100</span>] - <span class="constant">Client</span> <span class="symbol">environment:</span>zookeeper.version=<span class="number">3.4</span>.<span class="number">6</span>-<span class="number">1569965</span>, built on <span class="number">02</span>/<span class="number">20</span>/<span class="number">2014</span> 09<span class="symbol">:</span>09 <span class="constant">GMT</span></span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span> 09<span class="symbol">:</span><span class="number">34</span><span class="symbol">:</span><span class="number">34</span>,<span class="number">487</span> [<span class="symbol">myid:</span>] - <span class="constant">INFO</span>  [<span class="symbol">main:</span><span class="constant">Environment</span><span class="variable">@100</span>] - <span class="constant">Client</span> <span class="symbol">environment:</span>host.name=cloud4</span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span> 09<span class="symbol">:</span><span class="number">34</span><span class="symbol">:</span><span class="number">34</span>,<span class="number">487</span> [<span class="symbol">myid:</span>] - <span class="constant">INFO</span>  [<span class="symbol">main:</span><span class="constant">Environment</span><span class="variable">@100</span>] - <span class="constant">Client</span> <span class="symbol">environment:</span>java.version=<span class="number">1.8</span>.<span class="number">0_</span>...</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">[<span class="symbol">zk:</span> <span class="symbol">localhost:</span><span class="number">2181</span>(<span class="constant">CONNECTED</span>) <span class="number">2</span>] ls /hadoop-ha</span><br><span class="line">[ns1]</span><br><span class="line">[<span class="symbol">zk:</span> <span class="symbol">localhost:</span><span class="number">2181</span>(<span class="constant">CONNECTED</span>) <span class="number">3</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 格式化HDFS(在bin目录下),在cloud1上执行命令:</span></span><br><span class="line">root<span class="variable">@cloud1</span><span class="symbol">:/usr/local/hadoop-</span><span class="number">2.6</span>.<span class="number">1</span>/bin<span class="comment"># hdfs namenode -format</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 首先启动active节点，执行如下命令(在cloud1上执行)</span></span><br><span class="line">root<span class="variable">@cloud1</span><span class="symbol">:/usr/local/hadoop-</span><span class="number">2.6</span>.<span class="number">1</span>/sbin<span class="comment"># hadoop-daemon.sh start namenode  </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 进入cloud2，需要启动standy模式</span></span><br><span class="line">root<span class="variable">@cloud2</span><span class="symbol">:/usr/local/hadoop-</span><span class="number">2.6</span>.<span class="number">1</span>/bin<span class="comment"># pwd</span></span><br><span class="line">/usr/local/hadoop-<span class="number">2.6</span>.<span class="number">1</span>/bin</span><br><span class="line">root<span class="variable">@cloud2</span><span class="symbol">:/usr/local/hadoop-</span><span class="number">2.6</span>.<span class="number">1</span>/bin<span class="comment"># ./hdfs namenode -bootstrapStandby</span></span><br><span class="line">root<span class="variable">@cloud2</span><span class="symbol">:/usr/local/hadoop-</span><span class="number">2.6</span>.<span class="number">1</span>/sbin<span class="comment"># pwd</span></span><br><span class="line">/usr/local/hadoop-<span class="number">2.6</span>.<span class="number">1</span>/sbin</span><br><span class="line"><span class="comment"># 这条命令可以等到hadoop-daemons.sh start zkfc 成功以后执行</span></span><br><span class="line"><span class="comment"># 貌似是zkfc的启动慢导致standby模式启动出错？</span></span><br><span class="line"></span><br><span class="line">root<span class="variable">@cloud1</span><span class="symbol">:/usr/local/spark-</span><span class="number">1.5</span>.<span class="number">1</span>-bin-hadoop2.<span class="number">6</span>/bin<span class="comment"># jps</span></span><br><span class="line"><span class="number">1522</span> <span class="constant">NameNode</span></span><br><span class="line"><span class="number">2546</span> <span class="constant">Jps</span></span><br><span class="line"><span class="number">1859</span> <span class="constant">DFSZKFailoverController</span></span><br><span class="line"><span class="number">1109</span> <span class="constant">Worker</span></span><br><span class="line"><span class="number">104</span> <span class="constant">JournalNode</span></span><br><span class="line"><span class="number">426</span> <span class="constant">DataNode</span></span><br><span class="line"><span class="number">558</span> <span class="constant">NodeManager</span></span><br><span class="line"><span class="number">943</span> <span class="constant">Master</span></span><br><span class="line"><span class="comment"># 仔细观察DFSZKFailoverController 这个进程的存在情况</span></span><br><span class="line"></span><br><span class="line">root<span class="variable">@cloud2</span><span class="symbol">:/usr/local/hadoop-</span><span class="number">2.6</span>.<span class="number">1</span>/sbin<span class="comment"># ./hadoop-daemon.sh start namenode</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 重新进入cloud1 ，启动datanode</span></span><br><span class="line">root<span class="variable">@cloud4</span><span class="symbol">:/usr/local/hadoop-</span><span class="number">2.6</span>.<span class="number">1</span>/sbin<span class="comment"># pwd</span></span><br><span class="line">/usr/local/hadoop-<span class="number">2.6</span>.<span class="number">1</span>/sbin</span><br><span class="line">root<span class="variable">@cloud4</span><span class="symbol">:/usr/local/hadoop-</span><span class="number">2.6</span>.<span class="number">1</span>/sbin<span class="comment"># ./hadoop-daemons.sh start datanode</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在cloud3上执行start-yarn.sh</span></span><br><span class="line">root<span class="variable">@cloud3</span><span class="symbol">:/usr/local/hadoop-</span><span class="number">2.6</span>.<span class="number">1</span>/sbin<span class="comment"># start-yarn.sh</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动ZKFC</span></span><br><span class="line">root<span class="variable">@cloud1</span><span class="symbol">:/usr/local/hadoop-</span><span class="number">2.6</span>.<span class="number">1</span>/sbin<span class="comment"># ./hadoop-daemons.sh start zkfc</span></span><br><span class="line"><span class="comment"># 启动spark集群</span></span><br><span class="line">root<span class="variable">@cloud1</span><span class="symbol">:/usr/local/hadoop-</span><span class="number">2.6</span>.<span class="number">1</span>/sbin<span class="comment"># cd /usr/local/spark-1.5.1-bin-hadoop2.6/sbin/</span></span><br><span class="line">root<span class="variable">@cloud1</span><span class="symbol">:/usr/local/spark-</span><span class="number">1.5</span>.<span class="number">1</span>-bin-hadoop2.<span class="number">6</span>/sbin<span class="comment"># start-all.sh</span></span><br></pre></td></tr></table></figure>
<p>此时可以通过CURL访问服务了，如果宿主机中的hosts文件没有配置docker容器的主机名和IP地址映射关系的话要换成用IP访问</p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[reason<span class="annotation">@localhost</span> ~]$ curl <span class="string">http:</span><span class="comment">//172.17.0.56:50070</span></span><br><span class="line">[reason<span class="annotation">@localhost</span> ~]$</span><br></pre></td></tr></table></figure>
<h4 id="u5176_u4ED6"><a href="#u5176_u4ED6" class="headerlink" title="其他"></a>其他</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor"># 删除镜像</span></span><br><span class="line">[reason@localhost ~]$ docker images</span><br><span class="line">REPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZE</span><br><span class="line">mofun/spark         v2<span class="number">.0</span>                b2dacac3e132        <span class="number">3</span> hours ago         <span class="number">1.508</span> GB</span><br><span class="line">mofun/spark         v1<span class="number">.3</span>                <span class="number">4</span>d2f33ca61ee        <span class="number">21</span> hours ago        <span class="number">1.506</span> GB</span><br><span class="line">ubuntu              latest              <span class="number">0</span>a17decee413        <span class="number">6</span> days ago          <span class="number">188.3</span> MB</span><br><span class="line">docker/whalesay     latest              ded5e192a685        <span class="number">4</span> months ago        <span class="number">247</span> MB</span><br><span class="line">[reason@localhost ~]$ sudo docker rmi <span class="number">4</span>d2f</span><br></pre></td></tr></table></figure>
<figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 删除容器</span></span><br><span class="line">[reason<span class="variable">@localhost</span> ~]<span class="variable">$ </span>docker ps -a</span><br><span class="line"><span class="constant">CONTAINER ID </span>       <span class="constant">IMAGE </span>              <span class="constant">COMMAND </span>            <span class="constant">CREATED </span>            <span class="constant">STATUS </span>                       <span class="constant">PORTS </span>              <span class="constant">NAMES</span></span><br><span class="line"><span class="number">8258875263</span>be        ubuntu              <span class="string">"/bin/bash"</span>         <span class="number">3</span> minutes ago       <span class="constant">Exited </span>(<span class="number">127</span>) <span class="number">3</span> minutes ago                        mad_mestorf</span><br><span class="line">[reason<span class="variable">@localhost</span> ~]<span class="variable">$ </span>sudo docker rm <span class="number">8258875263</span>be</span><br><span class="line"><span class="number">8258875263</span>be</span><br><span class="line">[reason<span class="variable">@localhost</span> ~]$</span><br></pre></td></tr></table></figure>
<figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 删除(NULL) 容器</span></span><br><span class="line">root<span class="variable">@iZ28ikebrg6Z</span><span class="symbol">:/var/run</span><span class="comment"># docker images  </span></span><br><span class="line"><span class="constant">REPOSITORY </span>            <span class="constant">TAG </span>                <span class="constant">IMAGE ID </span>           <span class="constant">CREATED </span>            <span class="constant">VIRTUAL SIZE </span> </span><br><span class="line">&lt;none&gt;                 &lt;none&gt;              def2e0b08cbc        <span class="constant">About </span>an hour ago   <span class="number">1.37</span> <span class="constant">GB </span> </span><br><span class="line">sameersbn/redmine      latest              f0bec095f291        <span class="number">2</span> hours ago         <span class="number">614.6</span> <span class="constant">MB </span> </span><br><span class="line">root<span class="variable">@iZ28ikebrg6Z</span><span class="symbol">:/var/run</span><span class="comment"># docker ps -a  </span></span><br><span class="line"><span class="constant">CONTAINER ID </span>       <span class="constant">IMAGE </span>                   <span class="constant">COMMAND </span>               <span class="constant">CREATED </span>            <span class="constant">STATUS </span>                    <span class="constant">PORTS </span>              <span class="constant">NAMES </span> </span><br><span class="line"><span class="number">5</span>d6373cb79e6        <span class="number">224</span>b40d4b89f             <span class="string">"/bin/sh -c 'apt-get   25 hours ago        Exited (0) 25 hours ago                        distracted_blackwell    </span><br><span class="line">root@iZ28ikebrg6Z:/var/run# docker rm 5d63  </span><br><span class="line">5d63</span></span><br></pre></td></tr></table></figure>
<figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 镜像导出</span></span><br><span class="line">[reason<span class="variable">@localhost</span> ~]<span class="variable">$ </span>sudo docker ps -a</span><br><span class="line"><span class="constant">CONTAINER ID </span>       <span class="constant">IMAGE </span>                <span class="constant">COMMAND </span>                 <span class="constant">CREATED </span>            <span class="constant">STATUS </span>                        <span class="constant">PORTS </span>              <span class="constant">NAMES</span></span><br><span class="line">d4e581ba6af8        mofun/<span class="symbol">spark_rc:</span>v1.<span class="number">0</span>   <span class="string">"/bin/bash"</span>              <span class="number">11</span> seconds ago      <span class="constant">Exited </span>(<span class="number">0</span>) <span class="number">7</span> seconds ago                           hungry_pasteur</span><br><span class="line">[reason<span class="variable">@localhost</span> ~]<span class="variable">$ </span>sudo docker export d4e581ba6af8 &gt; mofunspark_v1.<span class="number">0</span>.tar</span><br><span class="line"></span><br><span class="line"><span class="comment"># 和导入恢复</span></span><br><span class="line">[reason<span class="variable">@localhost</span> ~]<span class="variable">$ </span>cat mofunspark_v1.<span class="number">0</span>.tar | sudo docker import - mofun/<span class="symbol">spark:</span>v1.<span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>执行Scala交互模式时出错时，需要检查</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">root@cloud1:/usr/local/jdk1<span class="number">.8</span><span class="number">.0</span>_60/jre/lib/ext<span class="preprocessor"># ls -la</span></span><br><span class="line">total <span class="number">25632</span></span><br><span class="line">drwxr-xr-x.  <span class="number">3</span>  <span class="number">501</span> staff     <span class="number">4096</span> Oct <span class="number">19</span> <span class="number">02</span>:<span class="number">35</span> .</span><br><span class="line">drwxr-xr-x. <span class="number">15</span>  <span class="number">501</span> staff     <span class="number">4096</span> Oct <span class="number">18</span> <span class="number">03</span>:<span class="number">34</span> ..</span><br><span class="line">-rw-r--r--.  <span class="number">1</span>  <span class="number">501</span> staff  <span class="number">3860522</span> Aug  <span class="number">4</span> <span class="number">19</span>:<span class="number">29</span> cldrdata.jar</span><br><span class="line">-rw-r--r--.  <span class="number">1</span>  <span class="number">501</span> staff     <span class="number">8286</span> Aug  <span class="number">4</span> <span class="number">19</span>:<span class="number">29</span> dnsns.jar</span><br><span class="line">-rw-r--r--.  <span class="number">1</span>  <span class="number">501</span> staff    <span class="number">44516</span> Aug  <span class="number">4</span> <span class="number">19</span>:<span class="number">29</span> jaccess.jar</span><br><span class="line">-rwxr-xr-x.  <span class="number">1</span>  <span class="number">501</span> staff <span class="number">18464934</span> Aug  <span class="number">3</span> <span class="number">17</span>:<span class="number">58</span> jfxrt.jar</span><br><span class="line">-rw-r--r--.  <span class="number">1</span>  <span class="number">501</span> staff  <span class="number">1178935</span> Aug  <span class="number">4</span> <span class="number">19</span>:<span class="number">29</span> localedata.jar</span><br><span class="line">-rw-r--r--.  <span class="number">1</span>  <span class="number">501</span> staff     <span class="number">1269</span> Aug  <span class="number">4</span> <span class="number">19</span>:<span class="number">29</span> meta-index</span><br><span class="line">-rw-r--r--.  <span class="number">1</span>  <span class="number">501</span> staff  <span class="number">2014239</span> Aug  <span class="number">4</span> <span class="number">19</span>:<span class="number">29</span> nashorn.jar</span><br><span class="line">-rw-r--r--.  <span class="number">1</span>  <span class="number">501</span> staff    <span class="number">39771</span> Aug  <span class="number">4</span> <span class="number">19</span>:<span class="number">29</span> sunec.jar</span><br><span class="line">-rw-r--r--.  <span class="number">1</span>  <span class="number">501</span> staff   <span class="number">278680</span> Aug  <span class="number">4</span> <span class="number">19</span>:<span class="number">29</span> sunjce_provider.jar</span><br><span class="line">-rw-r--r--.  <span class="number">1</span>  <span class="number">501</span> staff   <span class="number">250826</span> Aug  <span class="number">4</span> <span class="number">19</span>:<span class="number">29</span> sunpkcs11.jar</span><br><span class="line">drwxr-xr-x.  <span class="number">2</span> root root      <span class="number">4096</span> Oct <span class="number">19</span> <span class="number">02</span>:<span class="number">35</span> tmp</span><br><span class="line">-rw-r--r--.  <span class="number">1</span>  <span class="number">501</span> staff    <span class="number">68848</span> Aug  <span class="number">4</span> <span class="number">19</span>:<span class="number">29</span> zipfs.jar</span><br><span class="line">root@cloud1:/usr/local/jdk1<span class="number">.8</span><span class="number">.0</span>_60/jre/lib/ext<span class="preprocessor">#</span></span><br><span class="line"><span class="preprocessor"># 该目录下出现了很多类似._jfxrt.jar 的包，直接予以删除即可。</span></span><br></pre></td></tr></table></figure>
<h4 id="u53C2_u8003_u6587_u732E"><a href="#u53C2_u8003_u6587_u732E" class="headerlink" title="参考文献"></a>参考文献</h4><ul>
<li><a href="http://eksliang.iteye.com/blog/2226986" target="_blank" rel="external">http://eksliang.iteye.com/blog/2226986</a></li>
<li><a href="http://dockerpool.com/static/books/docker_practice/container/daemon.html" target="_blank" rel="external">http://dockerpool.com/static/books/docker_practice/container/daemon.html</a></li>
<li><a href="http://dockerpool.com/static/books/docker_practice/install/ubuntu.html" target="_blank" rel="external">http://dockerpool.com/static/books/docker_practice/install/ubuntu.html</a></li>
<li><a href="http://dockerpool.com/static/books/docker_practice/image/create.html" target="_blank" rel="external">http://dockerpool.com/static/books/docker_practice/image/create.html</a></li>
<li><a href="http://dockerpool.com/static/books/docker_practice/image/save_load.html" target="_blank" rel="external">http://dockerpool.com/static/books/docker_practice/image/save_load.html</a></li>
<li><a href="http://dockerpool.com/static/books/docker_practice/container/rm.html" target="_blank" rel="external">http://dockerpool.com/static/books/docker_practice/container/rm.html</a></li>
<li><a href="http://blog.csdn.net/qq1010885678/article/details/46353101" target="_blank" rel="external">http://blog.csdn.net/qq1010885678/article/details/46353101</a></li>
<li><a href="http://cn.soulmachine.me/blog/20131027/" target="_blank" rel="external">http://cn.soulmachine.me/blog/20131027/</a></li>
<li><a href="http://my.oschina.net/zjzhai/blog/225112" target="_blank" rel="external">http://my.oschina.net/zjzhai/blog/225112</a></li>
<li><a href="http://blog.csdn.net/minimicall/article/details/40188251" target="_blank" rel="external">http://blog.csdn.net/minimicall/article/details/40188251</a></li>
<li><a href="http://www.scala-lang.org/documentation/" target="_blank" rel="external">http://www.scala-lang.org/documentation/</a>  </li>
<li><a href="https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/streaming/NetworkWordCount.scala" target="_blank" rel="external">https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/streaming/NetworkWordCount.scala</a></li>
<li><a href="http://spark.apache.org/docs/latest/" target="_blank" rel="external">http://spark.apache.org/docs/latest/</a></li>
<li><a href="https://docs.sigmoidanalytics.com/index.php/Error:_Failed_to_initialize_compiler:_object_scala_not_found." target="_blank" rel="external">https://docs.sigmoidanalytics.com/index.php/Error:_Failed_to_initialize_compiler:_object_scala_not_found.</a></li>
<li><a href="http://docs.docker.com/linux/step_one/" target="_blank" rel="external">http://docs.docker.com/linux/step_one/</a></li>
<li><a href="http://blog.sequenceiq.com/blog/2015/01/09/spark-1-2-0-docker/" target="_blank" rel="external">http://blog.sequenceiq.com/blog/2015/01/09/spark-1-2-0-docker/</a></li>
</ul>
]]></content>
    <summary type="html">
    <![CDATA[<!--  -->
<!--  -->
<!--  _ __    __     __      ____    ___     ___   _____   __  __    ___     -->
<!-- /\`'__\/'__`\ /'__`\   /',__\  / _]]>
    </summary>
    
      <category term="Docker" scheme="http://reasonpun.com/tags/Docker/"/>
    
      <category term="HDFS" scheme="http://reasonpun.com/tags/HDFS/"/>
    
      <category term="Spark" scheme="http://reasonpun.com/tags/Spark/"/>
    
      <category term="数据挖掘" scheme="http://reasonpun.com/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
      <category term="实时计算" scheme="http://reasonpun.com/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97/"/>
    
      <category term="虚拟化" scheme="http://reasonpun.com/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[nginx-logrotate]]></title>
    <link href="http://reasonpun.com/2015/12/04/nginx-logrotate/"/>
    <id>http://reasonpun.com/2015/12/04/nginx-logrotate/</id>
    <published>2015-12-04T08:11:51.000Z</published>
    <updated>2015-12-25T03:34:28.000Z</updated>
    <content type="html"><![CDATA[<!--  -->
<!--  -->
<!--  _ __    __     __      ____    ___     ___   _____   __  __    ___     -->
<!-- /\`'__\/'__`\ /'__`\   /',__\  / __`\ /' _ `\/\ '__`\/\ \/\ \ /' _ `\   -->
<!-- \ \ \//\  __//\ \L\.\_/\__, `\/\ \L\ \/\ \/\ \ \ \L\ \ \ \_\ \/\ \/\ \  -->
<!--  \ \_\\ \____\ \__/.\_\/\____/\ \____/\ \_\ \_\ \ ,__/\ \____/\ \_\ \_\ -->
<!--   \/_/ \/____/\/__/\/_/\/___/  \/___/  \/_/\/_/\ \ \/  \/___/  \/_/\/_/ -->
<!--                                                 \ \_\                   -->
<!--                                                  \/_/                   -->
<!--  -->
<h3 id="Centos_Logrotate"><a href="#Centos_Logrotate" class="headerlink" title="Centos Logrotate"></a>Centos Logrotate</h3><h4 id="u5173_u4E8E"><a href="#u5173_u4E8E" class="headerlink" title="关于"></a>关于</h4><p><em>logrotate</em> is  designed to ease administration of systems that generatelarge numbers of log files.  It allows automatic rotation, compression,removal, and mailing of log files.  Each log file may be handled daily,weekly, monthly, or when it grows too large. (<a href="http://linuxcommand.org/man_pages/logrotate8.html" target="_blank" rel="external">Logrotate man page</a>)</p>
<h4 id="u914D_u7F6E_u6587_u4EF6"><a href="#u914D_u7F6E_u6587_u4EF6" class="headerlink" title="配置文件"></a>配置文件</h4><h5 id="u5185_u5BB9_u793A_u4F8B"><a href="#u5185_u5BB9_u793A_u4F8B" class="headerlink" title="内容示例"></a>内容示例</h5><figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">[reason@online_http:/etc/logrotate.d]$ cat nginx.logrotate.d</span><br><span class="line"><span class="preprocessor">#</span></span><br><span class="line"><span class="preprocessor"># author: reason@mofunsky.com</span></span><br><span class="line"><span class="preprocessor"># date:   2015-03-31 10:15</span></span><br><span class="line"><span class="preprocessor"># cron:</span></span><br><span class="line"><span class="preprocessor">#       59 23 * * * /usr/sbin/logrotate /etc/logrotate.conf &gt; /dev/null</span></span><br><span class="line"><span class="preprocessor">#</span></span><br><span class="line"><span class="preprocessor"># location:</span></span><br><span class="line"><span class="preprocessor">#       /etc/logrotate.d/nginx.logrotate.d</span></span><br><span class="line"><span class="preprocessor">#</span></span><br><span class="line">/local/nginx*.log &#123;</span><br><span class="line">    daily</span><br><span class="line">    rotate <span class="number">10</span></span><br><span class="line">    missingok</span><br><span class="line">    compress</span><br><span class="line">    notifempty</span><br><span class="line">    sharedscripts</span><br><span class="line">    postrotate</span><br><span class="line">        /bin/kill -USR1 $(cat /<span class="keyword">var</span>/nginx.pid <span class="number">2</span>&gt;/dev/<span class="literal">null</span>) <span class="number">2</span>&gt;/dev/<span class="literal">null</span> || :</span><br><span class="line">    endscript</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="u5B58_u653E_u4F4D_u7F6E"><a href="#u5B58_u653E_u4F4D_u7F6E" class="headerlink" title="存放位置"></a>存放位置</h5><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[reason<span class="variable">@online_http</span><span class="symbol">:/etc/logrotate</span>.d]<span class="variable">$ </span>pwd</span><br><span class="line">/etc/logrotate.d</span><br><span class="line">[reason<span class="variable">@online_http</span><span class="symbol">:/etc/logrotate</span>.d]$</span><br></pre></td></tr></table></figure>
<h5 id="u6267_u884C_u65B9_u5F0F_u548C_u65F6_u95F4"><a href="#u6267_u884C_u65B9_u5F0F_u548C_u65F6_u95F4" class="headerlink" title="执行方式和时间"></a>执行方式和时间</h5><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor"># add by reason @ <span class="number">2015</span>-<span class="number">04</span>-<span class="number">05</span> <span class="number">20</span>:<span class="number">41</span></span></span><br><span class="line"><span class="preprocessor"># logrotate nginx log daily</span></span><br><span class="line"><span class="number">59</span>      <span class="number">23</span>      *       *       *       root /usr/sbin/logrotate /etc/logrotate.conf &gt;/dev/null <span class="number">2</span>&gt;&amp;<span class="number">1</span></span><br></pre></td></tr></table></figure>
<h5 id="u6267_u884C_u6548_u679C"><a href="#u6267_u884C_u6548_u679C" class="headerlink" title="执行效果"></a>执行效果</h5><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor"># 会按照日期，在每天的<span class="number">23</span>：<span class="number">59</span>分将改天日志转储压缩为*.gz文件</span></span><br><span class="line">[reason@online_http:/local/nginx_access_log]$ ls -lsh</span><br><span class="line">total <span class="number">5.1</span>G</span><br><span class="line"><span class="number">427</span>M -rwxrwxrwx <span class="number">1</span> nginx nginx <span class="number">427</span>M Oct <span class="number">11</span> <span class="number">12</span>:<span class="number">01</span> nginx_me_access.<span class="built_in">log</span></span><br><span class="line"><span class="number">259</span>M -rwxrwxrwx <span class="number">1</span> nginx nginx <span class="number">259</span>M Oct  <span class="number">1</span> <span class="number">23</span>:<span class="number">59</span> nginx_me_access.<span class="built_in">log</span>-<span class="number">20151001.</span>gz</span><br><span class="line"><span class="number">233</span>M -rwxrwxrwx <span class="number">1</span> nginx nginx <span class="number">233</span>M Oct  <span class="number">2</span> <span class="number">23</span>:<span class="number">59</span> nginx_me_access.<span class="built_in">log</span>-<span class="number">20151002.</span>gz</span><br><span class="line"><span class="number">228</span>M -rwxrwxrwx <span class="number">1</span> nginx nginx <span class="number">228</span>M Oct  <span class="number">3</span> <span class="number">23</span>:<span class="number">59</span> nginx_me_access.<span class="built_in">log</span>-<span class="number">20151003.</span>gz</span><br><span class="line"><span class="number">225</span>M -rwxrwxrwx <span class="number">1</span> nginx nginx <span class="number">225</span>M Oct  <span class="number">4</span> <span class="number">23</span>:<span class="number">59</span> nginx_me_access.<span class="built_in">log</span>-<span class="number">20151004.</span>gz</span><br><span class="line"><span class="number">232</span>M -rwxrwxrwx <span class="number">1</span> nginx nginx <span class="number">232</span>M Oct  <span class="number">5</span> <span class="number">23</span>:<span class="number">59</span> nginx_me_access.<span class="built_in">log</span>-<span class="number">20151005.</span>gz</span><br></pre></td></tr></table></figure>
<h5 id="u53E6_u5916_u4E00_u79CD_u6267_u884C_u65B9_u5F0F"><a href="#u53E6_u5916_u4E00_u79CD_u6267_u884C_u65B9_u5F0F" class="headerlink" title="另外一种执行方式"></a>另外一种执行方式</h5><p>logrotate 是linux系统的缺省安装命令，初始化状态下即存在缺省的配置信息，其中包括执行文件和时间</p>
<figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[reason<span class="variable">@online_http</span><span class="symbol">:/local/nginx_access_log</span>]<span class="variable">$ </span>cd /etc/logrotate.d/</span><br><span class="line">[reason<span class="variable">@online_http</span><span class="symbol">:/etc/logrotate</span>.d]<span class="variable">$ </span>ls</span><br><span class="line">nginx.logrotate.d</span><br><span class="line">[reason<span class="variable">@online_http</span><span class="symbol">:/etc/logrotate</span>.d]<span class="variable">$ </span>pwd</span><br><span class="line">/etc/logrotate.d</span><br><span class="line">[reason<span class="variable">@online_http</span><span class="symbol">:/etc/logrotate</span>.d]$</span><br></pre></td></tr></table></figure>
<p>比如在/etc/logrotate.d目录下会缺省放置一批需要转储的日志服务对应的配置文件，比如httpd，exim等，logrotate会按照配置文件信息按照既定时间对其产生的日志数据进行转储操作。</p>
<figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[reason<span class="variable">@online_http</span><span class="symbol">:/etc/logrotate</span>.d]<span class="variable">$ </span>pwd</span><br><span class="line">/etc/logrotate.d</span><br><span class="line">[reason<span class="variable">@online_http</span><span class="symbol">:/etc/logrotate</span>.d]<span class="variable">$ </span>cat yum</span><br><span class="line">/var/log/yum.log &#123;</span><br><span class="line">    missingok</span><br><span class="line">    notifempty</span><br><span class="line">    size <span class="number">30</span>k</span><br><span class="line">    yearly</span><br><span class="line">    create <span class="number">0600</span> root root</span><br><span class="line">&#125;</span><br><span class="line">[reason<span class="variable">@online_http</span><span class="symbol">:/etc/logrotate</span>.d]$</span><br></pre></td></tr></table></figure>
<p>具体参数涵义可参考<a href="http://linuxcommand.org/man_pages/logrotate8.html" target="_blank" rel="external">Logrotate man page</a></p>
<p>缺省状态下，logrotate的自动执行分别被放入了如下几个文件中</p>
<ul>
<li>cron.daily</li>
<li>cron.weekly</li>
<li>cron.monthly</li>
</ul>
<p>而对于不同的linux发行版本以上脚本可能被放置在/etc/crontab中<br>或者</p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[root<span class="variable">@i</span>-bntub2bp logrotate.d]<span class="comment"># cat /etc/anacrontab</span></span><br><span class="line"><span class="comment"># /etc/anacrontab: configuration file for anacron</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># See anacron(8) and anacrontab(5) for details.</span></span><br><span class="line"></span><br><span class="line"><span class="constant">SHELL</span>=<span class="regexp">/bin/sh</span></span><br><span class="line"><span class="constant">PATH</span>=<span class="regexp">/sbin:/bin</span><span class="symbol">:/usr/sbin</span><span class="symbol">:/usr/bin</span></span><br><span class="line"><span class="constant">MAILTO</span>=root</span><br><span class="line"><span class="comment"># the maximal random delay added to the base delay of the jobs</span></span><br><span class="line"><span class="constant">RANDOM_DELAY</span>=<span class="number">45</span></span><br><span class="line"><span class="comment"># the jobs will be started during the following hours only</span></span><br><span class="line"><span class="constant">START_HOURS_RANGE</span>=<span class="number">3</span>-<span class="number">22</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#period in days   delay in minutes   job-identifier   command</span></span><br><span class="line"><span class="number">1</span>       <span class="number">5</span>       cron.daily              nice run-parts /etc/cron.daily</span><br><span class="line"><span class="number">7</span>       <span class="number">25</span>      cron.weekly             nice run-parts /etc/cron.weekly</span><br><span class="line"><span class="variable">@monthly</span> <span class="number">45</span>     cron.monthly            nice run-parts /etc/cron.monthly</span><br><span class="line">[root<span class="variable">@i</span>-bntub2bp logrotate.d]<span class="comment">#</span></span><br></pre></td></tr></table></figure>
<p>针对</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@i-bntub2bp logrotate.d]# cat /etc/redhat-<span class="operator"><span class="keyword">release</span></span><br><span class="line">CentOS <span class="keyword">release</span> <span class="number">6.6</span> (<span class="keyword">Final</span>)</span><br><span class="line">[root@<span class="keyword">i</span>-bntub2bp logrotate.<span class="keyword">d</span>]#</span></span><br></pre></td></tr></table></figure>
<p>而言，可以直接在/etc/<a href="https://www.centos.org/docs/2/rhl-cg-en-7.2/anacron.html" target="_blank" rel="external">anacrontab</a>中找到如上信息。</p>
<p>大体解释下anacrontab的部分参数涵义</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">START_HOURS_RANGE=<span class="number">3</span>-<span class="number">22</span>  <span class="preprocessor"># 执行时间为<span class="number">3</span>点到<span class="number">22</span>点之间</span></span><br><span class="line">RANDOM_DELAY=<span class="number">45</span> <span class="preprocessor"># 时间处于可执行区间内随机延迟<span class="number">45</span>分钟之内任意时间</span></span><br><span class="line"><span class="number">1</span>       <span class="number">5</span>       cron.daily              nice run-parts /etc/cron.daily</span><br><span class="line"><span class="preprocessor"># 执行时间为<span class="number">3</span>点到<span class="number">22</span>点之间执行/etc/cron.daily脚本 (after reboot and after the machine has been up for <span class="number">5</span> minutes^^), 如果没有重启服务，则会在<span class="number">3</span>：<span class="number">05</span>之后执行。</span></span><br></pre></td></tr></table></figure>
<h4 id="u53C2_u8003_u6587_u732E"><a href="#u53C2_u8003_u6587_u732E" class="headerlink" title="参考文献"></a>参考文献</h4><ul>
<li><a href="http://huoding.com/2013/04/21/246" target="_blank" rel="external">http://huoding.com/2013/04/21/246</a></li>
<li><a href="http://serverfault.com/questions/135906/when-does-cron-daily-run" target="_blank" rel="external">http://serverfault.com/questions/135906/when-does-cron-daily-run</a></li>
<li><a href="http://www.cyberciti.biz/faq/linux-when-does-cron-daily-weekly-monthly-run/" target="_blank" rel="external">http://www.cyberciti.biz/faq/linux-when-does-cron-daily-weekly-monthly-run/</a></li>
<li><a href="http://linuxcommand.org/man_pages/logrotate8.html" target="_blank" rel="external">http://linuxcommand.org/man_pages/logrotate8.html</a></li>
<li><a href="https://www.centos.org/docs/2/rhl-cg-en-7.2/anacron.html" target="_blank" rel="external">https://www.centos.org/docs/2/rhl-cg-en-7.2/anacron.html</a></li>
</ul>
]]></content>
    <summary type="html">
    <![CDATA[<!--  -->
<!--  -->
<!--  _ __    __     __      ____    ___     ___   _____   __  __    ___     -->
<!-- /\`'__\/'__`\ /'__`\   /',__\  / _]]>
    </summary>
    
      <category term="Logrorate" scheme="http://reasonpun.com/tags/Logrorate/"/>
    
      <category term="Nginx" scheme="http://reasonpun.com/tags/Nginx/"/>
    
      <category term="系统维护" scheme="http://reasonpun.com/categories/%E7%B3%BB%E7%BB%9F%E7%BB%B4%E6%8A%A4/"/>
    
      <category term="日志处理" scheme="http://reasonpun.com/categories/%E7%B3%BB%E7%BB%9F%E7%BB%B4%E6%8A%A4/%E6%97%A5%E5%BF%97%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[spark 源码分析]]></title>
    <link href="http://reasonpun.com/2015/12/04/spark-source-1/"/>
    <id>http://reasonpun.com/2015/12/04/spark-source-1/</id>
    <published>2015-12-04T02:59:14.000Z</published>
    <updated>2016-01-25T03:17:11.000Z</updated>
    <content type="html"><![CDATA[<!--  -->
<!--  -->
<!--  _ __    __     __      ____    ___     ___   _____   __  __    ___     -->
<!-- /\`'__\/'__`\ /'__`\   /',__\  / __`\ /' _ `\/\ '__`\/\ \/\ \ /' _ `\   -->
<!-- \ \ \//\  __//\ \L\.\_/\__, `\/\ \L\ \/\ \/\ \ \ \L\ \ \ \_\ \/\ \/\ \  -->
<!--  \ \_\\ \____\ \__/.\_\/\____/\ \____/\ \_\ \_\ \ ,__/\ \____/\ \_\ \_\ -->
<!--   \/_/ \/____/\/__/\/_/\/___/  \/___/  \/_/\/_/\ \ \/  \/___/  \/_/\/_/ -->
<!--                                                 \ \_\                   -->
<!--                                                  \/_/                   -->
<!--  -->
<h3 id="fork_git"><a href="#fork_git" class="headerlink" title="fork git"></a>fork git</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git <span class="built_in">clone</span> https://github.com/apache/spark.git</span><br></pre></td></tr></table></figure>
]]></content>
    <summary type="html">
    <![CDATA[<!--  -->
<!--  -->
<!--  _ __    __     __      ____    ___     ___   _____   __  __    ___     -->
<!-- /\`'__\/'__`\ /'__`\   /',__\  / _]]>
    </summary>
    
      <category term="Kernel" scheme="http://reasonpun.com/tags/Kernel/"/>
    
      <category term="Source" scheme="http://reasonpun.com/tags/Source/"/>
    
      <category term="Spark" scheme="http://reasonpun.com/tags/Spark/"/>
    
      <category term="数据挖掘" scheme="http://reasonpun.com/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
      <category term="实时计算" scheme="http://reasonpun.com/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97/"/>
    
      <category term="源码分析" scheme="http://reasonpun.com/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
</feed>
